{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d4549b",
   "metadata": {
    "id": "82d4549b"
   },
   "source": "# xdetox with llm masking, llm infilling, and decompx reranking\n\nThis notebook runs an XDetox pipeline with:\n\n1. **llm masking** using Mistral-7B-Instruct (`mistralai/Mistral-7B-Instruct-v0.2`), which detects toxic spans and replaces them with `<mask>`.\n2. **llm infilling** (same mistral model) that fills the `<mask>` tokens with safe alternatives while keeping the rest of the sentence almost unchanged.\n3. **decompx reranking** of multiple llm candidates per input, using token-level importance scores for toxicity from decompx.\n\nthe goal is to pick, for each toxic input sentence, **one best detoxified candidate** that is:\n\n* As **non-toxic** as possible.\n* As **semantically close** as possible to the original.\n* As **fluent and natural** as possible.\n\nmain differences from other pipelines:\n\n* both **masking** and **infilling** are done by an llm (Mistral-7B-Instruct).\n* There is **no MaRCo / BART generation**.\n* **decompx is used only for reranking**, not for masking.\n\n---\n\n## scoring: decompx reranking\n\nLet $s_j$ be a candidate detoxified sentence, and let $t_{i,j}$ be its tokens.\ndecompx gives an **importance score** $\\text{Importance}(t_{i,j})$ for each token, which measures how much that token contributes to predicted toxicity.\n\nFor each candidate $s_j$, decompx reranking computes a **toxicity score** as the sum of token importances:\n\n$$\n\\text{Score}(s_j) = \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n$$\n\nwhere $N_j$ is the number of tokens in $s_j$.\n\nthe final chosen sentence $s^*$ is:\n\n$$\ns^* = \\arg\\min_{s_j} \\left( \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j}) \\right)\n$$\n\nin words:\n\n* decompx is applied to **each candidate sentence**.\n* For each candidate, we **aggregate token-level toxicity importance**.\n* The candidate with the **lowest total importance** (lowest contribution to toxicity) is selected.\n\nthe notebook uses a **decompx threshold** `decompx_threshold` that controls which tokens are considered toxic enough to be masked / receive high importance. a lower threshold is more sensitive and may highlight more tokens, while a higher threshold is stricter.\n\n---\n\n## llm masking (mistral-7b-instruct)\n\n### prompted masking behavior\n\nmasking is done by a chat-style llm:\n\n* Model: `mistralai/Mistral-7B-Instruct-v0.2`.\n* The LLM is instructed to:\n\n  * **Identify toxic, offensive, or profane words or short phrases**.\n  * Replace **each toxic span** with a **single `<mask>` token**.\n  * allow **multiple `<mask>` tokens** in one sentence (one per toxic span).\n  * if multiple neighboring words are toxic, **collapse them into one `<mask>`**\n    (no `<mask> <mask> ...` runs).\n  * Keep **all non-toxic words and punctuation unchanged**.\n  * **Not** paraphrase, summarize, or reorder the sentence.\n  * Return the masked sentence **inside exactly one pair of brackets**:\n\n    ```text\n    [This is a <mask> example.]\n    ```\n\n### post-processing of llm masks\n\nbecause llm output can be noisy, the notebook cleans the raw masked output:\n\n1. **Extract the bracket content**:\n\n   * read the first `[ ... ]` block if present.\n   * If there is `[` but no `]`, take everything after the first `[` as the sentence.\n   * if there are no brackets, fall back to the full string.\n\n2. **Strip outer brackets** that may remain.\n\n3. **normalize whitespace** (collapse repeated spaces).\n\n4. **Normalize `<mask>` tokens**:\n\n   * variants like `<Mask>`, `<MASK>`, `< mask >` are normalized to `<mask>`.\n\n5. **collapse runs of `<mask>`**:\n\n   * any sequence `<mask> <mask> <mask>` becomes a single `<mask>`.\n\n6. if cleaning yields an empty string, fall back to the original masked text.\n\nall cleaned, llm-masked sentences are written to:\n\n* `data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/masked_inputs.txt`\n\nand reused across later runs with the same `output_folder` and `data_type`.\n\n---\n\n## llm infilling (mistral-7b-instruct)\n\nafter masking, detoxification is also done by mistral-7b-instruct via **infilling**.\n\nfor each example, the llm sees two inputs:\n\n1. **Toxic Sentence**: the original toxic sentence.\n2. **masked sentence**: the same sentence, where toxic spans have been replaced with `<mask>`.\n\nthe infilling prompt instructs the llm to:\n\n* treat the **masked sentence** as a **template**.\n* For each `<mask>` token:\n\n  * Insert a **short, non-toxic word or phrase** that fits the context.\n  * preserve the meaning and intent of the **toxic sentence** as much as possible.\n* Keep **all non-masked text unchanged**, except for small edits needed for grammar or agreement.\n* keep the **language the same** (no translation).\n* output only the final detoxified sentence in **one pair of brackets**:\n\n  ```text\n  [Detoxified sentence here.]\n  ```\n\nfor each `(source, masked)` pair, the notebook:\n\n1. builds a prompt that includes both **toxic sentence** and **masked sentence** plus a few-shot example.\n2. calls `generate(...)` with:\n\n   * `num_return_sequences = num_candidates`,\n   * `do_sample = llm_sample`,\n   * `temperature = llm_temperature` (if sampling),\n   * `top_p = llm_top_p`,\n   * `max_new_tokens = llm_max_new_tokens`.\n3. Decodes only the **new tokens** after the prompt.\n4. extracts the text inside `[ ... ]` and cleans it:\n\n   * remove outer brackets,\n   * normalize whitespace,\n   * remove any leftover `<mask>` tokens.\n\nif cleaning produces an empty string, the pipeline falls back to the original toxic input for that candidate.\n\nthis produces a list of candidates:\n\n* `candidates[i]` is a list of length `num_candidates` for input $i$.\n\n---\n\n## decompx reranking of llm candidates\n\nonce we have llm-infilling candidates, we apply **decompx reranking**:\n\n1. **flatten** all candidates into one list while tracking which input they belong to.\n\n2. for each candidate, run decompx to obtain token-level importance scores for toxicity.\n\n3. For each candidate $s_j$, compute:\n\n   $$\n   \\text{Score}(s_j) = \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n   $$\n\n   where $N_j$ is the number of tokens in the candidate.\n\n4. for each input sentence, collect the scores of all its candidates and choose the candidate with the **lowest decompx score**.\n\nthe `decompx_threshold` parameter:\n\n* controls the **sensitivity** of decompx to toxicity.\n* a lower value will tend to assign non-zero importance to more tokens (more sensitive).\n* A higher value will only highlight more strongly toxic tokens.\n\nfor each run folder, the notebook writes:\n\n* `orig.txt` — original toxic inputs (one per line),\n* `gen.txt` — chosen detoxified outputs (one per line).\n\noutputs are stored under:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/\n```\n\nwhere `{run_folder}` encodes llm infilling hyperparameters and decompx settings.\n\n---\n\n## evaluation\n\nif `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n\n* bertscore (F1)\n* meaningbert\n* bleu-4\n* toxicity (orig / gen) using xlm-r\n* perplexity (orig / gen) using gpt-2\n\nfor each run folder, it writes:\n\n* `gen_stats.txt` under:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/\n  ```\n\nit also creates a **summary csv per dataset**:\n\n* `data/model_outputs/{output_folder}/{data_type}/{data_type}.csv`\n\nthe csv aggregates metrics over all run folders inside `LLM_Mask_LLM_DecompX/`.\n\nfor compatibility with other xdetox pipelines, the csv keeps a `threshold` column, which is used here as a **label** (for example `0.20`) tied to the decompx reranking configuration, not to a masking threshold.\n\n---\n\n## how to use `detoxify()`\n\nfunction signature (conceptual):\n\n```python\ndef detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_llm_mask_infill_decompx\",\n    echo: bool = False,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    # LLM infilling:\n    num_candidates: int = 3,\n    llm_temperature: float = 0.7,\n    llm_top_p: float = 0.95,\n    llm_max_new_tokens: int = 64,\n    llm_sample: bool = True,\n    # DecompX reranking:\n    decompx_threshold: float = 0.20,\n    decompx_batch_size: int = 16,\n)\n```\n\n### key arguments\n\n#### core i/o\n\n* `data_type`: dataset key from `data_configs`, for example:\n\n  * `\"paradetox\"`, `\"dynabench_val\"`, `\"dynabench_test\"`,\n  * `\"jigsaw_toxic\"`, `\"microagressions_val\"`, `\"sbf_val\"`,\n  * `\"appdia_original\"`, `\"appdia_discourse\"`, etc.\n\n* `output_folder`:\n\n  * top-level directory under `data/model_outputs/`:\n\n    ```text\n    data/model_outputs/{output_folder}/{data_type}/...\n    ```\n\n* `num_examples`:\n\n  * if set, only the first `num_examples` examples are processed.\n  * use `None` to run on the full dataset.\n\n* `overwrite_gen`:\n\n  * if `False` and a matching `gen.txt` already exists, the notebook reuses previous generations.\n  * If `True`, it regenerates outputs for that run folder.\n\n* `echo`:\n\n  * if `True`, the notebook prints:\n\n    * dataset and subset path,\n    * output base directory,\n    * a few example inputs,\n    * a few llm-masked outputs,\n    * a few final detoxified outputs,\n    * and, if `run_eval=True`, evaluation metrics.\n\n#### llm masking (mistral)\n\n* masking uses **mistral-7b-instruct** with a fixed system prompt and few-shot example.\n* there is **no masking threshold**; the llm decides which spans to mask based on the instructions.\n* masked sentences are cached in:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/masked_inputs.txt\n  ```\n\nand reused if present.\n\n#### llm infilling (mistral)\n\n* `num_candidates`:\n\n  * number of infilling candidates to generate per input.\n\n* `llm_temperature`:\n\n  * sampling temperature (only used if `llm_sample=True`).\n\n* `llm_top_p`:\n\n  * nucleus sampling parameter during infilling.\n\n* `llm_max_new_tokens`:\n\n  * maximum number of new tokens to generate beyond the prompt.\n\n* `llm_sample`:\n\n  * `True`: stochastic sampling with `temperature` and `top_p`.\n  * `False`: deterministic decoding (similar to greedy).\n\nthe infilling step uses **both** the toxic sentence and the masked sentence, and is strictly instructed to **only change `<mask>` spans** plus minor grammar fixes.\n\n#### decompx reranking\n\n* `decompx_threshold`:\n\n  * decompx threshold used when computing token-level toxicity importance.\n  * roughly controls how aggressively decompx marks tokens as toxic.\n\n* `decompx_batch_size`:\n\n  * batch size for decompx processing during reranking.\n\nfor each input:\n\n* the pipeline uses decompx to score each llm candidate.\n* it picks the candidate whose sum of token importance scores is **minimal**.\n\n#### evaluation\n\n* `run_eval`:\n\n  * if `True`, run `evaluation.evaluate_all` and write `gen_stats.txt`.\n\n* `overwrite_eval`:\n\n  * If `False` and `gen_stats.txt` exists, keep existing metrics.\n  * if `True`, recompute metrics.\n\n* `skip_ref_eval`:\n\n  * if `True`, skip some reference-based parts (for example, reference perplexity).\n\n---\n\n## example calls\n\n### quick sanity check on a small subset\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"colab_run_llm_mask_infill_decompx_demo_50_examples\",\n    echo=True,\n    num_examples=50,               # small subset for testing\n    run_eval=True,                 # BLEU / BERTScore / MeaningBERT / PPL / Toxicity\n    overwrite_gen=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n    num_candidates=10,             # LLM infilling candidates per input\n    llm_temperature=0.7,\n    llm_top_p=0.95,\n    llm_max_new_tokens=64,\n    llm_sample=True,\n    decompx_threshold=0.20,\n    decompx_batch_size=16,\n)\n```\n\n### larger run (more candidates, full dataset)\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"paradetox_llm_mask_infill_decompx_full\",\n    echo=True,\n    num_examples=None,             # full dataset\n    run_eval=True,\n    overwrite_gen=False,\n    overwrite_eval=False,\n    skip_ref_eval=False,\n    num_candidates=20,\n    llm_temperature=0.7,\n    llm_top_p=0.95,\n    llm_max_new_tokens=64,\n    llm_sample=True,\n    decompx_threshold=0.20,\n    decompx_batch_size=16,\n)\n```\n\nafter running `detoxify`, you can inspect:\n\n* final inputs and outputs:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/orig.txt\n  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/gen.txt\n  ```\n\n* per-run evaluation metrics:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Mask_LLM_DecompX/{run_folder}/gen_stats.txt\n  ```\n\n* aggregated metrics across runs:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n  ```\n\nthis pipeline lets you compare:\n\n* **llm-masking + llm-infilling + decompx reranking**\n\nagainst other xdetox pipelines such as:\n\n* **llm-masking + llm-infilling + global reranking**,\n* **decompx-masking + llm-infilling + decompx or global reranking**, and\n* **llm-masking or decompx-masking with marco generation**."
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive; drive.mount('/content/drive')\n\nimport os, glob, re, sys, json, shutil, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom subprocess import run, PIPE\nimport torch\nimport nltk\nfrom typing import List\n\n# try my drive\ncandidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\nprint(\"try mydrive:\", candidate, \"->\", os.path.isdir(candidate))\n\nXDETOX_DIR = candidate\nprint(\"using xdetox_dir:\", XDETOX_DIR)\nassert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\"",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfBoQTrjtynY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559915440,
     "user_tz": 480,
     "elapsed": 35813,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "5c994425-f684-48aa-88cd-153182a1fb67"
   },
   "id": "kfBoQTrjtynY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\nos.makedirs(HF_CACHE, exist_ok=True)\nos.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n\nif XDETOX_DIR not in sys.path:\n    sys.path.append(XDETOX_DIR)\n\nprint(\"xdetox_dir:\", XDETOX_DIR)\nprint(\"transformers_cache:\", HF_CACHE)\nprint(\"cuda available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ITPlTNBtzQx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559915987,
     "user_tz": 480,
     "elapsed": 561,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "6348b089-9d78-4268-a43f-163d1c0a6f91"
   },
   "id": "7ITPlTNBtzQx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\nprint(\"repo folders ok\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEy2TGYetzIb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559916005,
     "user_tz": 480,
     "elapsed": 17,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "37ddb218-f598-4c9a-fd2e-32274ce3e849"
   },
   "id": "MEy2TGYetzIb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip -q install --upgrade pip setuptools wheel\n!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n                sentencepiece\n!pip -q install bert-score\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeTzwxVDtzNn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559937981,
     "user_tz": 480,
     "elapsed": 21972,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "de0e825b-b60a-4cfa-98a5-ae9ef60a0bdc"
   },
   "id": "GeTzwxVDtzNn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n)\nfrom rewrite.mask_orig import Masker as Masker_single\nfrom rewrite import rewrite_example as rx\nimport argparse as _argparse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfnuR2YVCmW9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559948925,
     "user_tz": 480,
     "elapsed": 10941,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "e41ca59a-11a6-4d82-ed25-9faac57fcce7"
   },
   "id": "tfnuR2YVCmW9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "nltk.download(\"punkt\", quiet=True)\ntry:\n    nltk.download(\"punkt_tab\", quiet=True)\nexcept Exception:\n    pass\nprint(\"ok\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0Up7SKstzK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949369,
     "user_tz": 480,
     "elapsed": 446,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "45b17dba-6b16-48c1-c03d-7ff5e2bde002"
   },
   "id": "y0Up7SKstzK9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data_configs = {\n    \"microagressions_val\": {\n        \"data_path\": \"./datasets/microagressions/val.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"microagressions_test\": {\n        \"data_path\": \"./datasets/microagressions/test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"sbf_val\": {\n        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"sbf_test\": {\n        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"dynabench_val\": {\n        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"dynabench_test\": {\n        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"jigsaw_toxic\": {\n        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"paradetox\": {\n        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_original\": {\n        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_discourse\": {\n        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    }\n}\nprint(\"datasets:\", \", \".join(data_configs.keys()))\n\nREPO = XDETOX_DIR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nBku39IuAgb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949407,
     "user_tz": 480,
     "elapsed": 35,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "2867d6c9-05da-47b7-c681-6ae9305cb128"
   },
   "id": "7nBku39IuAgb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _abs_repo_path(rel: str) -> str:\n    return os.path.join(REPO, rel.lstrip(\"./\"))\n\ndef _ensure_dir(p: str):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef _subset_for_data_type(data_type, data_path, n, out_dir):\n    if n is None or n <= 0:\n        return data_path\n\n    src = _abs_repo_path(data_path)\n    _ensure_dir(out_dir)\n\n    if \"microagressions\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"sbf\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"dynabench\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n        if data_path.endswith(\".txt\"):\n            with open(src, \"r\") as f:\n                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n            out = os.path.join(out_dir, os.path.basename(src))\n            with open(out, \"w\") as g:\n                for s in lines[:n]:\n                    g.write(s + \"\\n\")\n            return out\n        elif data_path.endswith(\".csv\"):\n            df = pd.read_csv(src).head(n)\n            out = os.path.join(out_dir, os.path.basename(src))\n            df.to_csv(out, index=False)\n            return out\n\n    if \"appdia\" in data_path:\n        df = pd.read_csv(src, sep=\"\\t\").head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        df.to_csv(out, sep=\"\\t\", index=False)\n        return out\n\n    out = os.path.join(out_dir, os.path.basename(src))\n    shutil.copy(src, out)\n    return out",
   "metadata": {
    "id": "ToytrY0SuAjr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949412,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "ToytrY0SuAjr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _decompx_mask_texts(texts: List[str],\n                        threshold: float = 0.20,\n                        batch_size: int = 16) -> List[str]:\n    if not texts:\n        return []\n\n    masker = Masker_single()\n    masked_all = []\n    for i in tqdm(range(0, len(texts), batch_size),\n                  desc=\"decompx masking for reranking\", leave=False):\n        batch = texts[i:i + batch_size]\n        batch_out = masker.process_text(sentence=batch, threshold=threshold)\n        masked_all.extend(batch_out)\n    cleaned = [\n        m.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n        for m in masked_all\n    ]\n    masker.release_model()\n    return cleaned\n\ndef _decompx_toxicity_scores(texts: List[str],\n                             threshold: float = 0.20,\n                             batch_size: int = 16) -> np.ndarray:\n    if not texts:\n        return np.zeros((0,), dtype=float)\n\n    masked = _decompx_mask_texts(texts, threshold=threshold, batch_size=batch_size)\n    scores = []\n    for m in masked:\n        num_masks = len(re.findall(r\"<mask>\", m))\n        tokens = m.split()\n        length = max(len(tokens), 1)\n        scores.append(num_masks / length)\n    return np.asarray(scores, dtype=float)\n\ndef rerank_candidates_decompx(\n    sources: List[str],\n    candidates: List[List[str]],\n    threshold: float = 0.20,\n    batch_size_mask: int = 16,\n):\n    N = len(sources)\n    assert len(candidates) == N, \"candidates length mismatch\"\n\n    if N == 0:\n        return np.array([], dtype=int), {}\n\n    C_list = [len(c) for c in candidates]\n    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n    C = C_list[0]\n    if C == 0:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    flat_cands = []\n    flat_src_idx = []\n    for i, cand_list in enumerate(candidates):\n        for cand in cand_list:\n            flat_cands.append(cand)\n            flat_src_idx.append(i)\n    flat_src_idx = np.array(flat_src_idx, dtype=int)\n\n    scores = _decompx_toxicity_scores(\n        flat_cands,\n        threshold=threshold,\n        batch_size=batch_size_mask,\n    )\n\n    scores2 = scores.reshape(N, C)\n    best_idx = np.argmin(scores2, axis=1)\n\n    details = {\n        \"score\": scores2,\n    }\n    return best_idx, details",
   "metadata": {
    "id": "-MxQChqOuAnT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949427,
     "user_tz": 480,
     "elapsed": 12,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "-MxQChqOuAnT",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False,\n                        tox_threshold=0.5, tox_batch_size=32):\n    import sys as _sys\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        if not os.path.isdir(gen_dir):\n            continue\n        orig_path = os.path.join(gen_dir, \"orig.txt\")\n        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n            continue\n        if os.path.exists(out_stats) and not overwrite_eval:\n            continue\n\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n        cmd = [\n            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n            \"--orig_path\", orig_path,\n            \"--gen_path\",  gen_path,\n            \"--tox_threshold\", str(tox_threshold),\n            \"--tox_batch_size\", str(tox_batch_size),\n        ]\n        if skip_ref:\n            cmd.append(\"--skip_ref\")\n        print(\"eval:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n        if res.returncode != 0:\n            print(res.stdout)\n            print(res.stderr)\n            res.check_returncode()\n\ndef _safe_float(x):\n    try:\n        return float(x)\n    except Exception:\n        return float('nan')\n\ndef _read_stats_file(path):\n    out = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            k, v = line.strip().split(\": \", 1)\n            k = k.replace(\"(skipped)\", \"\").strip().lower()\n            out[k] = _safe_float(v)\n    return out\n\ndef _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n    rows = []\n\n    mask_dir = \"LLM_Mask_LLM_DecompX\"\n    base_path = os.path.join(base_out_dir, data_type, mask_dir)\n    if not os.path.isdir(base_path):\n        print(\"no evaluation directory found:\", base_path)\n        return\n\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not os.path.exists(stats_path):\n            continue\n        s = _read_stats_file(stats_path)\n        rows.append({\n            \"threshold\":        0.20,\n            \"folder\":           folder,\n            \"bertscore\":        s.get(\"bertscore\", np.nan),\n            \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n            \"bleu4\":            s.get(\"bleu4\", np.nan),\n            \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n            \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n            \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n            \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n        })\n\n    if rows:\n        cols = [\n            \"threshold\", \"folder\",\n            \"bertscore\", \"meaningbert\", \"bleu4\",\n            \"perplexity_gen\", \"perplexity_orig\",\n            \"toxicity_gen\", \"toxicity_orig\",\n        ]\n        df = pd.DataFrame(rows)\n        df = df[cols]\n        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n        df.to_csv(out_csv, index=False)\n        print(\"wrote summary csv:\", out_csv)\n    else:\n        print(\"no evaluation files found\")",
   "metadata": {
    "id": "u-7I09Uvqb8c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949433,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "u-7I09Uvqb8c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "USE_LLM_GPU = True\nDEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\nprint(\"llm device:\", DEVICE_LLM)\n\nLLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n_LLM_TOKENIZER = None\n_LLM_MODEL = None\n\ndef _lazy_load_llm():\n    global _LLM_MODEL, _LLM_TOKENIZER\n    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n        return\n    print(f\"loading llm: {LLM_MODEL_NAME} on {DEVICE_LLM}\")\n    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL_NAME,\n        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n        device_map=None,\n    ).to(DEVICE_LLM)\n    _LLM_MODEL.eval()\n    print(\"llm loaded\")\n\n\ndef _extract_bracket_content(text: str) -> str:\n    text = text.strip()\n\n    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n    if m:\n        return m.group(1).strip()\n\n    if \"[\" in text:\n        return text.split(\"[\", 1)[1].strip()\n\n    return text",
   "metadata": {
    "id": "nAVpxvu2b_Me",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949452,
     "user_tz": 480,
     "elapsed": 17,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "86afc61d-21c9-42ba-dcf4-1895c1423c8b"
   },
   "id": "nAVpxvu2b_Me",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MASK_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences safer\nby MASKING toxic words with the special token <mask>, while keeping the original sentence\nstructure and wording as much as possible.\n\nYou must behave like a MASKER, not a full rewriter.\n\nYour task:\n1. Identify toxic, offensive, or profane words or short phrases.\n2. For each toxic span, replace the entire span with a single <mask> token.\n3. There may be multiple toxic spans in one sentence, so multiple <mask> tokens are allowed.\n4. If several neighboring words are toxic, you must still use only a single <mask> token\n   in that place. In other words, if you would place \"<mask> <mask>\" or a longer sequence\n   of <mask> tokens, collapse them into a single <mask> so that there are never multiple\n   <mask> tokens in a row.\n5. Do NOT rewrite, paraphrase, or summarize the sentence.\n6. Do NOT add, remove, or reorder non-toxic words or punctuation.\n7. Keep punctuation and spacing as close to the original as possible.\n8. If there is no toxic content, return the sentence unchanged.\n\nOutput rules (format is very strict):\n- ONLY return the final masked sentence inside ONE pair of square brackets, like:\n  [This is a <mask> example.]\n- Do NOT print anything before or after the brackets.\n- Do NOT add explanations, comments, or extra lines.\n- Do NOT include any language tags or metadata.\n- Do NOT include additional '[' or ']' characters inside the sentence.\n\"\"\"\n\nMASK_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\nStep 1 - Identify toxic words: \"stupid idiot\", \"crap\"\nStep 2 - Mask toxic words (do NOT rewrite the rest):\nYou're such a <mask>, nobody wants to hear your <mask>.\nFinal Output: [You're such a <mask>, nobody wants to hear your <mask>.]\"\"\"\n\ndef _postprocess_llm_mask(masked_text: str) -> str:\n    s = masked_text.strip()\n\n    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n        s = s[1:-1].strip()\n    else:\n        if s.startswith(\"[\"):\n            s = s[1:].strip()\n        if s.endswith(\"]\"):\n            s = s[:-1].strip()\n\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    s = re.sub(r\"<\\s*mask\\s*>\", \"<mask>\", s, flags=re.IGNORECASE)\n    s = re.sub(r\"(?:\\s*<mask>\\s*){2,}\", \" <mask> \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    if not s:\n        return masked_text.strip()\n\n    return s\n\n@torch.no_grad()\ndef llm_mask_sentences(sentences: List[str]) -> List[str]:\n    _lazy_load_llm()\n    masked = []\n    for s in tqdm(sentences, desc=\"llm masking\", leave=False):\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": MASK_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + MASK_FEW_SHOT,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Toxic Sentence: {s}\\nFinal Output:\",\n            },\n        ]\n        try:\n            prompt = _LLM_TOKENIZER.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            prompt = (\n                MASK_SYSTEM_PROMPT\n                + \"\\n\\nExample:\\n\"\n                + MASK_FEW_SHOT\n                + \"\\n\\nToxic Sentence: \"\n                + s\n                + \"\\nFinal Output:\"\n            )\n\n        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n        gen = _LLM_MODEL.generate(\n            **inputs,\n            max_new_tokens=64,\n            do_sample=False,\n            temperature=0.0,\n            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n        )\n        gen_text = _LLM_TOKENIZER.decode(\n            gen[0][inputs[\"input_ids\"].shape[1]:],\n            skip_special_tokens=True,\n        )\n\n        masked_text = _extract_bracket_content(gen_text)\n        masked_text = _postprocess_llm_mask(masked_text)\n\n        if not masked_text:\n            masked_text = s\n        masked.append(masked_text)\n\n    return masked",
   "metadata": {
    "id": "nG355GLlqcp-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949548,
     "user_tz": 480,
     "elapsed": 94,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "nG355GLlqcp-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "INFILL_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences\nmore polite and respectful by INFILLING the special token <mask>.\n\nYou are NOT a free rewriter. You must keep all non-masked text as close as possible\nto the given masked sentence.\n\nYou are given two inputs:\n1) Toxic Sentence: the original toxic sentence.\n2) Masked Sentence: the same sentence, where toxic spans are replaced with <mask>.\n\nYour task:\n1. For each <mask> token in the Masked Sentence, replace it with a short, non-toxic\n   word or phrase that fits the context and preserves the meaning of the Toxic Sentence.\n2. Do NOT modify any other words or punctuation outside the <mask> spans, unless a very\n   small change is needed to fix grammar or agreement.\n3. Preserve the original meaning and intent as much as possible, but make the sentence\n   safe and respectful.\n4. Keep the language the same as the original (do NOT translate).\n\nOutput rules (VERY STRICT):\n- ONLY return the final detoxified sentence with all <mask> tokens filled.\n- Wrap the final sentence in exactly ONE pair of square brackets, e.g.:\n  [Detoxified sentence here.]\n- Do NOT include the Toxic Sentence or Masked Sentence in your output.\n- Do NOT add explanations, comments, or extra lines.\n- Do NOT include any other '[' or ']' characters.\n\"\"\"\n\nINFILL_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\nMasked Sentence: You're such a <mask>, nobody wants to hear your <mask>.\nStep 1 - Decide safe replacements for each <mask>: \"rude person\", \"opinion\"\nStep 2 - Infill the masked sentence, keeping all other words the same:\nYou're such a rude person, nobody wants to hear your opinion.\nFinal Output: [You're such a rude person, nobody wants to hear your opinion.]\"\"\"\n\ndef _postprocess_llm_infill(text: str) -> str:\n    s = text.strip()\n\n    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n        s = s[1:-1].strip()\n    else:\n        if s.startswith(\"[\"):\n            s = s[1:].strip()\n        if s.endswith(\"]\"):\n            s = s[:-1].strip()\n\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    s = s.replace(\"<mask>\", \" \")\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    if not s:\n        return text.strip()\n    return s\n\n@torch.no_grad()\ndef llm_infill_candidates(\n    sources: List[str],\n    masked: List[str],\n    num_candidates: int = 3,\n    temperature: float = 0.7,\n    top_p: float = 0.95,\n    max_new_tokens: int = 64,\n    sample: bool = True,\n) -> List[List[str]]:\n    assert len(sources) == len(masked), \"sources and masked length mismatch\"\n    if num_candidates < 1:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    _lazy_load_llm()\n    all_cands: List[List[str]] = []\n\n    for src, msk in tqdm(\n        list(zip(sources, masked)),\n        desc=\"llm infilling\",\n        leave=False,\n    ):\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": INFILL_SYSTEM_PROMPT + \"\\n\\nHere is an example:\\n\" + INFILL_FEW_SHOT,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    f\"Toxic Sentence: {src}\\n\"\n                    f\"Masked Sentence: {msk}\\n\"\n                    f\"Final Output:\"\n                ),\n            },\n        ]\n        try:\n            prompt = _LLM_TOKENIZER.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            prompt = (\n                INFILL_SYSTEM_PROMPT\n                + \"\\n\\nExample:\\n\"\n                + INFILL_FEW_SHOT\n                + \"\\n\\nToxic Sentence: \"\n                + src\n                + \"\\nMasked Sentence: \"\n                + msk\n                + \"\\nFinal Output:\"\n            )\n\n        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n        input_len = inputs[\"input_ids\"].shape[1]\n\n        gen = _LLM_MODEL.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=sample,\n            temperature=float(temperature) if sample else 0.0,\n            top_p=top_p,\n            num_return_sequences=num_candidates,\n            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n        )\n\n        cand_list = []\n        for idx in range(num_candidates):\n            gen_text = _LLM_TOKENIZER.decode(\n                gen[idx][input_len:],\n                skip_special_tokens=True,\n            )\n            cleaned = _extract_bracket_content(gen_text)\n            cleaned = _postprocess_llm_infill(cleaned)\n            if not cleaned:\n                cleaned = src\n            cand_list.append(cleaned)\n\n        all_cands.append(cand_list)\n\n    return all_cands",
   "metadata": {
    "id": "PDRoBhdM8461",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949584,
     "user_tz": 480,
     "elapsed": 33,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "PDRoBhdM8461",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _bool2str(x: bool) -> str:\n    return \"T\" if x else \"F\"\n\ndef _build_run_folder_name(\n    llm_temperature: float,\n    llm_top_p: float,\n    llm_sample: bool,\n    num_candidates: int,\n    max_new_tokens: int,\n    decompx_threshold: float,\n):\n    return (\n        f\"llmtemp{llm_temperature}_topp{llm_top_p}_\"\n        f\"sample{_bool2str(llm_sample)}_\"\n        f\"nc{num_candidates}_\"\n        f\"maxntok{max_new_tokens}_\"\n        f\"dxth{decompx_threshold}\"\n    )",
   "metadata": {
    "id": "SLDKLRVBcKCy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949601,
     "user_tz": 480,
     "elapsed": 15,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "SLDKLRVBcKCy",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_llm_mask_infill_decompx\",\n    echo: bool = False,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    num_candidates: int = 3,\n    llm_temperature: float = 0.7,\n    llm_top_p: float = 0.95,\n    llm_max_new_tokens: int = 64,\n    llm_sample: bool = True,\n    decompx_threshold: float = 0.20,\n    decompx_batch_size_mask: int = 16,\n):\n    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n    cfg = data_configs[data_type].copy()\n\n    if num_candidates < 1:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n    base_out_abs = os.path.join(REPO, base_out_rel)\n    _ensure_dir(base_out_abs)\n\n    original_data_path = cfg[\"data_path\"]\n    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n    _ensure_dir(subset_dir)\n    subset_path = _subset_for_data_type(\n        data_type, original_data_path, num_examples, subset_dir\n    )\n\n    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n    inputs = rx.get_data(args_data)\n    num_inputs = len(inputs)\n\n    if echo:\n        print(\"=\" * 80)\n        print(f\"dataset: {data_type}\")\n        print(f\"subset path: {subset_path}\")\n        print(f\"output base: {base_out_abs}\")\n        print(f\"num examples: {num_inputs}\")\n        print(f\"num_candidates: {num_candidates}\")\n        print(f\"decompx threshold: {decompx_threshold}\")\n        print(\"\\nexample inputs (first 3):\")\n        for i, s in enumerate(inputs[:3]):\n            print(f\"  input[{i}]: {s}\")\n        print(\"=\" * 80)\n\n    mask_dir = \"LLM_Mask_LLM_DecompX\"\n    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n    cur_abs = os.path.join(REPO, cur_rel)\n    _ensure_dir(cur_abs)\n\n    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n\n    if not os.path.exists(masked_file):\n        print(\"running llm masking to create masked_inputs.txt\")\n        masked_inputs = llm_mask_sentences(inputs)\n        masked_inputs = [re.sub(r\"\\s+\", \" \", d).strip() for d in masked_inputs]\n        with open(masked_file, \"w\") as f:\n            for d in masked_inputs:\n                f.write(d + \"\\n\")\n    else:\n        with open(masked_file, \"r\") as f:\n            masked_inputs = [s.strip() for s in f.readlines()]\n        print(\"reusing existing masked_inputs.txt\")\n\n    assert len(masked_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n\n    if echo:\n        print(\"\\nexample llm-masked inputs (first 3):\")\n        for i, m in enumerate(masked_inputs[:3]):\n            print(f\"  masked[{i}]: {m}\")\n\n    run_folder = _build_run_folder_name(\n        llm_temperature=llm_temperature,\n        llm_top_p=llm_top_p,\n        llm_sample=llm_sample,\n        num_candidates=num_candidates,\n        max_new_tokens=llm_max_new_tokens,\n        decompx_threshold=decompx_threshold,\n    )\n    final_abs = os.path.join(cur_abs, run_folder)\n    _ensure_dir(final_abs)\n    orig_txt = os.path.join(final_abs, \"orig.txt\")\n    gen_txt = os.path.join(final_abs, \"gen.txt\")\n\n    if os.path.exists(gen_txt) and not overwrite_gen:\n        print(\"gen already exists at:\", gen_txt, \"— skipping.\")\n        with open(gen_txt, \"r\") as f:\n            best_generations = [s.strip() for s in f.readlines()]\n        if echo:\n            print(\"\\nexample detoxified outputs (first 3):\")\n            for i in range(min(3, len(best_generations))):\n                print(f\"  detox[{i}]: {best_generations[i]}\")\n    else:\n        print(f\"llm infilling: generating {num_candidates} candidates per input (sampling={llm_sample})\")\n        all_candidates = llm_infill_candidates(\n            sources=inputs,\n            masked=masked_inputs,\n            num_candidates=num_candidates,\n            temperature=llm_temperature,\n            top_p=llm_top_p,\n            max_new_tokens=llm_max_new_tokens,\n            sample=llm_sample,\n        )\n\n        global _LLM_MODEL, _LLM_TOKENIZER\n        try:\n            del _LLM_MODEL\n            del _LLM_TOKENIZER\n        except Exception:\n            pass\n        _LLM_MODEL = None\n        _LLM_TOKENIZER = None\n        if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n            torch.cuda.empty_cache()\n\n        print(f\"decompx reranking (threshold={decompx_threshold:.2f})\")\n        best_idx, details = rerank_candidates_decompx(\n            sources=inputs,\n            candidates=all_candidates,\n            threshold=decompx_threshold,\n            batch_size_mask=decompx_batch_size_mask,\n        )\n        best_generations = [\n            all_candidates[i][best_idx[i]] for i in range(len(inputs))\n        ]\n\n        with open(orig_txt, \"w\") as f:\n            for l in inputs:\n                f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n        with open(gen_txt, \"w\") as f:\n            for l in best_generations:\n                f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n\n        print(\"saved:\", orig_txt)\n        print(\"saved:\", gen_txt)\n\n        if echo:\n            print(\"\\nexample detoxified outputs (first 3):\")\n            for i in range(min(3, len(best_generations))):\n                print(f\"  detox[{i}]: {best_generations[i]}\")\n\n    if run_eval:\n        base_path = os.path.join(base_out_abs, data_type, mask_dir)\n        _eval_with_toxicity(\n            base_path,\n            overwrite_eval=overwrite_eval,\n            skip_ref=skip_ref_eval,\n            tox_threshold=0.5,\n            tox_batch_size=32,\n        )\n        _aggregate_eval_csv(\n            output_folder,\n            data_type,\n            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n        )\n\n        if echo:\n            stats_path = os.path.join(final_abs, \"gen_stats.txt\")\n            if os.path.exists(stats_path):\n                stats = _read_stats_file(stats_path)\n                print(\"\\neval metrics for this run:\")\n                metric_keys = [\n                    (\"bertscore\", \"bertscore\"),\n                    (\"meaningbert\", \"meaningbert\"),\n                    (\"bleu4\", \"bleu-4\"),\n                    (\"perplexity gen\", \"perplexity (gen)\"),\n                    (\"perplexity orig\", \"perplexity (orig)\"),\n                    (\"toxicity gen\", \"toxicity (gen)\"),\n                    (\"toxicity orig\", \"toxicity (orig)\"),\n                ]\n                for key, label in metric_keys:\n                    val = stats.get(key, None)\n                    if isinstance(val, float) and math.isnan(val):\n                        continue\n                    if val is None:\n                        continue\n                    print(f\"  {label}: {val:.4f}\")\n            else:\n                print(\"\\ngen_stats.txt not found\")",
   "metadata": {
    "id": "U5oOUWRYuA6E",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949606,
     "user_tz": 480,
     "elapsed": 2,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "U5oOUWRYuA6E",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# detoxify(\n#     data_type=\"paradetox\",\n#     output_folder=\"colab_run_llm_mask_infill_decompx_demo_50_examples\",\n#     echo=True,\n#     num_examples=50,\n#     run_eval=True,\n#     overwrite_gen=True,\n#     overwrite_eval=True,\n#     skip_ref_eval=False,\n#     num_candidates=10,\n#     llm_temperature=0.7,\n#     llm_top_p=0.95,\n#     llm_max_new_tokens=64,\n#     llm_sample=True,\n#     decompx_threshold=0.20,\n#     decompx_batch_size_mask=16,\n# )",
   "metadata": {
    "id": "u5LlySYquA9g",
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764559949611,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "u5LlySYquA9g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "detoxify(\n",
    "    data_type=\"paradetox\",\n",
    "    output_folder=\"XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\",\n",
    "    echo=True,\n",
    "    num_examples=1000,\n",
    "    run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n",
    "    overwrite_gen=True,\n",
    "    overwrite_eval=True,\n",
    "    skip_ref_eval=False,\n",
    "    num_candidates=10,         # LLM candidates per input\n",
    "    llm_temperature=0.7,\n",
    "    llm_top_p=0.95,\n",
    "    llm_max_new_tokens=64,\n",
    "    llm_sample=True,\n",
    "    decompx_threshold=0.20,\n",
    "    decompx_batch_size_mask=16,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934,
     "referenced_widgets": [
      "e62b3afb1b0e4c0bbc2e3a8d8738e9e6",
      "6d9a38d293e943259991bd488a1cda2f",
      "ca111dd05a2d4d04953ebc54ccf52969",
      "6b210ff8485149e0a3e3a90f390e8cf4",
      "695084a4635447b8a22a1462ac8c6ac7",
      "1e75d69a538f425b8ddcb0e0a79e0fe6",
      "00ffaf237ac94816a381c5f1ad6c811e",
      "db19e2602e544d0893b1019063f038c6",
      "5a7faeeded5444e1891b25d14d580cbe",
      "608a4af3cf0d43049a1930cff71a32fa",
      "03ce31b14a284e8a8b06c471ff710a57",
      "faef3457741e4f5ea2a17df747bbf85c",
      "f9bf9567d04f43279ead89ad756030d0",
      "1db1ff1a90534808aea9d3592247f1be",
      "09f17fee7c424d6f9e1cc0960346f639",
      "09485d3698f54ecbb98eb2fcacc8fb76",
      "3ebbea1c5a4f471db630f5520ec9e71b",
      "e95863c631dd4f6aa5c87d2ab923596f",
      "4e754d049cf2423fa7860995623cefca",
      "76f12d8d682b4ea98e98fd5c61281297",
      "982534d299bf42509d629f72eed49719",
      "6eb68fbf44814b1abfe43b2fbe8f6e14",
      "034f40fe7a504ab7b4adbc7567e4b8a0",
      "7e04f5cdac6742aa86173a132f478e03",
      "61ce3aa1ba764caa9ee7bae15c6aec2d",
      "41e4bc673b0745bea490beab43e2c06b",
      "1459507584b7484d84cfaca95b9b2e17",
      "25fa85af1379471b857619b43f145d5a",
      "56b8c8e0aeaa45fb9a61d6f8f0dbaf76",
      "76a82a9cda844e11b94442e9b2f7e84f",
      "834e3510609b45b593e2f281c5cdbd4a",
      "f2bf68bf8a79428899afe37ab431202a",
      "7f20fb69f3074dbbae635563df858469",
      "372c39a0a8274706a20d91eb20e8ca19",
      "488d738942224b1c90992536f7783144",
      "ffecc1166f4e4308a242817c3e4e1fef",
      "fa8b7bfd07484ce8bda7bcec816470c7",
      "20eb2903da9e4b0ca08fbe2d47b561fa",
      "d825002a5cc5456f871726a5072604f8",
      "7780d6d63f2b4901b0c471a58f65a62c",
      "fc0e9441394345c38d0e23dd916c1ffc",
      "7295a112f5cb4b75b9fd30d7ecb6e96f",
      "5a3fc1ded11d46739eb7f4a64254062d",
      "c8823295cb444c87a0bb225584c1fbc4",
      "1fb4df54ecb748dca844f683c38d90d1",
      "4871aef7967c42288555d008cad97899",
      "b50af3c2d3fa4a25ac1288f57409b491",
      "e55d45de473b4c41badf8edcac7ee840",
      "d8f5b27dede14ff0bc32847612b8a12d",
      "a29aae4f8d4f478d9f460bb933a1e39b",
      "d94e0ebe956140fbacc6ed30b2685998",
      "ac1102b44fa54fb4a6606478e96af0d7",
      "f1d6d10dd1f6433fa3dc68c4c069a989",
      "34eb0136fb3e487793f1c574e7d835a6",
      "4458e359953e4cdd81d1b072ae16d149"
     ]
    },
    "id": "O2S9fX-adatU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764563595492,
     "user_tz": 480,
     "elapsed": 3645879,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "9fb2b867-05d7-4aa5-f1a4-4908abbc9a19"
   },
   "id": "O2S9fX-adatU",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "[echo] Dataset: paradetox\n",
      "[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n",
      "[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\n",
      "[echo] Number of examples to detoxify: 671\n",
      "[echo] num_candidates per input: 10\n",
      "[echo] DecompX threshold (reranking): 0.2\n",
      "\n",
      "[echo] Example inputs (first up to 3):\n",
      "  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n",
      "  input[1]: mandated  and \" right fucking now \" would be good .\n",
      "  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n",
      "================================================================================\n",
      "Running LLM masking (Mistral) to create masked_inputs.txt ...\n",
      "Loading LLM (Mistral-7B-Instruct): mistralai/Mistral-7B-Instruct-v0.2 on cuda ...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e62b3afb1b0e4c0bbc2e3a8d8738e9e6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faef3457741e4f5ea2a17df747bbf85c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LLM loaded.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LLM masking (Mistral):   0%|          | 0/671 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "034f40fe7a504ab7b4adbc7567e4b8a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[echo] Example LLM-masked inputs (first up to 3):\n",
      "  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n",
      "  masked[1]: mandated and \"<mask> right <mask> now <mask> \" would be good .\n",
      "  masked[2]: neither of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n",
      "LLM infilling: generating 10 candidates per input (sampling=True)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LLM infilling (Mistral):   0%|          | 0/671 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "372c39a0a8274706a20d91eb20e8ca19"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DecompX reranking (threshold=0.20) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DecompX masking for reranking:   0%|          | 0/420 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fb4df54ecb748dca844f683c38d90d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/orig.txt\n",
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/gen.txt\n",
      "\n",
      "[echo] Example detoxified outputs (first up to 3):\n",
      "  detox[0]: or the loud obnoxious one - thousand ton beast roaring towards you howling its horn.\n",
      "  detox[1]: mandated and \"immediate\" \"soon\" \"would be good\".\n",
      "  detox[2]: neither of my coworkers showed concern when it came time to let Mitch go . ugh.\n",
      "Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/LLM_Mask_LLM_DecompX/llmtemp0.7_topp0.95_sampleT_nc10_maxntok64_dxth0.2/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n",
      "Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/paradetox.csv\n",
      "\n",
      "[echo] Evaluation metrics for this run:\n",
      "  BERTScore: 0.9313\n",
      "  MeaningBERT: 62.5550\n",
      "  BLEU-4: 81.5360\n",
      "  Perplexity (gen): 149.2200\n",
      "  Perplexity (orig): 273.7500\n",
      "  Toxicity (gen): 0.1810\n",
      "  Toxicity (orig): 0.9253\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VgZRbEBED8w4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764563595501,
     "user_tz": 480,
     "elapsed": 4,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "VgZRbEBED8w4",
   "execution_count": 17,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e62b3afb1b0e4c0bbc2e3a8d8738e9e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d9a38d293e943259991bd488a1cda2f",
       "IPY_MODEL_ca111dd05a2d4d04953ebc54ccf52969",
       "IPY_MODEL_6b210ff8485149e0a3e3a90f390e8cf4"
      ],
      "layout": "IPY_MODEL_695084a4635447b8a22a1462ac8c6ac7"
     }
    },
    "6d9a38d293e943259991bd488a1cda2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e75d69a538f425b8ddcb0e0a79e0fe6",
      "placeholder": "​",
      "style": "IPY_MODEL_00ffaf237ac94816a381c5f1ad6c811e",
      "value": "Downloading shards: 100%"
     }
    },
    "ca111dd05a2d4d04953ebc54ccf52969": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db19e2602e544d0893b1019063f038c6",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a7faeeded5444e1891b25d14d580cbe",
      "value": 3
     }
    },
    "6b210ff8485149e0a3e3a90f390e8cf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_608a4af3cf0d43049a1930cff71a32fa",
      "placeholder": "​",
      "style": "IPY_MODEL_03ce31b14a284e8a8b06c471ff710a57",
      "value": " 3/3 [00:02&lt;00:00,  1.15it/s]"
     }
    },
    "695084a4635447b8a22a1462ac8c6ac7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e75d69a538f425b8ddcb0e0a79e0fe6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00ffaf237ac94816a381c5f1ad6c811e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db19e2602e544d0893b1019063f038c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a7faeeded5444e1891b25d14d580cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "608a4af3cf0d43049a1930cff71a32fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03ce31b14a284e8a8b06c471ff710a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "faef3457741e4f5ea2a17df747bbf85c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9bf9567d04f43279ead89ad756030d0",
       "IPY_MODEL_1db1ff1a90534808aea9d3592247f1be",
       "IPY_MODEL_09f17fee7c424d6f9e1cc0960346f639"
      ],
      "layout": "IPY_MODEL_09485d3698f54ecbb98eb2fcacc8fb76"
     }
    },
    "f9bf9567d04f43279ead89ad756030d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ebbea1c5a4f471db630f5520ec9e71b",
      "placeholder": "​",
      "style": "IPY_MODEL_e95863c631dd4f6aa5c87d2ab923596f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1db1ff1a90534808aea9d3592247f1be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e754d049cf2423fa7860995623cefca",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76f12d8d682b4ea98e98fd5c61281297",
      "value": 3
     }
    },
    "09f17fee7c424d6f9e1cc0960346f639": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_982534d299bf42509d629f72eed49719",
      "placeholder": "​",
      "style": "IPY_MODEL_6eb68fbf44814b1abfe43b2fbe8f6e14",
      "value": " 3/3 [03:59&lt;00:00, 75.57s/it]"
     }
    },
    "09485d3698f54ecbb98eb2fcacc8fb76": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ebbea1c5a4f471db630f5520ec9e71b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e95863c631dd4f6aa5c87d2ab923596f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e754d049cf2423fa7860995623cefca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76f12d8d682b4ea98e98fd5c61281297": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "982534d299bf42509d629f72eed49719": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6eb68fbf44814b1abfe43b2fbe8f6e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "034f40fe7a504ab7b4adbc7567e4b8a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e04f5cdac6742aa86173a132f478e03",
       "IPY_MODEL_61ce3aa1ba764caa9ee7bae15c6aec2d",
       "IPY_MODEL_41e4bc673b0745bea490beab43e2c06b"
      ],
      "layout": "IPY_MODEL_1459507584b7484d84cfaca95b9b2e17"
     }
    },
    "7e04f5cdac6742aa86173a132f478e03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25fa85af1379471b857619b43f145d5a",
      "placeholder": "​",
      "style": "IPY_MODEL_56b8c8e0aeaa45fb9a61d6f8f0dbaf76",
      "value": "LLM masking (Mistral): 100%"
     }
    },
    "61ce3aa1ba764caa9ee7bae15c6aec2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76a82a9cda844e11b94442e9b2f7e84f",
      "max": 671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_834e3510609b45b593e2f281c5cdbd4a",
      "value": 671
     }
    },
    "41e4bc673b0745bea490beab43e2c06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2bf68bf8a79428899afe37ab431202a",
      "placeholder": "​",
      "style": "IPY_MODEL_7f20fb69f3074dbbae635563df858469",
      "value": " 671/671 [19:57&lt;00:00,  1.76s/it]"
     }
    },
    "1459507584b7484d84cfaca95b9b2e17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "25fa85af1379471b857619b43f145d5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56b8c8e0aeaa45fb9a61d6f8f0dbaf76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76a82a9cda844e11b94442e9b2f7e84f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "834e3510609b45b593e2f281c5cdbd4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2bf68bf8a79428899afe37ab431202a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f20fb69f3074dbbae635563df858469": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "372c39a0a8274706a20d91eb20e8ca19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_488d738942224b1c90992536f7783144",
       "IPY_MODEL_ffecc1166f4e4308a242817c3e4e1fef",
       "IPY_MODEL_fa8b7bfd07484ce8bda7bcec816470c7"
      ],
      "layout": "IPY_MODEL_20eb2903da9e4b0ca08fbe2d47b561fa"
     }
    },
    "488d738942224b1c90992536f7783144": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d825002a5cc5456f871726a5072604f8",
      "placeholder": "​",
      "style": "IPY_MODEL_7780d6d63f2b4901b0c471a58f65a62c",
      "value": "LLM infilling (Mistral): 100%"
     }
    },
    "ffecc1166f4e4308a242817c3e4e1fef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc0e9441394345c38d0e23dd916c1ffc",
      "max": 671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7295a112f5cb4b75b9fd30d7ecb6e96f",
      "value": 671
     }
    },
    "fa8b7bfd07484ce8bda7bcec816470c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a3fc1ded11d46739eb7f4a64254062d",
      "placeholder": "​",
      "style": "IPY_MODEL_c8823295cb444c87a0bb225584c1fbc4",
      "value": " 671/671 [27:21&lt;00:00,  2.85s/it]"
     }
    },
    "20eb2903da9e4b0ca08fbe2d47b561fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "d825002a5cc5456f871726a5072604f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7780d6d63f2b4901b0c471a58f65a62c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc0e9441394345c38d0e23dd916c1ffc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7295a112f5cb4b75b9fd30d7ecb6e96f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a3fc1ded11d46739eb7f4a64254062d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8823295cb444c87a0bb225584c1fbc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fb4df54ecb748dca844f683c38d90d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4871aef7967c42288555d008cad97899",
       "IPY_MODEL_b50af3c2d3fa4a25ac1288f57409b491",
       "IPY_MODEL_e55d45de473b4c41badf8edcac7ee840"
      ],
      "layout": "IPY_MODEL_d8f5b27dede14ff0bc32847612b8a12d"
     }
    },
    "4871aef7967c42288555d008cad97899": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a29aae4f8d4f478d9f460bb933a1e39b",
      "placeholder": "​",
      "style": "IPY_MODEL_d94e0ebe956140fbacc6ed30b2685998",
      "value": "DecompX masking for reranking: 100%"
     }
    },
    "b50af3c2d3fa4a25ac1288f57409b491": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac1102b44fa54fb4a6606478e96af0d7",
      "max": 420,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1d6d10dd1f6433fa3dc68c4c069a989",
      "value": 420
     }
    },
    "e55d45de473b4c41badf8edcac7ee840": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34eb0136fb3e487793f1c574e7d835a6",
      "placeholder": "​",
      "style": "IPY_MODEL_4458e359953e4cdd81d1b072ae16d149",
      "value": " 420/420 [00:48&lt;00:00, 14.52it/s]"
     }
    },
    "d8f5b27dede14ff0bc32847612b8a12d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "a29aae4f8d4f478d9f460bb933a1e39b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d94e0ebe956140fbacc6ed30b2685998": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac1102b44fa54fb4a6606478e96af0d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1d6d10dd1f6433fa3dc68c4c069a989": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "34eb0136fb3e487793f1c574e7d835a6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4458e359953e4cdd81d1b072ae16d149": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}