{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d4549b",
   "metadata": {
    "id": "82d4549b"
   },
   "source": "# xdetox with llm masking and decompx reranking\n\nThis notebook runs an XDetox pipeline with:\n\n1. **llm masking** using Mistral-7B-Instruct (`mistralai/Mistral-7B-Instruct-v0.2`), which detects toxic spans and replaces them with `<mask>`.\n2. **marco-style generation** (base / expert / anti-expert BART mixture).\n3. **decompx-based reranking** of multiple candidates per input using token-level toxicity importances.\n\nThe goal is to pick, for each toxic input sentence, **one best detoxified candidate** that is:\n\n- As **non-toxic** as possible under DecompX.\n- Still a reasonable rewrite of the original sentence.\n- Grammatically acceptable (enforced by MaRCo, not by a separate fluency model).\n\nUnlike the global-reranking pipeline, this notebook uses **only DecompX** for reranking (no XLM-R, LaBSE, or GPT-2 in the scoring).\n\n---\n\n## decompx-based reranking\n\nReranking uses DecompX on **each candidate sentence** to measure its contribution to toxicity.\n\nFor a candidate sentence $s_j$ with $N_j$ tokens $t_{1,j}, \\dots, t_{N_j,j}$, DecompX assigns a toxicity importance score $\\text{Importance}(t_{i,j})$ to each token $t_{i,j}$.\n\nWe define the DecompX toxicity score of $s_j$ as the sum of these importances:\n\n$$\n\\text{DecompXScore}(s_j) = \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n$$\n\nThe reranking step then selects the candidate with the **lowest** cumulative toxicity:\n\n$$\ns^* = \\arg\\min_{s_j} \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n$$\n\nThis follows Equation (3) from the XDetox paper: the chosen sentence $s^*$ is the one whose tokens have the smallest total contribution to toxicity.\n\n### implementation note\n\nIn the code, DecompX is accessed through `Masker_single`, which:\n\n- Computes token-level importance scores internally.\n- Masks tokens whose importance exceeds a threshold `decompx_threshold` by replacing them with `<mask>`.\n\nTo obtain a scalar toxicity score for reranking, we use the DecompX-masked sentence as a simple, monotonic proxy for the summed importances. Concretely, for each candidate sentence $s_j$:\n\n- Let $\\tilde{s}_j$ be its DecompX-masked version (with some tokens replaced by `<mask>`).\n- We compute\n\n  $$\n  \\widehat{\\text{DecompXScore}}(s_j)\n  = \\frac{\\#\\{\\text{`<mask>' tokens in } \\tilde{s}_j\\}}{\\text{number of tokens in } s_j}\n  $$\n\n- We then choose the candidate with the **smallest** $\\widehat{\\text{DecompXScore}}(s_j)$ for each input.\n\nThis approximates the ideal scoring from the paper: more and/or larger toxic spans (higher cumulative importance) lead to more `<mask>` tokens and thus a higher $\\widehat{\\text{DecompXScore}}$.\n\n---\n\n## llm masking (mistral-7b-instruct)\n\n### prompted masking behavior\n\nMasking is done by a chat-style LLM:\n\n- Model: `mistralai/Mistral-7B-Instruct-v0.2`.\n- The LLM is instructed to:\n  - Identify **toxic, offensive, or profane words or short phrases**.\n  - Replace **each toxic span** with a **single `<mask>` token**.\n  - Allow **multiple `<mask>` tokens** in one sentence (one per toxic span).\n  - If several neighboring words are toxic, **collapse them into one `<mask>`**  \n    (do not output `<mask> <mask>` or longer runs).\n  - Keep **all non-toxic words and punctuation in place**.\n  - **Not** paraphrase, summarize, or reorder the sentence.\n  - Return the masked sentence **inside exactly one pair of brackets**:\n\n    ```text\n    [This is a <mask> example.]\n    ```\n\n### post-processing of llm masks\n\nBecause LLM output is not always perfectly formatted, the notebook cleans the raw masked output before passing it to MaRCo:\n\nFor each LLM output:\n\n1. **Extract bracket content**:\n\n   - Try to read the first `[ ... ]` block.\n   - If there is `[` but no `]`, take everything after the first `[` as the sentence.\n   - If no brackets exist, fall back to the full string.\n\n2. **Strip stray outer brackets** if they still remain.\n\n3. **Normalize whitespace**:\n\n   - Collapse multiple spaces to a single space.\n   - Trim leading and trailing spaces.\n\n4. **Normalize `<mask>` casing and spacing**:\n\n   - Any variant like `<Mask>`, `<MASK>`, `< mask >` is normalized to `<mask>`.\n\n5. **Collapse runs of `<mask>`**:\n\n   - Any sequence like `<mask> <mask> <mask>` is collapsed to a single `<mask>`.\n\n6. If the cleaned sentence becomes empty, fall back to the original masked text.\n\nAll cleaned, LLM-masked sentences are saved once per dataset and output folder:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/masked_inputs.txt\n````\n\nIf this file already exists, it is reused rather than re-calling the LLM.\n\n---\n\n## marco generation (bart base / expert / anti-expert)\n\nAfter LLM masking, generation uses the **MaRCo** setup:\n\n1. **Models**:\n\n   * Base: `facebook/bart-base`\n   * Anti-expert (toxic): `hallisky/bart-base-toxic-antiexpert`\n   * Expert (non-toxic): `hallisky/bart-base-nontoxic-expert`\n\n   These are combined via `rewrite.generation.Infiller`.\n\n2. **inputs**:\n\n   * Original sentences (for reference).\n   * LLM-masked sentences (with `<mask>` tokens), used as infilling prompts.\n\n3. **hyperparameters**:\n\n   * `alpha_a`, `alpha_e`, `alpha_b`: anti-expert, expert, base weights.\n   * `temperature`: sampling temperature.\n   * `top_k_gen`: top-k for sampling.\n   * `top_p`: nucleus sampling on ensembled logits.\n   * `filter_p`: nucleus filter on base logits (advanced).\n   * `rep_penalty`: repetition penalty.\n   * `max_length`: maximum generation length.\n   * `sample`: sampling (`True`) vs greedy (`False`).\n   * `batch_size`: generation batch size.\n\n4. **multiple candidates per input**:\n\n   * The notebook calls `Infiller.generate(...)` **`num_candidates` times**.\n   * Each call produces one candidate per input (with sampling, if enabled).\n   * Candidates are collected as:\n\n     ```python\n     all_candidates[i] = [cand_0, cand_1, ..., cand_{num_candidates-1}]\n     ```\n\n5. **folder structure**:\n\n   Generated outputs go to a run folder whose name encodes the main decoding settings:\n\n   ```text\n   data/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/{run_folder}/\n   ```\n\n   where `{run_folder}` is built from:\n\n   * `alpha_a`, `alpha_e`, `alpha_b`\n   * `base_type`, `antiexpert_type`, `expert_type`\n   * `temperature`, `sample`, `top_k_gen`, `rep_penalty`, `filter_p`, `max_length`, `top_p`\n\nIn each run folder, the notebook writes:\n\n* `orig.txt` — original inputs (one per line).\n* `gen.txt` — DecompX-selected candidates (one per line).\n\n---\n\n## evaluation\n\nIf `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n\n* BERTScore (F1)\n* MeaningBERT\n* BLEU-4\n* Toxicity (orig / gen) using XLM-R\n* Perplexity (orig / gen) using GPT-2\n\nFor each `{run_folder}`, it writes:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/{run_folder}/gen_stats.txt\n```\n\nThen `_aggregate_eval_csv` collects all runs under `LLM_Masking_DecompX/` for that dataset and writes:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n```\n\nThe CSV contains, per run folder:\n\n* `folder`: run folder name (encodes hyperparameters),\n* `threshold`: fixed label `0.20` (kept for compatibility; it is not used for masking here),\n* `bertscore`, `meaningbert`, `bleu4`,\n* `perplexity_gen`, `perplexity_orig`,\n* `toxicity_gen`, `toxicity_orig`.\n\n---\n\n## how to use `detoxify()`\n\nFunction signature:\n\n```python\ndef detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_llm_mask_decompx\",\n    echo: bool = False,\n    batch_size: int = 10,\n    sample: bool = True,\n    top_k_gen: int = 50,\n    top_p: float = 0.95,\n    filter_p: float = 1.0,\n    max_length: int = 128,\n    alpha_a: float = None,\n    alpha_e: float = None,\n    alpha_b: float = 1.0,\n    temperature: float = None,\n    rep_penalty: float = None,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    num_candidates: int = 3,\n    decompx_threshold: float = 0.20,\n)\n```\n\n### key arguments\n\n#### core i/o\n\n* `data_type`: dataset key from `data_configs`, for example:\n\n  * `\"paradetox\"`, `\"dynabench_val\"`, `\"dynabench_test\"`,\n  * `\"jigsaw_toxic\"`, `\"microagressions_val\"`, `\"sbf_val\"`,\n  * `\"appdia_original\"`, `\"appdia_discourse\"`, etc.\n\n* `output_folder`: folder under `data/model_outputs/` where results are stored:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/...\n  ```\n\n* `num_examples`:\n\n  * If set to an integer, only the first `num_examples` examples from the dataset are used.\n  * If `None`, the full dataset is used.\n\n#### llm masking (mistral)\n\n* Masking behavior is controlled by a fixed system prompt and a few-shot example.\n* The notebook caches masked sentences to:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/masked_inputs.txt\n  ```\n\n  and reuses this file when it exists.\n\n#### generation (marco / bart)\n\n* `sample`: `True` for stochastic sampling, `False` for greedy decoding.\n\n* `top_k_gen`: top-k sampling on the ensembled logits.\n\n* `top_p`: nucleus sampling on the ensembled logits.\n\n* `filter_p`: nucleus filter on the base logits (often left at `1.0`).\n\n* `max_length`: maximum sequence length.\n\n* `alpha_a`, `alpha_e`, `alpha_b`:\n\n  * Anti-expert, expert, base mixture weights.\n  * If `None`, defaults are taken from `data_configs[data_type]`.\n\n* `temperature`: sampling temperature (falls back to `data_configs[data_type][\"temperature\"]`).\n\n* `rep_penalty`: repetition penalty (falls back to `data_configs[data_type][\"rep_penalty\"]`).\n\n* `batch_size`: generation batch size.\n\n* `num_candidates`: number of candidates to generate for each input.\n\n#### decompx reranking\n\n* `decompx_threshold`:\n\n  * Threshold used by DecompX when deciding which tokens to mask.\n  * Controls how aggressively tokens are treated as toxic when computing the DecompX-based score.\n  * The candidate with the **lowest** approximate DecompX score is selected.\n\n#### evaluation\n\n* `run_eval`: if `True`, run evaluation and write `gen_stats.txt` plus summary CSV.\n* `overwrite_gen`: if `True`, regenerate outputs even if `gen.txt` exists.\n* `overwrite_eval`: if `True`, recompute evaluation even if `gen_stats.txt` exists.\n* `skip_ref_eval`: if `True`, skip some reference-based evaluation (for example, perplexity on references).\n\n#### echo / debugging\n\n* `echo`: if `True`, print:\n\n  * Dataset and output paths.\n  * Number of examples.\n  * Example inputs.\n  * Example LLM-masked inputs.\n  * Example final detoxified outputs.\n  * Evaluation metrics for this run (if `run_eval=True`).\n\n---\n\n## example calls\n\n### quick sanity check on a small subset\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"colab_run_llm_mask_decompx_demo_50_examples\",\n    echo=True,\n    batch_size=8,\n    sample=True,\n    top_k_gen=50,\n    top_p=0.95,\n    max_length=96,\n    num_examples=50,\n    run_eval=True,\n    overwrite_gen=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n    num_candidates=10,\n    decompx_threshold=0.20,\n)\n```\n\n### larger run (full dataset)\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"paradetox_llm_mask_decompx_full\",\n    echo=True,\n    batch_size=8,\n    sample=True,\n    top_k_gen=50,\n    top_p=0.95,\n    max_length=96,\n    num_examples=None,\n    run_eval=True,\n    overwrite_gen=False,\n    overwrite_eval=False,\n    skip_ref_eval=False,\n    num_candidates=10,\n    decompx_threshold=0.20,\n)\n```\n\nAfter running `detoxify`, you can inspect:\n\n* Final chosen generations:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/{run_folder}/gen.txt\n  ```\n\n* Original inputs:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/{run_folder}/orig.txt\n  ```\n\n* Per-run evaluation metrics:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/LLM_Masking_DecompX/{run_folder}/gen_stats.txt\n  ```\n\n* Aggregated metrics over runs:\n\n  ```text\n  data/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n  ```\n\nThis notebook is designed to be directly comparable to:\n\n* **decompx-masking + decompx-reranking** (original XDetox pipeline), and\n* **llm-masking + global reranking** (toxicity + similarity + fluency),\n\nso you can isolate the effect of **llm-based masking** vs **decompx-based masking**, and compare **decompx-only reranking** to **multi-objective global reranking**."
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive; drive.mount('/content/drive')\n\nimport os, glob, re, sys, json, shutil, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom subprocess import run, PIPE\nimport torch\nimport nltk\nfrom typing import List\n\ncandidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\nprint(\"try mydrive:\", candidate, \"->\", os.path.isdir(candidate))\n\nXDETOX_DIR = candidate\nprint(\"using xdetox_dir:\", XDETOX_DIR)\nassert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\"",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfBoQTrjtynY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495437373,
     "user_tz": 480,
     "elapsed": 901,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "3ef40e72-2d83-495c-83ab-bab1d5ac6427"
   },
   "id": "kfBoQTrjtynY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\nos.makedirs(HF_CACHE, exist_ok=True)\nos.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n\nif XDETOX_DIR not in sys.path:\n    sys.path.append(XDETOX_DIR)\n\nprint(\"xdetox_dir:\", XDETOX_DIR)\nprint(\"transformers_cache:\", HF_CACHE)\nprint(\"cuda available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ITPlTNBtzQx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495437381,
     "user_tz": 480,
     "elapsed": 6,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "f4d2e03d-1c87-44bf-d283-a8761c63b5c4"
   },
   "id": "7ITPlTNBtzQx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\nprint(\"repo folders ok\")\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEy2TGYetzIb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495437394,
     "user_tz": 480,
     "elapsed": 11,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "cbb50c43-13eb-4c0f-adb9-7e9e76e19ddb"
   },
   "id": "MEy2TGYetzIb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip -q install --upgrade pip setuptools wheel\n!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n                sentencepiece\n!pip -q install bert-score",
   "metadata": {
    "id": "GeTzwxVDtzNn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443070,
     "user_tz": 480,
     "elapsed": 5675,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "GeTzwxVDtzNn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import (\n    AutoTokenizer, AutoModelForCausalLM,\n)\nfrom rewrite.generation import Infiller\nfrom rewrite.mask_orig import Masker as Masker_single\nfrom rewrite import rewrite_example as rx\nimport argparse as _argparse\n",
   "metadata": {
    "id": "tfnuR2YVCmW9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443087,
     "user_tz": 480,
     "elapsed": 2,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "tfnuR2YVCmW9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "nltk.download(\"punkt\", quiet=True)\ntry:\n    nltk.download(\"punkt_tab\", quiet=True)\nexcept Exception:\n    pass\nprint(\"nltk ready\")\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0Up7SKstzK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443115,
     "user_tz": 480,
     "elapsed": 26,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "f4980259-f49a-4662-dfac-64d8b8ee7335"
   },
   "id": "y0Up7SKstzK9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data_configs = {\n    \"microagressions_val\": {\n        \"data_path\": \"./datasets/microagressions/val.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"microagressions_test\": {\n        \"data_path\": \"./datasets/microagressions/test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"sbf_val\": {\n        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"sbf_test\": {\n        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"dynabench_val\": {\n        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"dynabench_test\": {\n        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"jigsaw_toxic\": {\n        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"paradetox\": {\n        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_original\": {\n        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_discourse\": {\n        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    }\n}\nprint(\"datasets:\", \", \".join(data_configs.keys()))\n\nREPO = XDETOX_DIR\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nBku39IuAgb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443168,
     "user_tz": 480,
     "elapsed": 51,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "06503649-246e-47f6-92e9-4f9433e1e137"
   },
   "id": "7nBku39IuAgb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _abs_repo_path(rel: str) -> str:\n    return os.path.join(REPO, rel.lstrip(\"./\"))\n\ndef _ensure_dir(p: str):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef _subset_for_data_type(data_type, data_path, n, out_dir):\n    if n is None or n <= 0:\n        return data_path\n\n    src = _abs_repo_path(data_path)\n    _ensure_dir(out_dir)\n\n    if \"microagressions\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"sbf\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"dynabench\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n        if data_path.endswith(\".txt\"):\n            with open(src, \"r\") as f:\n                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n            out = os.path.join(out_dir, os.path.basename(src))\n            with open(out, \"w\") as g:\n                for s in lines[:n]:\n                    g.write(s + \"\\n\")\n            return out\n        elif data_path.endswith(\".csv\"):\n            df = pd.read_csv(src).head(n)\n            out = os.path.join(out_dir, os.path.basename(src))\n            df.to_csv(out, index=False)\n            return out\n\n    if \"appdia\" in data_path:\n        df = pd.read_csv(src, sep=\"\\t\").head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        df.to_csv(out, sep=\"\\t\", index=False)\n        return out\n\n    out = os.path.join(out_dir, os.path.basename(src))\n    shutil.copy(src, out)\n    return out",
   "metadata": {
    "id": "ToytrY0SuAjr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443180,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "ToytrY0SuAjr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _parse_run_folder_name(folder_name):\n    pattern = (\n        r\"aa(\\d+\\.\\d+)_ae(\\d+\\.\\d+)_ab(\\d+\\.\\d+)_base(.*?)_anti(.*?)_expert(.*?)_\"\n        r\"temp(\\d+\\.\\d+)_sample(.*?)_topk(\\d+)_reppenalty(\\d+\\.\\d+)_filterp(\\d+\\.\\d+)_\"\n        r\"maxlength(\\d+)_topp(\\d+\\.\\d+)\"\n    )\n    m = re.match(pattern, folder_name)\n    return bool(m)\n\ndef _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False,\n                        tox_threshold=0.5, tox_batch_size=32):\n    import sys as _sys, os as _os\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n            continue\n        orig_path = os.path.join(gen_dir, \"orig.txt\")\n        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n            continue\n        if os.path.exists(out_stats) and not overwrite_eval:\n            continue\n\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n        cmd = [\n            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n            \"--orig_path\", orig_path,\n            \"--gen_path\",  gen_path,\n            \"--tox_threshold\", str(tox_threshold),\n            \"--tox_batch_size\", str(tox_batch_size),\n        ]\n        if skip_ref:\n            cmd.append(\"--skip_ref\")\n        print(\"eval:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n        if res.returncode != 0:\n            print(res.stdout)\n            print(res.stderr)\n            res.check_returncode()\n\ndef _safe_float(x):\n    try:\n        return float(x)\n    except Exception:\n        return float('nan')\n\ndef _read_stats_file(path):\n    out = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            k, v = line.strip().split(\": \", 1)\n            k = k.replace(\"(skipped)\", \"\").strip().lower()\n            out[k] = _safe_float(v)\n    return out\n\ndef _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n    rows = []\n\n    mask_dir = \"LLM_Masking_DecompX\"\n    base_path = os.path.join(base_out_dir, data_type, mask_dir)\n    if not os.path.isdir(base_path):\n        print(\"no evaluation directory found:\", base_path)\n        return\n\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not os.path.exists(stats_path):\n            continue\n        s = _read_stats_file(stats_path)\n        rows.append({\n            \"threshold\":        0.20,\n            \"folder\":           folder,\n            \"bertscore\":        s.get(\"bertscore\", np.nan),\n            \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n            \"bleu4\":            s.get(\"bleu4\", np.nan),\n            \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n            \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n            \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n            \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n        })\n\n    if rows:\n        cols = [\n            \"threshold\", \"folder\",\n            \"bertscore\", \"meaningbert\", \"bleu4\",\n            \"perplexity_gen\", \"perplexity_orig\",\n            \"toxicity_gen\", \"toxicity_orig\",\n        ]\n        df = pd.DataFrame(rows)\n        df = df[cols]\n        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n        df.to_csv(out_csv, index=False)\n        print(\"wrote summary csv:\", out_csv)\n    else:\n        print(\"no evaluation files found to summarize\")\n",
   "metadata": {
    "id": "u-7I09Uvqb8c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443185,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "u-7I09Uvqb8c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "USE_LLM_GPU = True\nDEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\nprint(\"llm masker device:\", DEVICE_LLM)\n\nLLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n_LLM_TOKENIZER = None\n_LLM_MODEL = None\n\nMASK_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences safer\nby MASKING toxic words with the special token <mask>, while keeping the original sentence\nstructure and wording as much as possible.\n\nYou must behave like a MASKER, not a full rewriter.\n\nYour task:\n1. Identify toxic, offensive, or profane words or short phrases.\n2. For each toxic span, replace the entire span with a single <mask> token.\n3. There may be multiple toxic spans in one sentence, so multiple <mask> tokens are allowed.\n4. If several neighboring words are toxic, you must still use only a single <mask> token\n   in that place. In other words, if you would place \"<mask> <mask>\" or a longer sequence\n   of <mask> tokens, collapse them into a single <mask> so that there are never multiple\n   <mask> tokens in a row.\n5. Do NOT rewrite, paraphrase, or summarize the sentence.\n6. Do NOT add, remove, or reorder non-toxic words or punctuation.\n7. Keep punctuation and spacing as close to the original as possible.\n8. If there is no toxic content, return the sentence unchanged.\n\nOutput rules (format is very strict):\n- ONLY return the final masked sentence inside ONE pair of square brackets, like:\n  [This is a <mask> example.]\n- Do NOT print anything before or after the brackets.\n- Do NOT add explanations, comments, or extra lines.\n- Do NOT include any language tags or metadata.\n- Do NOT include additional '[' or ']' characters inside the sentence.\n\"\"\"\n\nMASK_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\nStep 1 - Identify toxic words: \"stupid idiot\", \"crap\"\nStep 2 - Mask toxic words (do NOT rewrite the rest):\nYou're such a <mask>, nobody wants to hear your <mask>.\nFinal Output: [You're such a <mask>, nobody wants to hear your <mask>.]\"\"\"\n\ndef _lazy_load_llm_masker():\n    global _LLM_MODEL, _LLM_TOKENIZER\n    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n        return\n    print(f\"loading llm masker: {LLM_MODEL_NAME} on {DEVICE_LLM} ...\")\n    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL_NAME,\n        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n        device_map=None,\n    ).to(DEVICE_LLM)\n    _LLM_MODEL.eval()\n    print(\"llm masker loaded\")\n\ndef _extract_bracket_content(text: str) -> str:\n    text = text.strip()\n\n    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n    if m:\n        return m.group(1).strip()\n\n    if \"[\" in text:\n        return text.split(\"[\", 1)[1].strip()\n\n    return text\n\ndef _postprocess_llm_mask(masked_text: str) -> str:\n    s = masked_text.strip()\n\n    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n        s = s[1:-1].strip()\n    else:\n        if s.startswith(\"[\"):\n            s = s[1:].strip()\n        if s.endswith(\"]\"):\n            s = s[:-1].strip()\n\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    s = re.sub(r\"<\\s*mask\\s*>\", \"<mask>\", s, flags=re.IGNORECASE)\n    s = re.sub(r\"(?:\\s*<mask>\\s*){2,}\", \" <mask> \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    if not s:\n        return masked_text.strip()\n    return s\n\n@torch.no_grad()\ndef llm_mask_sentences(sentences: List[str]) -> List[str]:\n    _lazy_load_llm_masker()\n    masked = []\n    for s in tqdm(sentences, desc=\"llm masking\", leave=False):\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": MASK_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + MASK_FEW_SHOT,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Toxic Sentence: {s}\\nFinal Output:\",\n            },\n        ]\n        try:\n            prompt = _LLM_TOKENIZER.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            prompt = (\n                MASK_SYSTEM_PROMPT\n                + \"\\n\\nExample:\\n\"\n                + MASK_FEW_SHOT\n                + \"\\n\\nToxic Sentence: \"\n                + s\n                + \"\\nFinal Output:\"\n            )\n\n        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n        gen = _LLM_MODEL.generate(\n            **inputs,\n            max_new_tokens=64,\n            do_sample=False,\n            temperature=0.0,\n            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n        )\n        gen_text = _LLM_TOKENIZER.decode(\n            gen[0][inputs[\"input_ids\"].shape[1]:],\n            skip_special_tokens=True,\n        )\n\n        masked_text = _extract_bracket_content(gen_text)\n        masked_text = _postprocess_llm_mask(masked_text)\n\n        if not masked_text:\n            masked_text = s\n        masked.append(masked_text)\n\n    return masked",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nG355GLlqcp-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443197,
     "user_tz": 480,
     "elapsed": 10,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "7a0544a1-9fab-4885-a80c-5ec26c93c8ea"
   },
   "id": "nG355GLlqcp-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _decompx_mask_texts(texts: List[str],\n                        threshold: float = 0.20,\n                        batch_size: int = 16) -> List[str]:\n    masker = Masker_single()\n    masked_all = []\n    for i in tqdm(range(0, len(texts), batch_size),\n                  desc=\"decompx masking\", leave=False):\n        batch = texts[i:i + batch_size]\n        batch_out = masker.process_text(sentence=batch, threshold=threshold)\n        masked_all.extend(batch_out)\n    cleaned = [\n        m.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n        for m in masked_all\n    ]\n    masker.release_model()\n    return cleaned\n\ndef _decompx_toxicity_scores(texts: List[str],\n                             threshold: float = 0.20,\n                             batch_size: int = 16) -> np.ndarray:\n    if not texts:\n        return np.zeros((0,), dtype=float)\n\n    masked = _decompx_mask_texts(texts, threshold=threshold, batch_size=batch_size)\n    scores = []\n    for m in masked:\n        num_masks = len(re.findall(r\"<mask>\", m))\n        tokens = m.split()\n        length = max(len(tokens), 1)\n        scores.append(num_masks / length)\n    return np.asarray(scores, dtype=float)\n\ndef rerank_candidates_decompx(\n    sources: List[str],\n    candidates: List[List[str]],\n    threshold: float = 0.20,\n    batch_size_mask: int = 16,\n):\n    N = len(sources)\n    assert len(candidates) == N, \"candidates length mismatch\"\n\n    if N == 0:\n        return np.array([], dtype=int), {}\n\n    C_list = [len(c) for c in candidates]\n    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n    C = C_list[0]\n    if C == 0:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    flat_cands = []\n    flat_src_idx = []\n    for i, cand_list in enumerate(candidates):\n        for cand in cand_list:\n            flat_cands.append(cand)\n            flat_src_idx.append(i)\n    flat_src_idx = np.array(flat_src_idx, dtype=int)\n\n    scores = _decompx_toxicity_scores(\n        flat_cands,\n        threshold=threshold,\n        batch_size=batch_size_mask,\n    )\n\n    scores2 = scores.reshape(N, C)\n    best_idx = np.argmin(scores2, axis=1)\n\n    details = {\n        \"score\": scores2,\n    }\n    return best_idx, details\n",
   "metadata": {
    "id": "dom7rBbguA2u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443201,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "dom7rBbguA2u",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _bool2str(x: bool) -> str:\n    return \"T\" if x else \"F\"\n\ndef _build_gen_folder_name(\n    alpha_a, alpha_e, alpha_b,\n    base_type, antiexpert_type, expert_type,\n    temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n):\n    return (\n        \"aa\" + str(alpha_a) +\n        \"_ae\" + str(alpha_e) +\n        \"_ab\" + str(alpha_b) +\n        \"_base\" + base_type[:5] +\n        \"_anti\" + antiexpert_type[:5] +\n        \"_expert\" + expert_type[:5] +\n        \"_temp\" + str(temperature) +\n        \"_sample\" + _bool2str(sample) +\n        \"_topk\" + str(top_k_gen) +\n        \"_reppenalty\" + str(rep_penalty) +\n        \"_filterp\" + str(filter_p) +\n        \"_maxlength\" + str(max_length) +\n        \"_topp\" + str(top_p)\n    )\n\ndef _run_llm_masking_and_decompx_reranking(\n    data_type,\n    subset_path,\n    base_out_rel,\n    batch_size,\n    alpha_a, alpha_e, alpha_b,\n    temperature,\n    rep_penalty,\n    max_length,\n    top_k_gen,\n    top_p,\n    filter_p,\n    sample,\n    num_candidates,\n    decompx_threshold,\n    overwrite_gen=False,\n    inputs=None,\n):\n    if inputs is None:\n        args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n        inputs = rx.get_data(args_data)\n    print(f\"inputs to detoxify: {len(inputs)}\")\n\n    mask_dir = \"LLM_Masking_DecompX\"\n    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n    cur_abs = os.path.join(REPO, cur_rel)\n    _ensure_dir(cur_abs)\n\n    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n\n    if not os.path.exists(masked_file):\n        print(\"running llm masking to create masked_inputs.txt ...\")\n        decoded_mask_inputs = llm_mask_sentences(inputs)\n        decoded_mask_inputs = [\n            re.sub(r\"\\s+\", \" \", d).strip() for d in decoded_mask_inputs\n        ]\n        with open(masked_file, \"w\") as f:\n            for d in decoded_mask_inputs:\n                f.write(d + \"\\n\")\n\n        global _LLM_MODEL, _LLM_TOKENIZER\n        del _LLM_MODEL\n        del _LLM_TOKENIZER\n        _LLM_MODEL = None\n        _LLM_TOKENIZER = None\n        if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n            torch.cuda.empty_cache()\n    else:\n        with open(masked_file, \"r\") as f:\n            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n        print(\"reusing existing masked_inputs.txt\")\n\n    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n\n    rewriter = Infiller(\n        seed=0,\n        base_path=\"facebook/bart-base\",\n        antiexpert_path=\"hallisky/bart-base-toxic-antiexpert\",\n        expert_path=\"hallisky/bart-base-nontoxic-expert\",\n        base_type=\"base\",\n        antiexpert_type=\"antiexpert\",\n        expert_type=\"expert\",\n        tokenizer=\"facebook/bart-base\",\n    )\n\n    base_type = \"base\"\n    antiexpert_type = \"antiexpert\"\n    expert_type = \"expert\"\n    gen_folder = _build_gen_folder_name(\n        alpha_a, alpha_e, alpha_b,\n        base_type, antiexpert_type, expert_type,\n        temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n    )\n    final_abs = os.path.join(cur_abs, gen_folder)\n    gen_txt = os.path.join(final_abs, \"gen.txt\")\n    orig_txt = os.path.join(final_abs, \"orig.txt\")\n\n    if os.path.exists(gen_txt) and not overwrite_gen:\n        print(\"generation already exists at:\", gen_txt, \"— skipping\")\n        _ensure_dir(final_abs)\n        if not os.path.exists(orig_txt):\n            with open(orig_txt, \"w\") as f:\n                for l in inputs:\n                    f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n\n        with open(masked_file, \"r\") as f:\n            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n        with open(gen_txt, \"r\") as f:\n            best_generations = [s.strip() for s in f.readlines()]\n\n        return inputs, decoded_mask_inputs, best_generations, final_abs\n\n    _ensure_dir(final_abs)\n\n    all_candidates: List[List[str]] = [[] for _ in range(len(inputs))]\n\n    print(f\"generating {num_candidates} candidates per input (sampling={sample})\")\n    for c in range(num_candidates):\n        outs, decoded = rewriter.generate(\n            inputs,\n            decoded_mask_inputs,\n            alpha_a=alpha_a,\n            alpha_e=alpha_e,\n            alpha_b=alpha_b,\n            temperature=temperature,\n            verbose=False,\n            max_length=max_length,\n            repetition_penalty=rep_penalty,\n            p=top_p,\n            filter_p=filter_p,\n            k=top_k_gen,\n            batch_size=batch_size,\n            sample=sample,\n            ranking=False,\n            ranking_eval_output=0,\n        )\n        for i, text in enumerate(decoded):\n            all_candidates[i].append(re.sub(r\"\\s+\", \" \", text).strip())\n\n    del rewriter\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    print(f\"decompx reranking (threshold={decompx_threshold:.2f}) ...\")\n    best_idx, details = rerank_candidates_decompx(\n        sources=inputs,\n        candidates=all_candidates,\n        threshold=decompx_threshold,\n        batch_size_mask=16,\n    )\n    best_generations = [\n        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n    ]\n\n    with open(orig_txt, \"w\") as f:\n        for l in inputs:\n            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n    with open(gen_txt, \"w\") as f:\n        for l in best_generations:\n            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n\n    print(\"saved:\", orig_txt)\n    print(\"saved:\", gen_txt)\n\n    return inputs, decoded_mask_inputs, best_generations, final_abs\n",
   "metadata": {
    "id": "U5oOUWRYuA6E",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443237,
     "user_tz": 480,
     "elapsed": 16,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "U5oOUWRYuA6E",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_llm_mask_decompx\",\n    echo: bool = False,\n    batch_size: int = 10,\n    sample: bool = True,\n    top_k_gen: int = 50,\n    top_p: float = 0.95,\n    filter_p: float = 1.0,\n    max_length: int = 128,\n    alpha_a: float = None,\n    alpha_e: float = None,\n    alpha_b: float = 1.0,\n    temperature: float = None,\n    rep_penalty: float = None,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    num_candidates: int = 3,\n    decompx_threshold: float = 0.20,\n):\n    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n    cfg = data_configs[data_type].copy()\n\n    if num_candidates < 1:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    if alpha_a is None:\n        alpha_a = cfg[\"alpha_a\"]\n    if alpha_e is None:\n        alpha_e = cfg[\"alpha_e\"]\n    if temperature is None:\n        temperature = cfg[\"temperature\"]\n    if rep_penalty is None:\n        rep_penalty = cfg[\"rep_penalty\"]\n\n    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n    base_out_abs = os.path.join(REPO, base_out_rel)\n    _ensure_dir(base_out_abs)\n\n    original_data_path = cfg[\"data_path\"]\n    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n    _ensure_dir(subset_dir)\n    subset_path = _subset_for_data_type(\n        data_type, original_data_path, num_examples, subset_dir\n    )\n\n    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n    inputs = rx.get_data(args_data)\n    num_inputs = len(inputs)\n\n    if echo:\n        print(\"=\" * 80)\n        print(f\"dataset: {data_type}\")\n        print(f\"subset path: {subset_path}\")\n        print(f\"output base: {base_out_abs}\")\n        print(f\"number of examples to detoxify: {num_inputs}\")\n        print(f\"num_candidates per input: {num_candidates}\")\n        print(f\"DecompX threshold (reranking): {decompx_threshold}\")\n        print(\"\\nexample inputs (first up to 3):\")\n        for i, s in enumerate(inputs[:3]):\n            print(f\"  input[{i}]: {s}\")\n        print(\"=\" * 80)\n\n    inputs, masked_inputs, best_generations, run_dir = _run_llm_masking_and_decompx_reranking(\n        data_type=data_type,\n        subset_path=subset_path,\n        base_out_rel=base_out_rel,\n        batch_size=batch_size,\n        alpha_a=alpha_a,\n        alpha_e=alpha_e,\n        alpha_b=alpha_b,\n        temperature=temperature,\n        rep_penalty=rep_penalty,\n        max_length=max_length,\n        top_k_gen=top_k_gen,\n        top_p=top_p,\n        filter_p=filter_p,\n        sample=sample,\n        num_candidates=num_candidates,\n        decompx_threshold=decompx_threshold,\n        overwrite_gen=overwrite_gen,\n        inputs=inputs,\n    )\n\n    if echo:\n        print(\"\\nexample masked inputs (first up to 3):\")\n        for i, m in enumerate(masked_inputs[:3]):\n            print(f\"  masked[{i}]: {m}\")\n\n        print(\"\\nExample detoxified outputs (first up to 3):\")\n        for i in range(min(3, len(best_generations))):\n            print(f\"  detox[{i}]: {best_generations[i]}\")\n\n    if run_eval:\n        mask_dir = \"LLM_Masking_DecompX\"\n        base_path = os.path.join(base_out_abs, data_type, mask_dir)\n        _eval_with_toxicity(\n            base_path,\n            overwrite_eval=overwrite_eval,\n            skip_ref=skip_ref_eval,\n            tox_threshold=0.5,\n            tox_batch_size=32,\n        )\n        _aggregate_eval_csv(\n            output_folder,\n            data_type,\n            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n        )\n\n        if echo:\n            stats_path = os.path.join(run_dir, \"gen_stats.txt\")\n            if os.path.exists(stats_path):\n                stats = _read_stats_file(stats_path)\n                print(\"\\nevaluation metrics for this run:\")\n                metric_keys = [\n                    (\"bertscore\", \"bertscore\"),\n                    (\"meaningbert\", \"meaningbert\"),\n                    (\"bleu4\", \"bleu-4\"),\n                    (\"perplexity gen\", \"perplexity (gen)\"),\n                    (\"perplexity orig\", \"perplexity (orig)\"),\n                    (\"toxicity gen\", \"toxicity (gen)\"),\n                    (\"toxicity orig\", \"toxicity (orig)\"),\n                ]\n                for key, label in metric_keys:\n                    val = stats.get(key, None)\n                    if isinstance(val, float) and math.isnan(val):\n                        continue\n                    if val is None:\n                        continue\n                    print(f\"  {label}: {val:.4f}\")\n            else:\n                print(\"\\ngen_stats.txt not found for this run\")\n\n",
   "metadata": {
    "id": "oqBLx5OSuA72",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443241,
     "user_tz": 480,
     "elapsed": 2,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "oqBLx5OSuA72",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# example: small demo (adjust num_examples as you like)\n# detoxify(\n#     data_type=\"paradetox\",\n#     output_folder=\"colab_run_llm_mask_decompx_demo_50_examples\",\n#     echo=True,\n#     batch_size=8,\n#     sample=True,\n#     top_k_gen=50,\n#     top_p=0.95,\n#     max_length=96,\n#     num_examples=50,\n#     run_eval=True,\n#     overwrite_gen=True,\n#     overwrite_eval=True,\n#     skip_ref_eval=False,\n#     num_candidates=10,\n#     decompx_threshold=0.20,\n# )\n",
   "metadata": {
    "id": "u5LlySYquA9g",
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764495443245,
     "user_tz": 480,
     "elapsed": 2,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "u5LlySYquA9g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "detoxify(\n",
    "    data_type=\"paradetox\",\n",
    "    output_folder=\"XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline\",\n",
    "    echo=True,\n",
    "    batch_size=8,\n",
    "    sample=True,\n",
    "    top_k_gen=50,\n",
    "    top_p=0.95,\n",
    "    max_length=96,\n",
    "    num_examples=1000,\n",
    "    run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n",
    "    overwrite_gen=True,\n",
    "    overwrite_eval=True,\n",
    "    skip_ref_eval=False,\n",
    "    num_candidates=10,\n",
    "    decompx_threshold=0.20\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "89f33a4b202c4097a4180d9afe0f71dc",
      "e7f9d3195bd4497b845ee11bccb6d239",
      "ee8d1cfc37654d7f90c7163a6a6ece7c",
      "2e6efd3e2cf442b5977d806b6e57749b",
      "05de425853de4c9ca020b5bbd7d56ca4",
      "81ee23444d014483b808fa8a7dfc46a4",
      "3968a489aa064872bf7e15992cb5ebab",
      "17917fe7c2f84fe19786dcd365d2e6cf",
      "cb12e606c11249a28c21e7ab38f35b4c",
      "8d22880971a44552838d6db69567635b",
      "9b48e11c8f6449c4955cc073d3fd8dc4",
      "f22e2c9cfdc14bb48ba539013e2a933e",
      "3c24ad8c6e8e4b80bdc32bf6776bd979",
      "b290760ad2604a3fa3778e9aa450c056",
      "9b683256875f4eaaa5d286a3d443fc73",
      "702419e6ee464336bc9e5d208f57e81c",
      "4848ca1611d2458f9663ce72e626523d",
      "a7de07f9f11a4982b44159313b6783f6",
      "2a95ae73e9314dd2947f40b4cf23353f",
      "2c7f799c87224035926dc95f65f85879",
      "8d09a6b907f14ca29113c5898306be31",
      "835b5c32ef154b20af919288e636dfee",
      "e4e4dfc525af4ea9ad335e0eccf7ff42",
      "ccaf1b657abe4106b5e8acb7303411a6",
      "5958720f0930417a871ece842d76f903",
      "44e8120eaddc4a938016d1997314c987",
      "0ffa86df24914a41aa4b19aa28f3a8ea",
      "fe5136607d3846edaf2ee45b1da2c2fe",
      "1a2fe01632434131864227a43a8d7a28",
      "9ff9586d16094542ac00fb5700482d7a",
      "507b9dac8d8b410dbad5fad5dc185250",
      "5e701ca9eba74c11abcb806c8a30aa22",
      "e746b047fc314d64aa6da547c3b5468a",
      "aa0843c02ce947ce892be13f335c9614",
      "e77116e6326247b4b529c2c16b129486",
      "a6c39ebc70c043e68d58b01d35d25b86",
      "0034c8be79934a1bb5c38ba1bb4ddf20",
      "b7d99051522e4543ae708e4f9031fbca",
      "40e61b1a75274b78aa23d0e372239cd9",
      "e258655026fd445cb12150db67a63a92",
      "d1c3d8cebece44f7bfa17659235c415b",
      "5ef0f242e1d1440da81be96bf67a2ef2",
      "cac545d163da429eaa1d28553978b85a",
      "3a1c16ed56084e8c92add673afbd7cff"
     ]
    },
    "id": "O2S9fX-adatU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764498600526,
     "user_tz": 480,
     "elapsed": 3157279,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "9915c4c9-4f5b-415b-b483-2a7ee98884c8"
   },
   "id": "O2S9fX-adatU",
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "[echo] Dataset: paradetox\n",
      "[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n",
      "[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline\n",
      "[echo] Number of examples to detoxify: 671\n",
      "[echo] num_candidates per input: 10\n",
      "[echo] DecompX threshold (reranking): 0.2\n",
      "\n",
      "[echo] Example inputs (first up to 3):\n",
      "  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n",
      "  input[1]: mandated  and \" right fucking now \" would be good .\n",
      "  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n",
      "================================================================================\n",
      "#inputs to detoxify: 671\n",
      "Running LLM masking (Mistral) to create masked_inputs.txt ...\n",
      "Loading LLM masker: mistralai/Mistral-7B-Instruct-v0.2 on cuda ...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89f33a4b202c4097a4180d9afe0f71dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f22e2c9cfdc14bb48ba539013e2a933e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LLM masker loaded.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LLM masking (Mistral):   0%|          | 0/671 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4e4dfc525af4ea9ad335e0eccf7ff42"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1 GPUS!\n",
      "Generating 10 candidates per input (sampling=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Filling in masks: 100%|██████████| 84/84 [01:33<00:00,  1.11s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:40<00:00,  1.19s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:46<00:00,  1.27s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:30<00:00,  1.07s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:51<00:00,  1.33s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:42<00:00,  1.22s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:24<00:00,  1.00s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:43<00:00,  1.23s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:35<00:00,  1.14s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:33<00:00,  1.11s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DecompX reranking (threshold=0.20) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DecompX masking for reranking:   0%|          | 0/420 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa0843c02ce947ce892be13f335c9614"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline/paradetox/LLM_Masking_DecompX/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt\n",
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline/paradetox/LLM_Masking_DecompX/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt\n",
      "\n",
      "[echo] Example masked inputs (first up to 3):\n",
      "  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n",
      "  masked[1]: mandated and \"<mask> right <mask> now <mask> \" would be good .\n",
      "  masked[2]: neither of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n",
      "\n",
      "[echo] Example detoxified outputs (first up to 3):\n",
      "  detox[0]: . or the loud sound of a one- thousand ton beast roaring towards you howling its horn.\n",
      "  detox[1]: mandated and \"right now\" would be good.\n",
      "  detox[2]: neither of my coworkers gave a damn when it came time to ditch mitch. ugh.\n",
      "Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline/paradetox/LLM_Masking_DecompX/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline/paradetox/LLM_Masking_DecompX/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n",
      "Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_DecompX-Reranking_Pipeline/paradetox/paradetox.csv\n",
      "\n",
      "[echo] Evaluation metrics for this run:\n",
      "  BERTScore: 0.9380\n",
      "  MeaningBERT: 69.5530\n",
      "  BLEU-4: 70.0470\n",
      "  Perplexity (gen): 90.6510\n",
      "  Perplexity (orig): 273.7500\n",
      "  Toxicity (gen): 0.2004\n",
      "  Toxicity (orig): 0.9253\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "89f33a4b202c4097a4180d9afe0f71dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7f9d3195bd4497b845ee11bccb6d239",
       "IPY_MODEL_ee8d1cfc37654d7f90c7163a6a6ece7c",
       "IPY_MODEL_2e6efd3e2cf442b5977d806b6e57749b"
      ],
      "layout": "IPY_MODEL_05de425853de4c9ca020b5bbd7d56ca4"
     }
    },
    "e7f9d3195bd4497b845ee11bccb6d239": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81ee23444d014483b808fa8a7dfc46a4",
      "placeholder": "​",
      "style": "IPY_MODEL_3968a489aa064872bf7e15992cb5ebab",
      "value": "Downloading shards: 100%"
     }
    },
    "ee8d1cfc37654d7f90c7163a6a6ece7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17917fe7c2f84fe19786dcd365d2e6cf",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb12e606c11249a28c21e7ab38f35b4c",
      "value": 3
     }
    },
    "2e6efd3e2cf442b5977d806b6e57749b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d22880971a44552838d6db69567635b",
      "placeholder": "​",
      "style": "IPY_MODEL_9b48e11c8f6449c4955cc073d3fd8dc4",
      "value": " 3/3 [00:02&lt;00:00,  1.09it/s]"
     }
    },
    "05de425853de4c9ca020b5bbd7d56ca4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81ee23444d014483b808fa8a7dfc46a4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3968a489aa064872bf7e15992cb5ebab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17917fe7c2f84fe19786dcd365d2e6cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb12e606c11249a28c21e7ab38f35b4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d22880971a44552838d6db69567635b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b48e11c8f6449c4955cc073d3fd8dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f22e2c9cfdc14bb48ba539013e2a933e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c24ad8c6e8e4b80bdc32bf6776bd979",
       "IPY_MODEL_b290760ad2604a3fa3778e9aa450c056",
       "IPY_MODEL_9b683256875f4eaaa5d286a3d443fc73"
      ],
      "layout": "IPY_MODEL_702419e6ee464336bc9e5d208f57e81c"
     }
    },
    "3c24ad8c6e8e4b80bdc32bf6776bd979": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4848ca1611d2458f9663ce72e626523d",
      "placeholder": "​",
      "style": "IPY_MODEL_a7de07f9f11a4982b44159313b6783f6",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b290760ad2604a3fa3778e9aa450c056": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a95ae73e9314dd2947f40b4cf23353f",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c7f799c87224035926dc95f65f85879",
      "value": 3
     }
    },
    "9b683256875f4eaaa5d286a3d443fc73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d09a6b907f14ca29113c5898306be31",
      "placeholder": "​",
      "style": "IPY_MODEL_835b5c32ef154b20af919288e636dfee",
      "value": " 3/3 [04:46&lt;00:00, 89.02s/it]"
     }
    },
    "702419e6ee464336bc9e5d208f57e81c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4848ca1611d2458f9663ce72e626523d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7de07f9f11a4982b44159313b6783f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a95ae73e9314dd2947f40b4cf23353f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c7f799c87224035926dc95f65f85879": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d09a6b907f14ca29113c5898306be31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "835b5c32ef154b20af919288e636dfee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4e4dfc525af4ea9ad335e0eccf7ff42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ccaf1b657abe4106b5e8acb7303411a6",
       "IPY_MODEL_5958720f0930417a871ece842d76f903",
       "IPY_MODEL_44e8120eaddc4a938016d1997314c987"
      ],
      "layout": "IPY_MODEL_0ffa86df24914a41aa4b19aa28f3a8ea"
     }
    },
    "ccaf1b657abe4106b5e8acb7303411a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe5136607d3846edaf2ee45b1da2c2fe",
      "placeholder": "​",
      "style": "IPY_MODEL_1a2fe01632434131864227a43a8d7a28",
      "value": "LLM masking (Mistral): 100%"
     }
    },
    "5958720f0930417a871ece842d76f903": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ff9586d16094542ac00fb5700482d7a",
      "max": 671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_507b9dac8d8b410dbad5fad5dc185250",
      "value": 671
     }
    },
    "44e8120eaddc4a938016d1997314c987": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e701ca9eba74c11abcb806c8a30aa22",
      "placeholder": "​",
      "style": "IPY_MODEL_e746b047fc314d64aa6da547c3b5468a",
      "value": " 671/671 [20:01&lt;00:00,  1.77s/it]"
     }
    },
    "0ffa86df24914a41aa4b19aa28f3a8ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "fe5136607d3846edaf2ee45b1da2c2fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a2fe01632434131864227a43a8d7a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ff9586d16094542ac00fb5700482d7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "507b9dac8d8b410dbad5fad5dc185250": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e701ca9eba74c11abcb806c8a30aa22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e746b047fc314d64aa6da547c3b5468a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa0843c02ce947ce892be13f335c9614": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e77116e6326247b4b529c2c16b129486",
       "IPY_MODEL_a6c39ebc70c043e68d58b01d35d25b86",
       "IPY_MODEL_0034c8be79934a1bb5c38ba1bb4ddf20"
      ],
      "layout": "IPY_MODEL_b7d99051522e4543ae708e4f9031fbca"
     }
    },
    "e77116e6326247b4b529c2c16b129486": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40e61b1a75274b78aa23d0e372239cd9",
      "placeholder": "​",
      "style": "IPY_MODEL_e258655026fd445cb12150db67a63a92",
      "value": "DecompX masking for reranking: 100%"
     }
    },
    "a6c39ebc70c043e68d58b01d35d25b86": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1c3d8cebece44f7bfa17659235c415b",
      "max": 420,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ef0f242e1d1440da81be96bf67a2ef2",
      "value": 420
     }
    },
    "0034c8be79934a1bb5c38ba1bb4ddf20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cac545d163da429eaa1d28553978b85a",
      "placeholder": "​",
      "style": "IPY_MODEL_3a1c16ed56084e8c92add673afbd7cff",
      "value": " 420/420 [00:49&lt;00:00, 13.75it/s]"
     }
    },
    "b7d99051522e4543ae708e4f9031fbca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "40e61b1a75274b78aa23d0e372239cd9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e258655026fd445cb12150db67a63a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1c3d8cebece44f7bfa17659235c415b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ef0f242e1d1440da81be96bf67a2ef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cac545d163da429eaa1d28553978b85a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a1c16ed56084e8c92add673afbd7cff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}