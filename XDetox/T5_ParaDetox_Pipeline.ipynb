{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flSJi4XFC5Lf"
   },
   "source": [
    "# T5-ParaDetox Pipeline\n",
    "This notebook mirrors the XDetox_Pipeline structure for direct comparison:\n",
    "\n",
    "- **Small-batch runs**: choose how many examples to process\n",
    "- **Dataset picker**: run a single dataset or **all**\n",
    "- **Same datasets** as XDetox (paradetox, microagressions, sbf, dynabench, jigsaw, appdia)\n",
    "- **Same evaluation metrics** (BLEU, BERTScore, Perplexity, Toxicity)\n",
    "- **Same output format** (CSV summaries)\n",
    "\n",
    "> **Prereqs**: You have the trained T5 model checkpoint on Drive and datasets available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xEP_aB-C5Lr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26821,
     "status": "ok",
     "timestamp": 1764846464933,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "gZ9T64FgC5Lt",
    "outputId": "16777257-24d7-4d73-eb0f-1b47558b7112"
   },
   "outputs": [],
   "source": "from google.colab import drive; drive.mount('/content/drive')\n\nimport os, sys, torch\n\nPROJECT_BASE = \"/content/drive/MyDrive/w266 - Project\"\nXDETOX_DIR   = os.path.join(PROJECT_BASE, \"XDetox\")\nT5_CHECKPOINT = os.path.join(PROJECT_BASE, \"t5-base-detox-model\")\n\nprint(\"project:\", PROJECT_BASE)\nprint(\"xdetox:\", XDETOX_DIR, \"->\", os.path.isdir(XDETOX_DIR))\nprint(\"checkpoint:\", T5_CHECKPOINT)\n\nassert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\"\nassert os.path.isdir(T5_CHECKPOINT), f\"T5_CHECKPOINT does not exist: {T5_CHECKPOINT}\"\n\nHF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\nos.makedirs(HF_CACHE, exist_ok=True)\nos.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nif XDETOX_DIR not in sys.path:\n    sys.path.append(XDETOX_DIR)\n\nprint(\"cache:\", HF_CACHE)\nprint(\"cuda:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))\n\nREPO = XDETOX_DIR\nDATASET_BASE = REPO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48477,
     "status": "ok",
     "timestamp": 1764846513421,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "6hf-KGmAC5Ly",
    "outputId": "0b379e91-a33c-472c-d7b9-05d871cdf1a5"
   },
   "outputs": [],
   "source": "!pip -q install --upgrade pip setuptools wheel\n!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n                sentencepiece\n!pip -q install bert-score"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17328,
     "status": "ok",
     "timestamp": 1764846530753,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "e7HZtFE9C5L0",
    "outputId": "0a87df62-216d-494a-b134-a867bc223e53"
   },
   "outputs": [],
   "source": "import nltk\nnltk.download(\"punkt\", quiet=True)\ntry:\n    nltk.download(\"punkt_tab\", quiet=True)\nexcept Exception:\n    pass\nprint(\"nltk ok\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7346,
     "status": "ok",
     "timestamp": 1764846538095,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "8pgdv2hxC5L1",
    "outputId": "7b94052e-ee45-4c95-a9d4-011b10eda7a6"
   },
   "outputs": [],
   "source": "import glob, re, json, shutil, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom subprocess import run, PIPE\nfrom typing import List\n\nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n)\n\nprint(\"imports done\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRsbxUmAC5L5"
   },
   "source": "## data config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1764846538174,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "V2-tzreZC5L7",
    "outputId": "e4f04730-e986-40c9-f837-8b6d7adfa625"
   },
   "outputs": [],
   "source": "data_configs = {\n    \"paradetox\": {\n        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n        \"format\": \"txt\",\n    },\n    \"microagressions_test\": {\n        \"data_path\": \"./datasets/microagressions/test.csv\",\n        \"format\": \"csv\",\n    },\n    \"sbf_test\": {\n        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n        \"format\": \"csv\",\n    },\n    \"dynabench_test\": {\n        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n        \"format\": \"csv\",\n    },\n    \"jigsaw_toxic\": {\n        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n        \"format\": \"txt\",\n    },\n    \"appdia_original\": {\n        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n        \"format\": \"tsv\",\n    },\n    \"appdia_discourse\": {\n        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n        \"format\": \"tsv\",\n    },\n}\nprint(f\"{len(data_configs)} datasets\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6_6TXaGC5L9"
   },
   "source": "## helpers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1764846538308,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "B0S8ot-1C5L9",
    "outputId": "98724250-6a09-46a9-c36a-eb74e3aa53ee"
   },
   "outputs": [],
   "source": "def _ensure_dir(p: str):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef load_test_data(data_type: str, num_examples: int = None) -> List[str]:\n    \"\"\"load data\"\"\"\n    if data_type not in data_configs:\n        raise ValueError(f\"Unknown data_type: {data_type}\")\n\n    cfg = data_configs[data_type]\n    data_path = os.path.join(DATASET_BASE, cfg[\"data_path\"].lstrip(\"./\"))\n\n    texts = []\n\n    if cfg[\"format\"] == \"txt\":\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            texts = [line.strip() for line in f if line.strip()]\n\n    elif cfg[\"format\"] == \"csv\":\n        df = pd.read_csv(data_path)\n        if \"text\" in df.columns:\n            texts = df[\"text\"].tolist()\n        elif \"toxic\" in df.columns:\n            texts = df[\"toxic\"].tolist()\n        else:\n            texts = df.iloc[:, 0].tolist()\n\n    elif cfg[\"format\"] == \"tsv\":\n        df = pd.read_csv(data_path, sep=\"\\t\")\n        if \"text\" in df.columns:\n            texts = df[\"text\"].tolist()\n        else:\n            texts = df.iloc[:, 0].tolist()\n\n    cleaned = []\n    for t in texts:\n        if pd.isna(t):\n            continue\n        s = str(t).strip()\n        if s:\n            cleaned.append(s)\n\n    if num_examples and num_examples > 0:\n        cleaned = cleaned[:num_examples]\n\n    return cleaned\n\ndef _safe_float(x):\n    try:\n        return float(x)\n    except Exception:\n        return float(\"nan\")\n\ndef _read_stats_file(path: str) -> dict:\n    out = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            k, v = line.strip().split(\": \", 1)\n            k = k.replace(\"(skipped)\", \"\").strip().lower()\n            out[k] = _safe_float(v)\n    return out\n\nprint(\"helpers loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyHQiqbxC5L_"
   },
   "source": "## t5 model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29593,
     "status": "ok",
     "timestamp": 1764846567905,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "PlPYvKZgC5L_",
    "outputId": "04cc08b4-648f-43ba-d103-04d3c154b842"
   },
   "outputs": [],
   "source": "print(f\"loading from {T5_CHECKPOINT}...\")\n\nt5_tokenizer = T5Tokenizer.from_pretrained(T5_CHECKPOINT)\nt5_model = T5ForConditionalGeneration.from_pretrained(T5_CHECKPOINT)\nt5_model.eval()\n\nDEVICE_T5 = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nt5_model.to(DEVICE_T5)\n\nprint(f\"loaded on {DEVICE_T5}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhxGPScEC5MA"
   },
   "source": "## inference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13308,
     "status": "ok",
     "timestamp": 1764846581201,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "TQWFFFZBC5MB",
    "outputId": "40d5a5fa-b9b5-4c8f-d133-70f604550397"
   },
   "outputs": [],
   "source": "def t5_detoxify_text(\n    text: str,\n    model: T5ForConditionalGeneration,\n    tokenizer: T5Tokenizer,\n    max_length: int = 128,\n    num_beams: int = 5,\n    device: torch.device = DEVICE_T5,\n) -> str:\n    \"\"\"single text\"\"\"\n    input_text = f\"detoxify: {text}\"\n    input_ids = tokenizer.encode(\n        input_text,\n        return_tensors=\"pt\",\n        max_length=max_length,\n        truncation=True,\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids,\n            max_length=max_length,\n            num_beams=num_beams,\n            early_stopping=True,\n            no_repeat_ngram_size=2,\n        )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ndef t5_detoxify_batch(\n    texts: List[str],\n    model: T5ForConditionalGeneration,\n    tokenizer: T5Tokenizer,\n    max_length: int = 128,\n    num_beams: int = 5,\n    batch_size: int = 8,\n    device: torch.device = DEVICE_T5,\n) -> List[str]:\n    \"\"\"batch\"\"\"\n    outputs_all: List[str] = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"T5 gen\"):\n        batch_texts = texts[i:i + batch_size]\n        prompts = [f\"detoxify: {t}\" for t in batch_texts]\n\n        enc = tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            max_length=max_length,\n            truncation=True,\n            padding=True,\n        ).to(device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                **enc,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                no_repeat_ngram_size=2,\n            )\n\n        decoded = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n        outputs_all.extend(decoded)\n\n    return outputs_all\n\n# test\ntest_text = \"This is a stupid idea\"\ndetoxified = t5_detoxify_text(test_text, t5_model, t5_tokenizer, device=DEVICE_T5)\nprint(f\"in: {test_text}\")\nprint(f\"out: {detoxified}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt0wA1kwC5MC"
   },
   "source": "## eval"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1764846581241,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "O6tJqcBJC5MC",
    "outputId": "7fa89469-157c-4623-f270-71ac7d568a44"
   },
   "outputs": [],
   "source": "def _eval_with_toxicity(\n    base_path: str,\n    overwrite_eval: bool = False,\n    skip_ref: bool = False,\n    tox_threshold: float = 0.5,\n    tox_batch_size: int = 32,\n):\n    \"\"\"run evaluate_all on each folder\"\"\"\n    import sys as _sys\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        if not os.path.isdir(gen_dir):\n            continue\n        orig_path = os.path.join(gen_dir, \"orig.txt\")\n        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n            continue\n        if os.path.exists(out_stats) and not overwrite_eval:\n            continue\n\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = REPO + (\n            \":\" + env.get(\"PYTHONPATH\", \"\") if env.get(\"PYTHONPATH\") else \"\"\n        )\n        cmd = [\n            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n            \"--orig_path\", orig_path,\n            \"--gen_path\",  gen_path,\n            \"--tox_threshold\", str(tox_threshold),\n            \"--tox_batch_size\", str(tox_batch_size),\n        ]\n        if skip_ref:\n            cmd.append(\"--skip_ref\")\n        print(\"eval:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n        if res.returncode != 0:\n            print(res.stdout)\n            print(res.stderr)\n            res.check_returncode()\n\ndef _aggregate_eval_csv_baseline(\n    output_folder: str,\n    data_type: str,\n    base_out_dir: str,\n    model_dir: str = \"T5_Baseline\",\n):\n    \"\"\"aggregate metrics\"\"\"\n    rows = []\n\n    base_path = os.path.join(base_out_dir, data_type, model_dir)\n    if not os.path.isdir(base_path):\n        print(\"no eval dir:\", base_path)\n        return\n\n    for folder in os.listdir(base_path):\n        gen_dir    = os.path.join(base_path, folder)\n        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not os.path.exists(stats_path):\n            continue\n        s = _read_stats_file(stats_path)\n        rows.append({\n            \"folder\":          folder,\n            \"bertscore\":       s.get(\"bertscore\", np.nan),\n            \"meaningbert\":     s.get(\"meaningbert\", np.nan),\n            \"bleu4\":           s.get(\"bleu4\", np.nan),\n            \"perplexity_gen\":  s.get(\"perplexity gen\", np.nan),\n            \"perplexity_orig\": s.get(\"perplexity orig\", np.nan),\n            \"toxicity_gen\":    s.get(\"toxicity gen\", np.nan),\n            \"toxicity_orig\":   s.get(\"toxicity orig\", np.nan),\n        })\n\n    if rows:\n        cols = [\n            \"folder\",\n            \"bertscore\", \"meaningbert\", \"bleu4\",\n            \"perplexity_gen\", \"perplexity_orig\",\n            \"toxicity_gen\", \"toxicity_orig\",\n        ]\n        df = pd.DataFrame(rows)\n        df = df[cols]\n        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n        _ensure_dir(os.path.dirname(out_csv))\n        df.to_csv(out_csv, index=False)\n        print(\"wrote csv:\", out_csv)\n    else:\n        print(\"no eval files\")\n\nprint(\"eval helpers ok\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1764846581252,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "GGjZtWDLC5ME",
    "outputId": "5eb126ef-9cdf-45c0-d21f-8bb8bda135e2"
   },
   "outputs": [],
   "source": "def _build_run_folder_name_t5_baseline(\n    max_length: int,\n    num_beams: int,\n) -> str:\n    return f\"t5_baseline_maxlen{max_length}_beams{num_beams}\"\n\nprint(\"folder naming ok\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujgMqtl6C5MF"
   },
   "source": "## detoxify"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1764846581321,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "08ro8C5iC5MF",
    "outputId": "6f9e4eec-01f8-49b1-fb18-58c21224288a"
   },
   "outputs": [],
   "source": "def detoxify_baseline(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"T5_ParaDetox_Pipeline\",\n    echo: bool = False,\n    num_examples: int = 100,\n    batch_size: int = 8,\n    max_length: int = 128,\n    num_beams: int = 5,\n    overwrite_gen: bool = False,\n    run_eval: bool = True,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n):\n    assert data_type in data_configs, f\"unknown: {data_type}\"\n\n    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n    base_out_abs = os.path.join(REPO, base_out_rel)\n    _ensure_dir(base_out_abs)\n\n    print(\"=\" * 60)\n    print(f\"[{data_type}] loading...\")\n    orig_texts = load_test_data(data_type, num_examples)\n    print(f\"  got {len(orig_texts)} examples\")\n\n    if echo:\n        print(\"\\ninputs (first 3):\")\n        for i, s in enumerate(orig_texts[:3]):\n            print(f\"  [{i}]: {s}\")\n\n    model_dir = \"T5_Baseline\"\n    cur_abs = os.path.join(base_out_abs, data_type, model_dir)\n    _ensure_dir(cur_abs)\n\n    run_folder = _build_run_folder_name_t5_baseline(\n        max_length=max_length,\n        num_beams=num_beams,\n    )\n    final_abs = os.path.join(cur_abs, run_folder)\n    _ensure_dir(final_abs)\n\n    orig_path  = os.path.join(final_abs, \"orig.txt\")\n    gen_path   = os.path.join(final_abs, \"gen.txt\")\n    stats_path = os.path.join(final_abs, \"gen_stats.txt\")\n\n    if overwrite_gen or not os.path.exists(gen_path):\n        print(\"  generating...\")\n        generations = t5_detoxify_batch(\n            texts=orig_texts,\n            model=t5_model,\n            tokenizer=t5_tokenizer,\n            max_length=max_length,\n            num_beams=num_beams,\n            batch_size=batch_size,\n            device=DEVICE_T5,\n        )\n\n        if echo:\n            print(\"\\noutputs (first 3):\")\n            for i, g in enumerate(generations[:3]):\n                print(f\"  [{i}]: {g}\")\n\n        with open(orig_path, \"w\") as f:\n            for t in orig_texts:\n                f.write(re.sub(r\"\\s+\", \" \", t).strip() + \"\\n\")\n        with open(gen_path, \"w\") as f:\n            for t in generations:\n                f.write(re.sub(r\"\\s+\", \" \", t).strip() + \"\\n\")\n\n        print(\"  saved to:\", final_abs)\n    else:\n        print(\"  reusing:\", final_abs)\n        with open(orig_path, \"r\") as f:\n            orig_texts = [l.strip() for l in f]\n        with open(gen_path, \"r\") as f:\n            generations = [l.strip() for l in f]\n        print(f\"  loaded {len(generations)} gen\")\n\n    metrics = None\n    if run_eval:\n        base_path = os.path.join(base_out_abs, data_type, model_dir)\n        _eval_with_toxicity(\n            base_path,\n            overwrite_eval=overwrite_eval,\n            skip_ref=skip_ref_eval,\n            tox_threshold=0.5,\n            tox_batch_size=32,\n        )\n        _aggregate_eval_csv_baseline(\n            output_folder,\n            data_type,\n            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n        )\n\n        if os.path.exists(stats_path):\n            metrics = _read_stats_file(stats_path)\n            if echo:\n                print(\"\\nmetrics:\")\n                for k, v in metrics.items():\n                    if isinstance(v, float) and math.isnan(v):\n                        continue\n                    print(f\"  {k}: {v:.4f}\")\n        else:\n            print(\"  no stats file\")\n\n    print(\"=\" * 60)\n    return metrics\n\nprint(\"detoxify_baseline() ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwSyLnQCC5MG"
   },
   "source": "## run"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "oqbPcQgGC5MH"
   },
   "outputs": [],
   "source": "metrics_paradetox = detoxify_baseline(\n    data_type=\"paradetox\",\n    output_folder=\"T5_ParaDetox_Pipeline\",\n    echo=True,\n    num_examples=1000,\n    batch_size=8,\n    max_length=128,\n    num_beams=5,\n    overwrite_gen=True,\n    run_eval=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n)\n\nif metrics_paradetox:\n    print(\"\\nfinal metrics:\")\n    for k, v in metrics_paradetox.items():\n        if isinstance(v, float) and math.isnan(v):\n            continue\n        print(f\"  {k}: {v:.4f}\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03b9de632cc343929917a6a536dcd7f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04850cd7fa954e8586274e9e8a54e5d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "388aea942b0244a39a20a1e51a732011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50308076fbfd4139b829da2fb3b2e3a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "607cec3e5e5741c1aeb0c89079529277": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c0747dd0c2be414a8e5b135e9578fbc9",
       "IPY_MODEL_85361f796f7a4b409e966867a4885a51",
       "IPY_MODEL_6d0e826a57504d6f9281de175c5689af"
      ],
      "layout": "IPY_MODEL_04850cd7fa954e8586274e9e8a54e5d5"
     }
    },
    "614259c7792d4a0383dd6162de437817": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d0e826a57504d6f9281de175c5689af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baf24f04d5a74213b79b057ef6ad767a",
      "placeholder": "​",
      "style": "IPY_MODEL_03b9de632cc343929917a6a536dcd7f0",
      "value": " 84/84 [16:09&lt;00:00,  8.30s/it]"
     }
    },
    "85361f796f7a4b409e966867a4885a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccf2c71f72fd4c5183094353717ebf38",
      "max": 84,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_388aea942b0244a39a20a1e51a732011",
      "value": 84
     }
    },
    "baf24f04d5a74213b79b057ef6ad767a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0747dd0c2be414a8e5b135e9578fbc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_614259c7792d4a0383dd6162de437817",
      "placeholder": "​",
      "style": "IPY_MODEL_50308076fbfd4139b829da2fb3b2e3a4",
      "value": "T5 Generation: 100%"
     }
    },
    "ccf2c71f72fd4c5183094353717ebf38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}