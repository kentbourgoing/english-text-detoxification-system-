{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d4549b",
   "metadata": {
    "id": "82d4549b"
   },
   "source": "# xdetox with llm masking and global reranking\n\nthis notebook runs the full xdetox pipeline with llm masking using mistral-7b-instruct, marco-style generation (bart mixture), and global reranking.\n\n## setup\n\nthe goal is to pick one best detoxified candidate per toxic input that is non-toxic, semantically close, and fluent.\n\nmain difference from decompx pipeline: masking done by llm instead of decompx. no threshold hyperparameter.\n\n## scoring\n\nfor each candidate c:\n- T(c): toxicity [0,1] from xlmr\n- S(c): semantic sim [0,1] from labse  \n- F(c): fluency [0,1] from gpt2 perplexity\n\nconvert toxicity to safety: T'(c) = 1 - T(c)\n\nglobal score: Score(c) = w_T * T'(c) + w_S * S(c) + w_F * F(c)\n\ncontrol weights via `weights = (w_T, w_S, w_F)`\n\nfor each input:\n1. generate num_candidates\n2. score each candidate\n3. select highest scoring\n\n## llm masking\n\nmodel: mistralai/Mistral-7B-Instruct-v0.2\n\nprompted to:\n- identify toxic words/phrases\n- replace each toxic span with single `<mask>`\n- allow multiple masks per sentence\n- collapse neighboring toxic words into one mask\n- keep all non-toxic words and punctuation\n- return masked sentence in brackets: `[This is a <mask> example.]`\n\n### post-processing\n\ncleans llm output:\n1. extract bracket content\n2. strip stray brackets\n3. normalize whitespace\n4. normalize mask tokens (variants -> `<mask>`)\n5. collapse mask runs\n6. fallback to original if empty\n\ncached to: `data/model_outputs/{output_folder}/{data_type}/LLM_Masking/masked_inputs.txt`\n\n## generation\n\nmarco: bart base / expert / anti-expert\n\nfor each dataset:\n\n1. subset selection - first num_examples rows, written to `datasets/_subsets/{data_type}/`\n\n2. llm masking - mistral masks toxic sentences, saves to masked_inputs.txt\n\n3. marco generation\n   - base: facebook/bart-base\n   - anti-expert: hallisky/bart-base-toxic-antiexpert\n   - expert: hallisky/bart-base-nontoxic-expert\n   - controls: alpha_a, alpha_e, alpha_b, temperature, top_k_gen, top_p, filter_p, rep_penalty, max_length, sample\n   - samples num_candidates per input\n\n4. global reranking - scores all candidates with toxicity + similarity + fluency, picks best\n\noutputs stored: `data/model_outputs/{output_folder}/{data_type}/LLM_Masking/{run_folder}/`\n\n## evaluation\n\nif run_eval=True, computes:\n* bertscore (f1)\n* meaningbert  \n* bleu-4\n* toxicity (orig/gen) via xlmr\n* perplexity (orig/gen) via gpt2\n\nwrites gen_stats.txt and summary csv per dataset\n\nthreshold column kept at 0.20 for compatibility (not used for masking)\n\n## usage\n\n```python\ndef detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_llm_mask\",\n    echo: bool = False,\n    batch_size: int = 10,\n    sample: bool = True,\n    top_k_gen: int = 50,\n    top_p: float = 0.95,\n    filter_p: float = 1.0,\n    max_length: int = 128,\n    alpha_a: float = None,\n    alpha_e: float = None,\n    alpha_b: float = 1.0,\n    temperature: float = None,\n    rep_penalty: float = None,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    weights = (0.5, 0.3, 0.2),\n    num_candidates: int = 3,\n)\n```\n\n### key args\n\ndata_type: dataset key (paradetox, dynabench_val, jigsaw_toxic, etc)\noutput_folder: top dir under data/model_outputs/\nnum_examples: subset size (None = full dataset)\n\nmasking: mistral-7b with fixed prompt, no threshold\n\ngeneration: sample, top_k_gen, top_p, filter_p, max_length, alphas, temperature, rep_penalty, batch_size\n\nreranking: weights=(w_T, w_S, w_F), num_candidates\n\neval: run_eval, overwrite_gen, overwrite_eval, skip_ref_eval\n\necho: print dataset info, examples, masked inputs, outputs, metrics\n\n### example calls\n\nquick test:\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"demo\",\n    echo=True,\n    batch_size=8,\n    sample=True,\n    top_k_gen=50,\n    top_p=0.95,\n    max_length=96,\n    num_examples=50,\n    run_eval=True,\n    overwrite_gen=True,\n    overwrite_eval=True,\n    weights=(0.5, 0.3, 0.2),\n    num_candidates=10,\n)\n```\n\nfull run:\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"paradetox_llm\",\n    echo=True,\n    num_examples=None,\n    run_eval=True,\n    weights=(0.5, 0.3, 0.2),\n    num_candidates=20,\n)\n```\n\noutputs:\n- orig.txt, gen.txt\n- gen_stats.txt\n- {data_type}.csv (summary)"
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive; drive.mount('/content/drive')\n\nimport os, glob, re, sys, json, shutil, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom subprocess import run, PIPE\nimport torch\nimport nltk\nfrom typing import List\n\ncandidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\nprint(\"trying MyDrive:\", candidate, \"->\", os.path.isdir(candidate))\n\nXDETOX_DIR = candidate\nprint(\"using XDETOX_DIR:\", XDETOX_DIR)\nassert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\"",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfBoQTrjtynY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492935124,
     "user_tz": 480,
     "elapsed": 36316,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "64c0e7be-5f6f-4296-e752-4c3b5ea1eff6"
   },
   "id": "kfBoQTrjtynY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\nos.makedirs(HF_CACHE, exist_ok=True)\nos.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n\nif XDETOX_DIR not in sys.path:\n    sys.path.append(XDETOX_DIR)\n\nprint(\"xdetox dir:\", XDETOX_DIR)\nprint(\"cache:\", HF_CACHE)\nprint(\"cuda?\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ITPlTNBtzQx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492935968,
     "user_tz": 480,
     "elapsed": 847,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "05953584-44c0-4700-cf16-5f9f46ae5489"
   },
   "id": "7ITPlTNBtzQx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\nprint(\"repo ok\")\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEy2TGYetzIb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492936000,
     "user_tz": 480,
     "elapsed": 28,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "0e22d114-0fc8-4fd4-b76f-f107f932ea02"
   },
   "id": "MEy2TGYetzIb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip -q install --upgrade pip setuptools wheel\n!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n                sentencepiece\n!pip -q install bert-score\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeTzwxVDtzNn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492957973,
     "user_tz": 480,
     "elapsed": 21968,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "a5ce0358-058b-4aa7-c5ac-5d5258a5641e"
   },
   "id": "GeTzwxVDtzNn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import (\n    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n    AutoModelForCausalLM,\n    GPT2LMHeadModel, GPT2TokenizerFast,\n)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfnuR2YVCmW9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492959875,
     "user_tz": 480,
     "elapsed": 1861,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "9e4ef029-6034-49ee-fa50-a51f16a57749"
   },
   "id": "tfnuR2YVCmW9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from rewrite.generation import Infiller\nfrom rewrite import rewrite_example as rx\nimport argparse as _argparse",
   "metadata": {
    "id": "ccJxAWrjA8Qc"
   },
   "id": "ccJxAWrjA8Qc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "nltk.download(\"punkt\", quiet=True)\ntry:\n    nltk.download(\"punkt_tab\", quiet=True)\nexcept Exception:\n    pass\nprint(\"ready\")\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0Up7SKstzK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492970072,
     "user_tz": 480,
     "elapsed": 496,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "53449cc1-22e1-410e-824b-b6e9140437ac"
   },
   "id": "y0Up7SKstzK9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data_configs = {\n    \"microagressions_val\": {\n        \"data_path\": \"./datasets/microagressions/val.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"microagressions_test\": {\n        \"data_path\": \"./datasets/microagressions/test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"sbf_val\": {\n        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"sbf_test\": {\n        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"dynabench_val\": {\n        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"dynabench_test\": {\n        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"jigsaw_toxic\": {\n        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"paradetox\": {\n        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_original\": {\n        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_discourse\": {\n        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    }\n}\nprint(\"datasets:\", \", \".join(data_configs.keys()))\n\nREPO = XDETOX_DIR\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nBku39IuAgb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492970107,
     "user_tz": 480,
     "elapsed": 23,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "2b6514f1-b085-4b20-c05d-e177aa93da9c"
   },
   "id": "7nBku39IuAgb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _abs_repo_path(rel: str) -> str:\n    return os.path.join(REPO, rel.lstrip(\"./\"))\n\ndef _ensure_dir(p: str):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef _subset_for_data_type(data_type, data_path, n, out_dir):\n    \"\"\"subset data\"\"\"\n    if n is None or n <= 0:\n        return data_path\n\n    src = _abs_repo_path(data_path)\n    _ensure_dir(out_dir)\n\n    if \"microagressions\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"sbf\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"dynabench\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n        if data_path.endswith(\".txt\"):\n            with open(src, \"r\") as f:\n                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n            out = os.path.join(out_dir, os.path.basename(src))\n            with open(out, \"w\") as g:\n                for s in lines[:n]:\n                    g.write(s + \"\\n\")\n            return out\n        elif data_path.endswith(\".csv\"):\n            df = pd.read_csv(src).head(n)\n            out = os.path.join(out_dir, os.path.basename(src))\n            df.to_csv(out, index=False)\n            return out\n\n    if \"appdia\" in data_path:\n        df = pd.read_csv(src, sep=\"\\t\").head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        df.to_csv(out, sep=\"\\t\", index=False)\n        return out\n\n    out = os.path.join(out_dir, os.path.basename(src))\n    shutil.copy(src, out)\n    return out",
   "metadata": {
    "id": "ToytrY0SuAjr"
   },
   "id": "ToytrY0SuAjr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "DEVICE_SCORE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"scoring device: {DEVICE_SCORE}\")\n\n_TOX_MODEL_NAME = \"textdetox/xlmr-large-toxicity-classifier-v2\"\n_TOX_TOKENIZER = None\n_TOX_MODEL = None\n\ndef _lazy_load_tox():\n    global _TOX_TOKENIZER, _TOX_MODEL\n    if _TOX_TOKENIZER is None or _TOX_MODEL is None:\n        _TOX_TOKENIZER = AutoTokenizer.from_pretrained(_TOX_MODEL_NAME)\n        _TOX_MODEL = AutoModelForSequenceClassification.from_pretrained(\n            _TOX_MODEL_NAME\n        ).to(DEVICE_SCORE)\n        _TOX_MODEL.eval()\n\n@torch.no_grad()\ndef get_toxicity_scores(texts, batch_size=32):\n    _lazy_load_tox()\n    scores = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Toxicity\", leave=False):\n        batch = texts[i:i+batch_size]\n        enc = _TOX_TOKENIZER(\n            batch, return_tensors=\"pt\",\n            truncation=True, max_length=512, padding=True\n        ).to(DEVICE_SCORE)\n        logits = _TOX_MODEL(**enc).logits\n        probs = torch.softmax(logits, dim=-1)\n        scores.extend(probs[:, 1].detach().cpu().tolist())\n    return scores\n\n_LABSE_NAME = \"sentence-transformers/LaBSE\"\n_LABSE_TOKENIZER = None\n_LABSE_MODEL = None\n\ndef _lazy_load_labse():\n    global _LABSE_TOKENIZER, _LABSE_MODEL\n    if _LABSE_TOKENIZER is None or _LABSE_MODEL is None:\n        _LABSE_TOKENIZER = AutoTokenizer.from_pretrained(_LABSE_NAME)\n        _LABSE_MODEL = AutoModel.from_pretrained(_LABSE_NAME).to(DEVICE_SCORE)\n        _LABSE_MODEL.eval()\n\n@torch.no_grad()\ndef get_labse_embeddings(texts, batch_size=32):\n    _lazy_load_labse()\n    embs = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"LaBSE embeddings\", leave=False):\n        batch = texts[i:i+batch_size]\n        enc = _LABSE_TOKENIZER(\n            batch, return_tensors=\"pt\",\n            truncation=True, max_length=256, padding=True\n        ).to(DEVICE_SCORE)\n        outputs = _LABSE_MODEL(**enc)\n        hidden = outputs.last_hidden_state\n        mask = enc[\"attention_mask\"].unsqueeze(-1)\n        masked = hidden * mask\n        summed = masked.sum(dim=1)\n        counts = mask.sum(dim=1).clamp(min=1e-6)\n        sent_emb = (summed / counts).cpu().numpy()\n        embs.append(sent_emb)\n    if not embs:\n        return np.zeros((0, 768), dtype=np.float32)\n    return np.vstack(embs)\n\n_GPT2_NAME = \"gpt2\"\n_GPT2_TOKENIZER = None\n_GPT2_MODEL = None\n\ndef _lazy_load_gpt2():\n    global _GPT2_TOKENIZER, _GPT2_MODEL\n    if _GPT2_TOKENIZER is None or _GPT2_MODEL is None:\n        _GPT2_TOKENIZER = GPT2TokenizerFast.from_pretrained(_GPT2_NAME)\n        _GPT2_MODEL = GPT2LMHeadModel.from_pretrained(_GPT2_NAME).to(DEVICE_SCORE)\n        _GPT2_MODEL.eval()\n\n@torch.no_grad()\ndef get_gpt2_perplexities(texts):\n    import math as _math\n    _lazy_load_gpt2()\n    ppls = []\n    for s in tqdm(texts, desc=\"GPT-2 PPL\", leave=False):\n        enc = _GPT2_TOKENIZER(s, return_tensors=\"pt\").to(DEVICE_SCORE)\n        out = _GPT2_MODEL(enc[\"input_ids\"], labels=enc[\"input_ids\"])\n        ppl = _math.exp(out.loss.item())\n        if ppl > 1e4:\n            ppl = 1e4\n        ppls.append(float(ppl))\n    return ppls\n\ndef perplexity_to_fluency(ppls, p_min=5.0, p_max=300.0):\n    import math as _math\n    ppls = np.asarray(ppls, dtype=float)\n    p = np.clip(ppls, p_min, p_max)\n    log_p = np.log(p)\n    log_min = _math.log(p_min)\n    log_max = _math.log(p_max)\n    F = (log_max - log_p) / (log_max - log_min + 1e-8)\n    F = np.clip(F, 0.0, 1.0)\n    return F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MxQChqOuAnT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492970141,
     "user_tz": 480,
     "elapsed": 12,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "0013c80f-7383-47de-b334-0bb7995c28f1"
   },
   "id": "-MxQChqOuAnT",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _parse_run_folder_name(folder_name):\n    pattern = r\"aa(\\d+\\.\\d+)_ae(\\d+\\.\\d+)_ab(\\d+\\.\\d+)_base(.*?)_anti(.*?)_expert(.*?)_temp(\\d+\\.\\d+)_sample(.*?)_topk(\\d+)_reppenalty(\\d+\\.\\d+)_filterp(\\d+\\.\\d+)_maxlength(\\d+)_topp(\\d+\\.\\d+)\"\n    m = re.match(pattern, folder_name)\n    return bool(m)\n\ndef _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False, tox_threshold=0.5, tox_batch_size=32):\n    import sys as _sys, os as _os\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n            continue\n        orig_path = os.path.join(gen_dir, \"orig.txt\")\n        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n            continue\n        if os.path.exists(out_stats) and not overwrite_eval:\n            continue\n\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n        cmd = [\n            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n            \"--orig_path\", orig_path,\n            \"--gen_path\",  gen_path,\n            \"--tox_threshold\", str(tox_threshold),\n            \"--tox_batch_size\", str(tox_batch_size),\n        ]\n        if skip_ref:\n            cmd.append(\"--skip_ref\")\n        print(\"eval:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n        if res.returncode != 0:\n            print(res.stdout)\n            print(res.stderr)\n            res.check_returncode()\n\ndef _safe_float(x):\n    try:\n        return float(x)\n    except Exception:\n        return float('nan')\n\ndef _read_stats_file(path):\n    out = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            k, v = line.strip().split(\": \", 1)\n            k = k.replace(\"(skipped)\", \"\").strip().lower()\n            out[k] = _safe_float(v)\n    return out\n\ndef _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n    \"\"\"aggregate eval metrics\"\"\"\n    rows = []\n\n    mask_dir = \"LLM_Masking\"\n    base_path = os.path.join(base_out_dir, data_type, mask_dir)\n    if not os.path.isdir(base_path):\n        print(\"no eval dir:\", base_path)\n        return\n\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not os.path.exists(stats_path):\n            continue\n        s = _read_stats_file(stats_path)\n        rows.append({\n            \"threshold\":        0.20,\n            \"folder\":           folder,\n            \"bertscore\":        s.get(\"bertscore\", np.nan),\n            \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n            \"bleu4\":            s.get(\"bleu4\", np.nan),\n            \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n            \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n            \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n            \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n        })\n\n    if rows:\n        cols = [\n            \"threshold\", \"folder\",\n            \"bertscore\", \"meaningbert\", \"bleu4\",\n            \"perplexity_gen\", \"perplexity_orig\",\n            \"toxicity_gen\", \"toxicity_orig\",\n        ]\n        df = pd.DataFrame(rows)\n        df = df[cols]\n        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n        df.to_csv(out_csv, index=False)\n        print(\"wrote csv:\", out_csv)\n    else:\n        print(\"no eval files found\")\n",
   "metadata": {
    "id": "u-7I09Uvqb8c"
   },
   "id": "u-7I09Uvqb8c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "USE_LLM_GPU = True\n\nDEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\nprint(\"llm device:\", DEVICE_LLM)\n\nLLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n_LLM_TOKENIZER = None\n_LLM_MODEL = None\n\nMASK_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences safer\nby MASKING toxic words with the special token <mask>, while keeping the original sentence\nstructure and wording as much as possible.\n\nYou must behave like a MASKER, not a full rewriter.\n\nYour task:\n1. Identify toxic, offensive, or profane words or short phrases.\n2. For each toxic span, replace the entire span with a single <mask> token.\n3. There may be multiple toxic spans in one sentence, so multiple <mask> tokens are allowed.\n4. If several neighboring words are toxic, you must still use only a single <mask> token\n   in that place. In other words, if you would place \"<mask> <mask>\" or a longer sequence\n   of <mask> tokens, collapse them into a single <mask> so that there are never multiple\n   <mask> tokens in a row.\n5. Do NOT rewrite, paraphrase, or summarize the sentence.\n6. Do NOT add, remove, or reorder non-toxic words or punctuation.\n7. Keep punctuation and spacing as close to the original as possible.\n8. If there is no toxic content, return the sentence unchanged.\n\nOutput rules (format is very strict):\n- ONLY return the final masked sentence inside ONE pair of square brackets, like:\n  [This is a <mask> example.]\n- Do NOT print anything before or after the brackets.\n- Do NOT add explanations, comments, or extra lines.\n- Do NOT include any language tags or metadata.\n- Do NOT include additional '[' or ']' characters inside the sentence.\n\"\"\"\n\nMASK_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\nStep 1 - Identify toxic words: \"stupid idiot\", \"crap\"\nStep 2 - Mask toxic words (do NOT rewrite the rest):\nYou're such a <mask>, nobody wants to hear your <mask>.\nFinal Output: [You're such a <mask>, nobody wants to hear your <mask>.]\"\"\"\n\n\ndef _lazy_load_llm_masker():\n    global _LLM_MODEL, _LLM_TOKENIZER\n    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n        return\n    print(f\"loading llm: {LLM_MODEL_NAME} on {DEVICE_LLM}\")\n    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL_NAME,\n        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n        device_map=None,\n    ).to(DEVICE_LLM)\n    _LLM_MODEL.eval()\n    print(\"llm ready\")\n\ndef _extract_bracket_content(text: str) -> str:\n    \"\"\"extract brackets\"\"\"\n    text = text.strip()\n\n    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n    if m:\n        return m.group(1).strip()\n\n    if \"[\" in text:\n        return text.split(\"[\", 1)[1].strip()\n\n    return text\n\ndef _postprocess_llm_mask(masked_text: str) -> str:\n    \"\"\"clean mask output\"\"\"\n    s = masked_text.strip()\n\n    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n        s = s[1:-1].strip()\n    else:\n        if s.startswith(\"[\"):\n            s = s[1:].strip()\n        if s.endswith(\"]\"):\n            s = s[:-1].strip()\n\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    s = re.sub(r\"<\\s*mask\\s*>\", \"<mask>\", s, flags=re.IGNORECASE)\n    s = re.sub(r\"(?:\\s*<mask>\\s*){2,}\", \" <mask> \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    if not s:\n        return masked_text.strip()\n\n    return s\n\n@torch.no_grad()\ndef llm_mask_sentences(sentences: List[str]) -> List[str]:\n    \"\"\"mistral masking\"\"\"\n    _lazy_load_llm_masker()\n    masked = []\n    for s in tqdm(sentences, desc=\"llm masking\", leave=False):\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": MASK_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + MASK_FEW_SHOT,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Toxic Sentence: {s}\\nFinal Output:\",\n            },\n        ]\n        try:\n            prompt = _LLM_TOKENIZER.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            prompt = (\n                MASK_SYSTEM_PROMPT\n                + \"\\n\\nExample:\\n\"\n                + MASK_FEW_SHOT\n                + \"\\n\\nToxic Sentence: \"\n                + s\n                + \"\\nFinal Output:\"\n            )\n\n        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n        gen = _LLM_MODEL.generate(\n            **inputs,\n            max_new_tokens=64,\n            do_sample=False,\n            temperature=0.0,\n            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n        )\n        gen_text = _LLM_TOKENIZER.decode(\n            gen[0][inputs[\"input_ids\"].shape[1]:],\n            skip_special_tokens=True,\n        )\n\n        masked_text = _extract_bracket_content(gen_text)\n        masked_text = _postprocess_llm_mask(masked_text)\n\n        if not masked_text:\n            masked_text = s\n        masked.append(masked_text)\n\n    return masked",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nG355GLlqcp-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764492970252,
     "user_tz": 480,
     "elapsed": 82,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "bea3ccde-3e57-4eb6-8085-308f48e5a7ea"
   },
   "id": "nG355GLlqcp-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def rerank_candidates_global(\n    sources,\n    candidates,\n    weights=(0.5, 0.3, 0.2),\n    ppl_min=5.0,\n    ppl_max=300.0,\n):\n    \"\"\"global reranking\"\"\"\n    w_T, w_S, w_F = weights\n    N = len(sources)\n    assert len(candidates) == N, \"candidates length mismatch\"\n\n    if N == 0:\n        return np.array([], dtype=int), {}\n\n    C_list = [len(c) for c in candidates]\n    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n    C = C_list[0]\n    if C == 0:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    flat_cands = []\n    flat_src_idx = []\n    for i, cand_list in enumerate(candidates):\n        for cand in cand_list:\n            flat_cands.append(cand)\n            flat_src_idx.append(i)\n    flat_src_idx = np.array(flat_src_idx, dtype=int)\n\n    tox = np.array(get_toxicity_scores(flat_cands), dtype=float)\n\n    src_embs = get_labse_embeddings(sources)\n    cand_embs = get_labse_embeddings(flat_cands)\n    src_embs = src_embs / np.clip(np.linalg.norm(src_embs, axis=1, keepdims=True), 1e-8, None)\n    cand_embs = cand_embs / np.clip(np.linalg.norm(cand_embs, axis=1, keepdims=True), 1e-8, None)\n    sims = np.sum(cand_embs * src_embs[flat_src_idx], axis=1)\n    sims = (sims + 1.0) / 2.0\n\n    ppls = np.array(get_gpt2_perplexities(flat_cands), dtype=float)\n    flus = perplexity_to_fluency(ppls, p_min=ppl_min, p_max=ppl_max)\n\n    safety = 1.0 - tox\n    scores = w_T * safety + w_S * sims + w_F * flus\n\n    tox2     = tox.reshape(N, C)\n    safety2  = safety.reshape(N, C)\n    sims2    = sims.reshape(N, C)\n    flus2    = flus.reshape(N, C)\n    scores2  = scores.reshape(N, C)\n\n    best_idx = scores2.argmax(axis=1)\n    details = {\n        \"tox\": tox2,\n        \"safety\": safety2,\n        \"sim\": sims2,\n        \"flu\": flus2,\n        \"score\": scores2,\n    }\n    return best_idx, details\n",
   "metadata": {
    "id": "dom7rBbguA2u"
   },
   "id": "dom7rBbguA2u",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def rerank_candidates_global(\n    sources,\n    candidates,\n    weights=(0.5, 0.3, 0.2),\n    ppl_min=5.0,\n    ppl_max=300.0,\n):\n    \"\"\"reranking\"\"\"\n    w_T, w_S, w_F = weights\n    N = len(sources)\n    assert len(candidates) == N, \"candidates length mismatch\"\n\n    if N == 0:\n        return np.array([], dtype=int), {}\n\n    C_list = [len(c) for c in candidates]\n    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n    C = C_list[0]\n    if C == 0:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    flat_cands = []\n    flat_src_idx = []\n    for i, cand_list in enumerate(candidates):\n        for cand in cand_list:\n            flat_cands.append(cand)\n            flat_src_idx.append(i)\n    flat_src_idx = np.array(flat_src_idx, dtype=int)\n\n    tox = np.array(get_toxicity_scores(flat_cands), dtype=float)\n    src_embs = get_labse_embeddings(sources)\n    cand_embs = get_labse_embeddings(flat_cands)\n\n    src_embs = src_embs / np.clip(np.linalg.norm(src_embs, axis=1, keepdims=True), 1e-8, None)\n    cand_embs = cand_embs / np.clip(np.linalg.norm(cand_embs, axis=1, keepdims=True), 1e-8, None)\n\n    sims = np.sum(cand_embs * src_embs[flat_src_idx], axis=1)\n    sims = (sims + 1.0) / 2.0\n\n    ppls = np.array(get_gpt2_perplexities(flat_cands), dtype=float)\n    flus = perplexity_to_fluency(ppls, p_min=ppl_min, p_max=ppl_max)\n\n    safety = 1.0 - tox\n    scores = w_T * safety + w_S * sims + w_F * flus\n\n    tox2    = tox.reshape(N, C)\n    safety2 = safety.reshape(N, C)\n    sims2   = sims.reshape(N, C)\n    flus2   = flus.reshape(N, C)\n    scores2 = scores.reshape(N, C)\n\n    best_idx = scores2.argmax(axis=1)\n    details = {\n        \"tox\": tox2,\n        \"safety\": safety2,\n        \"sim\": sims2,\n        \"flu\": flus2,\n        \"score\": scores2,\n    }\n    return best_idx, details\n",
   "metadata": {
    "id": "6FFAXYgJpgSs"
   },
   "id": "6FFAXYgJpgSs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _bool2str(x: bool) -> str:\n    return \"T\" if x else \"F\"\n\ndef _build_gen_folder_name(\n    alpha_a, alpha_e, alpha_b,\n    base_type, antiexpert_type, expert_type,\n    temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n):\n    return (\n        \"aa\" + str(alpha_a) +\n        \"_ae\" + str(alpha_e) +\n        \"_ab\" + str(alpha_b) +\n        \"_base\" + base_type[:5] +\n        \"_anti\" + antiexpert_type[:5] +\n        \"_expert\" + expert_type[:5] +\n        \"_temp\" + str(temperature) +\n        \"_sample\" + _bool2str(sample) +\n        \"_topk\" + str(top_k_gen) +\n        \"_reppenalty\" + str(rep_penalty) +\n        \"_filterp\" + str(filter_p) +\n        \"_maxlength\" + str(max_length) +\n        \"_topp\" + str(top_p)\n    )\n\ndef _run_llm_masking_and_global_reranking_for_threshold(\n    data_type,\n    subset_path,\n    thresh,\n    base_out_rel,\n    batch_size,\n    alpha_a, alpha_e, alpha_b,\n    temperature,\n    rep_penalty,\n    max_length,\n    top_k_gen,\n    top_p,\n    filter_p,\n    sample,\n    num_candidates,\n    weights,\n    overwrite_gen=False,\n    inputs=None,\n):\n    \"\"\"llm masking + marco + reranking\"\"\"\n    if inputs is None:\n        args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n        inputs = rx.get_data(args_data)\n    print(f\"#inputs to detoxify: {len(inputs)}\")\n\n    mask_dir = \"LLM_Masking\"\n    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n    cur_abs = os.path.join(REPO, cur_rel)\n    _ensure_dir(cur_abs)\n\n    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n\n    if not os.path.exists(masked_file):\n        print(\"running llm masking to create masked_inputs.txt\")\n        decoded_mask_inputs = llm_mask_sentences(inputs)\n        decoded_mask_inputs = [\n            re.sub(r\"\\s+\", \" \", d).strip() for d in decoded_mask_inputs\n        ]\n        with open(masked_file, \"w\") as f:\n            for d in decoded_mask_inputs:\n                f.write(d + \"\\n\")\n\n        global _LLM_MODEL, _LLM_TOKENIZER\n        del _LLM_MODEL\n        del _LLM_TOKENIZER\n        _LLM_MODEL = None\n        _LLM_TOKENIZER = None\n        if torch.cuda.is_available() and DEVICE_LLM.type == \"cuda\":\n            torch.cuda.empty_cache()\n    else:\n        with open(masked_file, \"r\") as f:\n            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n        print(\"reusing existing masked_inputs.txt\")\n\n    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n\n    rewriter = Infiller(\n        seed=0,\n        base_path=\"facebook/bart-base\",\n        antiexpert_path=\"hallisky/bart-base-toxic-antiexpert\",\n        expert_path=\"hallisky/bart-base-nontoxic-expert\",\n        base_type=\"base\",\n        antiexpert_type=\"antiexpert\",\n        expert_type=\"expert\",\n        tokenizer=\"facebook/bart-base\",\n    )\n\n    base_type = \"base\"\n    antiexpert_type = \"antiexpert\"\n    expert_type = \"expert\"\n    gen_folder = _build_gen_folder_name(\n        alpha_a, alpha_e, alpha_b,\n        base_type, antiexpert_type, expert_type,\n        temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n    )\n    final_abs = os.path.join(cur_abs, gen_folder)\n    gen_txt = os.path.join(final_abs, \"gen.txt\")\n    orig_txt = os.path.join(final_abs, \"orig.txt\")\n\n    if os.path.exists(gen_txt) and not overwrite_gen:\n        print(\"generation exists at:\", gen_txt, \"— skipping\")\n        _ensure_dir(final_abs)\n        if not os.path.exists(orig_txt):\n            with open(orig_txt, \"w\") as f:\n                for l in inputs:\n                    f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n\n        with open(masked_file, \"r\") as f:\n            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n\n        with open(gen_txt, \"r\") as f:\n            best_generations = [s.strip() for s in f.readlines()]\n\n        return inputs, decoded_mask_inputs, best_generations, final_abs\n\n    _ensure_dir(final_abs)\n\n    all_candidates: List[List[str]] = [[] for _ in range(len(inputs))]\n\n    print(f\"generating {num_candidates} candidates per input (sampling={sample})\")\n    for c in range(num_candidates):\n        outs, decoded = rewriter.generate(\n            inputs,\n            decoded_mask_inputs,\n            alpha_a=alpha_a,\n            alpha_e=alpha_e,\n            alpha_b=alpha_b,\n            temperature=temperature,\n            verbose=False,\n            max_length=max_length,\n            repetition_penalty=rep_penalty,\n            p=top_p,\n            filter_p=filter_p,\n            k=top_k_gen,\n            batch_size=batch_size,\n            sample=sample,\n            ranking=False,\n            ranking_eval_output=0,\n        )\n        for i, text in enumerate(decoded):\n            all_candidates[i].append(re.sub(r\"\\s+\", \" \", text).strip())\n\n    del rewriter\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    print(\"global reranking (tox + sim + flu)\")\n    best_idx, details = rerank_candidates_global(\n        sources=inputs,\n        candidates=all_candidates,\n        weights=weights,\n    )\n    best_generations = [\n        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n    ]\n\n    with open(orig_txt, \"w\") as f:\n        for l in inputs:\n            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n    with open(gen_txt, \"w\") as f:\n        for l in best_generations:\n            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n\n    print(\"saved:\", orig_txt)\n    print(\"saved:\", gen_txt)\n\n    return inputs, decoded_mask_inputs, best_generations, final_abs\n",
   "metadata": {
    "id": "U5oOUWRYuA6E"
   },
   "id": "U5oOUWRYuA6E",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_llm_mask\",\n    echo: bool = False,\n    batch_size: int = 10,\n    sample: bool = True,\n    top_k_gen: int = 50,\n    top_p: float = 0.95,\n    filter_p: float = 1.0,\n    max_length: int = 128,\n    alpha_a: float = None,\n    alpha_e: float = None,\n    alpha_b: float = 1.0,\n    temperature: float = None,\n    rep_penalty: float = None,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    weights = (0.5, 0.3, 0.2),\n    num_candidates: int = 3,\n):\n    \"\"\"xdetox pipeline\"\"\"\n    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n    cfg = data_configs[data_type].copy()\n\n    if num_candidates < 1:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    if alpha_a is None:\n        alpha_a = cfg[\"alpha_a\"]\n    if alpha_e is None:\n        alpha_e = cfg[\"alpha_e\"]\n    if temperature is None:\n        temperature = cfg[\"temperature\"]\n    if rep_penalty is None:\n        rep_penalty = cfg[\"rep_penalty\"]\n\n    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n    base_out_abs = os.path.join(REPO, base_out_rel)\n    _ensure_dir(base_out_abs)\n\n    original_data_path = cfg[\"data_path\"]\n    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n    _ensure_dir(subset_dir)\n    subset_path = _subset_for_data_type(\n        data_type, original_data_path, num_examples, subset_dir\n    )\n\n    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n    inputs = rx.get_data(args_data)\n    num_inputs = len(inputs)\n\n    if echo:\n        print(\"=\" * 80)\n        print(f\"dataset: {data_type}\")\n        print(f\"subset: {subset_path}\")\n        print(f\"output: {base_out_abs}\")\n        print(f\"num examples: {num_inputs}\")\n        print(f\"weights: {weights}\")\n        print(f\"candidates: {num_candidates}\")\n        print(\"\\nexample inputs (first 3):\")\n        for i, s in enumerate(inputs[:3]):\n            print(f\"  input[{i}]: {s}\")\n        print(\"=\" * 80)\n\n    folder_label = 0.20\n\n    inputs, masked_inputs, best_generations, run_dir = _run_llm_masking_and_global_reranking_for_threshold(\n        data_type=data_type,\n        subset_path=subset_path,\n        thresh=folder_label,\n        base_out_rel=base_out_rel,\n        batch_size=batch_size,\n        alpha_a=alpha_a,\n        alpha_e=alpha_e,\n        alpha_b=alpha_b,\n        temperature=temperature,\n        rep_penalty=rep_penalty,\n        max_length=max_length,\n        top_k_gen=top_k_gen,\n        top_p=top_p,\n        filter_p=filter_p,\n        sample=sample,\n        num_candidates=num_candidates,\n        weights=weights,\n        overwrite_gen=overwrite_gen,\n        inputs=inputs,\n    )\n\n    if echo:\n        print(\"\\nmasked inputs (first 3):\")\n        for i, m in enumerate(masked_inputs[:3]):\n            print(f\"  masked[{i}]: {m}\")\n\n        print(\"\\ndetoxified outputs (first 3):\")\n        for i in range(min(3, len(best_generations))):\n            print(f\"  detox[{i}]: {best_generations[i]}\")\n\n    if run_eval:\n        mask_dir = \"LLM_Masking\"\n        base_path = os.path.join(base_out_abs, data_type, mask_dir)\n        _eval_with_toxicity(\n            base_path,\n            overwrite_eval=overwrite_eval,\n            skip_ref=skip_ref_eval,\n            tox_threshold=0.5,\n            tox_batch_size=32,\n        )\n        _aggregate_eval_csv(\n            output_folder,\n            data_type,\n            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n        )\n\n        if echo:\n            stats_path = os.path.join(run_dir, \"gen_stats.txt\")\n            if os.path.exists(stats_path):\n                stats = _read_stats_file(stats_path)\n                print(\"\\neval metrics for this run:\")\n                metric_keys = [\n                    (\"bertscore\", \"bertscore\"),\n                    (\"meaningbert\", \"meaningBERT\"),\n                    (\"bleu4\", \"bleu4\"),\n                    (\"perplexity gen\", \"ppl gen\"),\n                    (\"perplexity orig\", \"ppl orig\"),\n                    (\"toxicity gen\", \"tox gen\"),\n                    (\"toxicity orig\", \"tox orig\"),\n                ]\n                for key, label in metric_keys:\n                    val = stats.get(key, None)\n                    if isinstance(val, float) and math.isnan(val):\n                        continue\n                    if val is None:\n                        continue\n                    print(f\"  {label}: {val:.4f}\")\n            else:\n                print(\"\\ngen_stats.txt not found, no metrics\")\n",
   "metadata": {
    "id": "oqBLx5OSuA72"
   },
   "id": "oqBLx5OSuA72",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# example run\n\n# detoxify(\n#     data_type=\"paradetox\",\n#     output_folder=\"colab_run_llm_mask_demo\",\n#     echo=True,\n#     batch_size=8,\n#     sample=True,\n#     top_k_gen=50,\n#     top_p=0.95,\n#     max_length=96,\n#     num_examples=50,\n#     run_eval=True,\n#     overwrite_gen=False,\n#     overwrite_eval=True,\n#     skip_ref_eval=False,\n#     weights=(0.5, 0.3, 0.2),\n#     num_candidates=10,\n# )\n\n# minimal example (re-run eval only)\n# detoxify(\n#     data_type=\"paradetox\",\n#     output_folder=\"colab_run_llm_mask_demo\",\n#     echo=True,\n#     batch_size=8,\n#     sample=True,\n#     top_k_gen=50,\n#     top_p=0.95,\n#     max_length=96,\n#     num_examples=50,\n#     run_eval=True,\n#     overwrite_gen=False,\n#     overwrite_eval=True,\n#     skip_ref_eval=False,\n#     weights=(0.5, 0.3, 0.2),\n#     num_candidates=10,\n# )",
   "metadata": {
    "id": "u5LlySYquA9g",
    "collapsed": true
   },
   "id": "u5LlySYquA9g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "detoxify(\n",
    "    data_type=\"paradetox\",\n",
    "    output_folder=\"XDetox_w_LLM-Masking_Global-Reranking_Pipeline\",\n",
    "    echo=True,\n",
    "    batch_size=8,\n",
    "    sample=True,\n",
    "    top_k_gen=50,\n",
    "    top_p=0.95,\n",
    "    max_length=96,\n",
    "    num_examples=1000,\n",
    "    run_eval=True,             # BLEU/BERTScore/MeaningBERT/PPL/Toxicity\n",
    "    overwrite_gen=True,\n",
    "    overwrite_eval=True,\n",
    "    skip_ref_eval=False,\n",
    "    weights=(0.5, 0.3, 0.2),\n",
    "    num_candidates=10,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922,
     "referenced_widgets": [
      "3b557a1651f34ece8902ea46d6e64dff",
      "38dbaf51c8ae432e87712c85558ee7ee",
      "6fbbd18b1e224e30b07fff9923cd9665",
      "77cc919598f94441b4eb6575a576d074",
      "04414ca4f3c446d9af7c7c4c7a23806d",
      "2bd08e0957d849adb6389f37455cf884",
      "164ff8b589ff4f93b72ead1ce28a87cb",
      "3e645f483a2344f09153ff338264de9f",
      "66d1dfeeec4141a29add821690a3c6b4",
      "ed207152c87146429db0c9f552f2b0c7",
      "73d7e4445aca43b28cbe260fc85dded5",
      "39acf9142eff48acb3c0adc85409dadd",
      "bda21c35a0604dc282bb992a4ae69d4f",
      "37aa8247d42a40e79ecd3acaf5436af5",
      "0623fca2c03a40deb7cfe139d95307a7",
      "77a154dbce0f428184857bb7b1a5664b",
      "5637a77ac62740b5a84af65f777f5e88",
      "037ea79bec264c829a8353cd5aae7558",
      "e38945e77935434a84c05847752f1e4d",
      "4bdddbc9a775460ab7836a905c8edb5a",
      "c8021c7650484cb387893b3c1d7f7c9c",
      "3a7033a7cbfb436fab547b9a8a1e3cc5",
      "c03848cd81c84c44ba3deb67c5d72f21",
      "c7ca6aa7b6f1402c8a9a1660b933197c",
      "2f47d1db2f424ac1b45e18a733337e0c",
      "a3ff4721839e467aa9860575fcddb4eb",
      "7e5c5619d01243f88a927c75e0983a09",
      "db47a03a2d09419290606398f542eaec",
      "17287c169e9b4437863b96c032f90446",
      "608e2e9059284ad2a497394b87dfb069",
      "abc14cb2fef547a58c11858d107375ed",
      "f413726323d24cf2973b0744416cce93",
      "163b984b74dd4d7595e8982ff54bad1a",
      "dc7224f815cd41cf9047e5f554cf2c07",
      "a2209ba2b76044fcb0566b3457b6a9e1",
      "79f03e1fc7b5470eb5f5588a82578f23",
      "4bb76d636d4e45ec9e63e8807bb26271",
      "0f7f12f1020f4186a835b24a260c1a29",
      "e1eda09817e2438ca9869a1c7d22cc59",
      "786c34ed8f224e07adf8e80481c72b42",
      "b0a74490673649f58a8dcaae2597f182",
      "3e2f290466294c2fb84dd10bc39c93e5",
      "9da9b96db59f4344b770cb6d3bfd5ac5",
      "a33588ab36274d17961de9c15da6005a"
     ]
    },
    "id": "O2S9fX-adatU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764494752543,
     "user_tz": 480,
     "elapsed": 1782234,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "1c1974f3-f975-4ab5-dd2c-1ea16bc3c036"
   },
   "id": "O2S9fX-adatU",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "[echo] Dataset: paradetox\n",
      "[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n",
      "[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline\n",
      "[echo] Number of examples to detoxify: 671\n",
      "[echo] Weights (w_T, w_S, w_F): (0.5, 0.3, 0.2)\n",
      "[echo] num_candidates per input: 10\n",
      "\n",
      "[echo] Example inputs (first up to 3):\n",
      "  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n",
      "  input[1]: mandated  and \" right fucking now \" would be good .\n",
      "  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n",
      "================================================================================\n",
      "#inputs to detoxify: 671\n",
      "Reusing existing masked_inputs.txt\n",
      "Found 1 GPUS!\n",
      "Generating 10 candidates per input (sampling=True)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Filling in masks: 100%|██████████| 84/84 [01:38<00:00,  1.17s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:38<00:00,  1.18s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:44<00:00,  1.25s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:28<00:00,  1.05s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:50<00:00,  1.31s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:41<00:00,  1.21s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:23<00:00,  1.01it/s]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:42<00:00,  1.23s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:35<00:00,  1.14s/it]\n",
      "Filling in masks: 100%|██████████| 84/84 [01:33<00:00,  1.11s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Global reranking (toxicity + similarity + fluency)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Toxicity:   0%|          | 0/210 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b557a1651f34ece8902ea46d6e64dff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LaBSE embeddings:   0%|          | 0/21 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39acf9142eff48acb3c0adc85409dadd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LaBSE embeddings:   0%|          | 0/210 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c03848cd81c84c44ba3deb67c5d72f21"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "GPT-2 PPL:   0%|          | 0/6710 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc7224f815cd41cf9047e5f554cf2c07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt\n",
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt\n",
      "\n",
      "[echo] Example masked inputs (first up to 3):\n",
      "  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n",
      "  masked[1]: mandated and \"<mask> right <mask> now <mask> \" would be good .\n",
      "  masked[2]: neither of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n",
      "\n",
      "[echo] Example detoxified outputs (first up to 3):\n",
      "  detox[0]: . or the loud sound of a one- thousand ton beast roaring towards you howling its horn.\n",
      "  detox[1]: mandated and \"right now\" would be good.\n",
      "  detox[2]: neither of my coworkers gave a hoot about it when it came time to ditch mitch. ugh.\n",
      "Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/LLM_Masking/aa1.5_ae4.75_ab1.0_basebase_antiantie_expertexper_temp2.5_sampleT_topk50_reppenalty1.0_filterp1.0_maxlength96_topp0.95/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n",
      "Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_LLM-Masking_Global-Reranking_Pipeline/paradetox/paradetox.csv\n",
      "\n",
      "[echo] Evaluation metrics for this run:\n",
      "  BERTScore: 0.9379\n",
      "  MeaningBERT: 69.0240\n",
      "  BLEU-4: 70.0470\n",
      "  Perplexity (gen): 86.5870\n",
      "  Perplexity (orig): 273.7500\n",
      "  Toxicity (gen): 0.1593\n",
      "  Toxicity (orig): 0.9253\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VgZRbEBED8w4"
   },
   "id": "VgZRbEBED8w4",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3b557a1651f34ece8902ea46d6e64dff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38dbaf51c8ae432e87712c85558ee7ee",
       "IPY_MODEL_6fbbd18b1e224e30b07fff9923cd9665",
       "IPY_MODEL_77cc919598f94441b4eb6575a576d074"
      ],
      "layout": "IPY_MODEL_04414ca4f3c446d9af7c7c4c7a23806d"
     }
    },
    "38dbaf51c8ae432e87712c85558ee7ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bd08e0957d849adb6389f37455cf884",
      "placeholder": "​",
      "style": "IPY_MODEL_164ff8b589ff4f93b72ead1ce28a87cb",
      "value": "Toxicity: 100%"
     }
    },
    "6fbbd18b1e224e30b07fff9923cd9665": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e645f483a2344f09153ff338264de9f",
      "max": 210,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66d1dfeeec4141a29add821690a3c6b4",
      "value": 210
     }
    },
    "77cc919598f94441b4eb6575a576d074": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed207152c87146429db0c9f552f2b0c7",
      "placeholder": "​",
      "style": "IPY_MODEL_73d7e4445aca43b28cbe260fc85dded5",
      "value": " 210/210 [00:08&lt;00:00, 27.88it/s]"
     }
    },
    "04414ca4f3c446d9af7c7c4c7a23806d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "2bd08e0957d849adb6389f37455cf884": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "164ff8b589ff4f93b72ead1ce28a87cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e645f483a2344f09153ff338264de9f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66d1dfeeec4141a29add821690a3c6b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed207152c87146429db0c9f552f2b0c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d7e4445aca43b28cbe260fc85dded5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39acf9142eff48acb3c0adc85409dadd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bda21c35a0604dc282bb992a4ae69d4f",
       "IPY_MODEL_37aa8247d42a40e79ecd3acaf5436af5",
       "IPY_MODEL_0623fca2c03a40deb7cfe139d95307a7"
      ],
      "layout": "IPY_MODEL_77a154dbce0f428184857bb7b1a5664b"
     }
    },
    "bda21c35a0604dc282bb992a4ae69d4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5637a77ac62740b5a84af65f777f5e88",
      "placeholder": "​",
      "style": "IPY_MODEL_037ea79bec264c829a8353cd5aae7558",
      "value": "LaBSE embeddings:  95%"
     }
    },
    "37aa8247d42a40e79ecd3acaf5436af5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e38945e77935434a84c05847752f1e4d",
      "max": 21,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bdddbc9a775460ab7836a905c8edb5a",
      "value": 21
     }
    },
    "0623fca2c03a40deb7cfe139d95307a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8021c7650484cb387893b3c1d7f7c9c",
      "placeholder": "​",
      "style": "IPY_MODEL_3a7033a7cbfb436fab547b9a8a1e3cc5",
      "value": " 20/21 [00:00&lt;00:00, 65.24it/s]"
     }
    },
    "77a154dbce0f428184857bb7b1a5664b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "5637a77ac62740b5a84af65f777f5e88": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "037ea79bec264c829a8353cd5aae7558": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e38945e77935434a84c05847752f1e4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bdddbc9a775460ab7836a905c8edb5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8021c7650484cb387893b3c1d7f7c9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a7033a7cbfb436fab547b9a8a1e3cc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c03848cd81c84c44ba3deb67c5d72f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7ca6aa7b6f1402c8a9a1660b933197c",
       "IPY_MODEL_2f47d1db2f424ac1b45e18a733337e0c",
       "IPY_MODEL_a3ff4721839e467aa9860575fcddb4eb"
      ],
      "layout": "IPY_MODEL_7e5c5619d01243f88a927c75e0983a09"
     }
    },
    "c7ca6aa7b6f1402c8a9a1660b933197c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db47a03a2d09419290606398f542eaec",
      "placeholder": "​",
      "style": "IPY_MODEL_17287c169e9b4437863b96c032f90446",
      "value": "LaBSE embeddings:  96%"
     }
    },
    "2f47d1db2f424ac1b45e18a733337e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_608e2e9059284ad2a497394b87dfb069",
      "max": 210,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abc14cb2fef547a58c11858d107375ed",
      "value": 210
     }
    },
    "a3ff4721839e467aa9860575fcddb4eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f413726323d24cf2973b0744416cce93",
      "placeholder": "​",
      "style": "IPY_MODEL_163b984b74dd4d7595e8982ff54bad1a",
      "value": " 202/210 [00:02&lt;00:00, 75.87it/s]"
     }
    },
    "7e5c5619d01243f88a927c75e0983a09": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "db47a03a2d09419290606398f542eaec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17287c169e9b4437863b96c032f90446": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "608e2e9059284ad2a497394b87dfb069": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abc14cb2fef547a58c11858d107375ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f413726323d24cf2973b0744416cce93": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163b984b74dd4d7595e8982ff54bad1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc7224f815cd41cf9047e5f554cf2c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2209ba2b76044fcb0566b3457b6a9e1",
       "IPY_MODEL_79f03e1fc7b5470eb5f5588a82578f23",
       "IPY_MODEL_4bb76d636d4e45ec9e63e8807bb26271"
      ],
      "layout": "IPY_MODEL_0f7f12f1020f4186a835b24a260c1a29"
     }
    },
    "a2209ba2b76044fcb0566b3457b6a9e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1eda09817e2438ca9869a1c7d22cc59",
      "placeholder": "​",
      "style": "IPY_MODEL_786c34ed8f224e07adf8e80481c72b42",
      "value": "GPT-2 PPL: 100%"
     }
    },
    "79f03e1fc7b5470eb5f5588a82578f23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0a74490673649f58a8dcaae2597f182",
      "max": 6710,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e2f290466294c2fb84dd10bc39c93e5",
      "value": 6710
     }
    },
    "4bb76d636d4e45ec9e63e8807bb26271": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9da9b96db59f4344b770cb6d3bfd5ac5",
      "placeholder": "​",
      "style": "IPY_MODEL_a33588ab36274d17961de9c15da6005a",
      "value": " 6709/6710 [01:15&lt;00:00, 90.92it/s]"
     }
    },
    "0f7f12f1020f4186a835b24a260c1a29": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "e1eda09817e2438ca9869a1c7d22cc59": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "786c34ed8f224e07adf8e80481c72b42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0a74490673649f58a8dcaae2597f182": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e2f290466294c2fb84dd10bc39c93e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9da9b96db59f4344b770cb6d3bfd5ac5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a33588ab36274d17961de9c15da6005a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}