{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d4549b",
   "metadata": {
    "id": "82d4549b"
   },
   "source": "# xdetox with decompx masking, llm infilling, and decompx reranking\n\nthis notebook runs an xdetox variant with:\n\n1. **decompx masking** (token-level toxicity attribution on roberta).\n2. **llm infilling** using mistral-7b-instruct  \n   (`mistralai/Mistral-7B-Instruct-v0.2`).\n3. **decompx-based reranking** of multiple llm candidates, following the\n   reranking strategy described in *\"xdetox: text detoxification with token-level toxicity explanations\"*.\n\nfor each toxic input sentence, the goal is to pick **one final detoxified candidate** that:\n\n- has the **lowest decompx toxicity importance**,\n- while being produced by an llm that sees both the **raw toxic sentence** and the **decompx-masked sentence**.\n\ncompared to the **decompx-masking + llm infilling + global reranking** pipeline, this notebook:\n\n- uses the **same decompx masking** and **same llm infilling** setup,\n- but replaces global reranking (toxicity + similarity + fluency) with **pure decompx toxicity-based reranking**.\n\n---\n\n## scoring: decompx-based reranking\n\nthe reranking stage follows the idea described in section 2.3 of the xdetox paper:\n\n1. for each input, we generate multiple candidate detoxified sentences $s_j$ with the llm.\n2. for each candidate sentence $s_j$, we run **decompx** and compute **token-level importance scores** with respect to toxicity.\n3. for each candidate, we sum the importance scores of its tokens:\n\n   - let $t_{i,j}$ be the $i$-th token in candidate sentence $s_j$.\n   - let $\\text{Importance}(t_{i,j})$ be its decompx importance score (toxicity contribution).\n\n   the **total toxicity importance** of candidate $s_j$ is:\n\n   $$\n   \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n   $$\n\n4. we choose the candidate with the **lowest total importance**:\n\n   $$\n   s^* = \\arg\\min_{s_j} \\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n   $$\n\nintuitively:\n\n- a **lower sum** of token-level importance scores means **lower overall toxicity**.\n- reranking selects the candidate with the **minimum decompx toxicity** among all candidates for that input.\n\nin the implementation, this is encapsulated in a helper like:\n\n- `rerank_candidates_decompx(sources, candidates, threshold, batch_size_mask)`\n\nwhich:\n\n- flattens all candidates,\n- runs decompx on each candidate sentence,\n- computes a decompx-based toxicity score per candidate,\n- reshapes scores back to `[num_inputs, num_candidates]`,\n- picks the **index of the candidate with the lowest score** for each input.\n\nthere is **no xlm-r / labse / gpt-2 global scoring** in this notebook.  \nall reranking is done by **decompx**.\n\n---\n\n## decompx masking\n\nmasking uses the original xdetox **decompx masker**:\n\n- implementation: `rewrite.mask_orig.Masker`.\n- backend: roberta with decompx token-level toxicity attribution.\n\nfor each input sentence:\n\n1. decompx computes an **importance score** for each token based on its contribution to toxicity.\n2. if the importance score of a token exceeds a **threshold** $t$, that token is considered **toxic**.\n3. such tokens are replaced by the `<mask>` token.\n\nfor a given dataset and decompx threshold $t$:\n\n- inputs are loaded via `rewrite_example.get_data`.\n- masked outputs are written to:\n\n```text\n  data/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/masked_inputs.txt\n```\n\n(where the exact directory name may encode the decompx threshold and the fact that this is a decompx-masking + llm pipeline.)\n\nthese masked sentences are later fed into the llm infiller.\n\n---\n\n## llm infilling (mistral-7b-instruct)\n\nafter decompx masking, we use **mistral-7b-instruct** as an **infilling model**.\n\n### inputs to the llm\n\nfor each example we provide **both**:\n\n* **toxic sentence**: the original toxic sentence, unchanged.\n* **masked sentence**: the decompx-masked sentence, where toxic spans have been replaced by `<mask>`.\n\nthe llm prompt is structured along the lines of:\n\n```text\nyou are a helpful assistant trained to make toxic or offensive sentences more polite and respectful\nwhile keeping their original meaning. ...\n\ntoxic sentence: {raw_toxic}\nmasked sentence: {masked_by_decompx}\nfinal output:\n```\n\nthe instructions emphasize:\n\n* **only fill in the `<mask>` tokens** in the masked sentence.\n* keep **all non-masked parts** of the masked sentence as close as possible to their original form.\n* preserve the **meaning and intent** of the toxic sentence.\n* use the **same language** as the toxic sentence.\n* return **only** the final detoxified sentence **inside one pair of square brackets**:\n\n```text\n[detoxified sentence here.]\n```\n\n### candidate generation\n\nfor each $(\\text{toxic}, \\text{masked})$ pair, we ask mistral for `num_candidates` completions:\n\n* the generation parameters include:\n\n  * `llm_sample` (sampling vs greedy),\n  * `llm_temperature`,\n  * `llm_top_p`,\n  * `max_new_tokens`.\n\n* for each completion, we:\n\n  1. **extract content inside the first `[ ... ]` block.**\n  2. **strip any remaining outer brackets.**\n  3. **normalize whitespace.**\n\nif a cleaned candidate is empty, we fall back to using the masked sentence.\n\nthe result: for each input sentence, we obtain a list of `num_candidates` **llm-generated candidates** to be reranked by decompx.\n\n---\n\n## end-to-end flow: masking, llm infilling, decompx reranking\n\nfor each dataset:\n\n1. **subset selection**\n\n   * the script can run on the full dataset or only the first `num_examples` instances.\n   * a subset file is written under:\n\n```text\ndatasets/_subsets/{data_type}/\n```\n\n2. **decompx masking (per threshold)**\n\n   * for each threshold $t$ in `thresholds`, we run decompx masking.\n   * masked sentences are saved to:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/masked_inputs.txt\n```\n\n3. **llm infilling (mistral-7b-instruct)**\n\n   * for each masked sentence and its corresponding toxic input, we call mistral with the prompt described above.\n   * we generate `num_candidates` candidates per input.\n   * the raw llm outputs are post-processed (bracket extraction, whitespace cleanup).\n\n4. **decompx-based reranking**\n\n   * for each threshold $t$ and each input sentence, we apply decompx to **all llm candidates**.\n\n   * for each candidate $s_j$, we compute the total toxicity importance:\n\n$$\n\\sum_{i=1}^{N_j} \\text{Importance}(t_{i,j})\n$$\n\n   * we choose the candidate with the **lowest** total importance as the final output.\n\n   * for each run, we write:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/orig.txt\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen.txt\n```\n\n   where:\n\n   * `orig.txt`: original toxic inputs (one per line),\n   * `gen.txt`: selected (reranked) llm outputs (one per line),\n   * `{run_folder}` encodes llm generation hyperparameters.\n\n---\n\n## evaluation\n\nif `run_eval=True`, the pipeline calls `evaluation.evaluate_all` to compute:\n\n* bertscore (f1),\n* meaningbert,\n* bleu-4,\n* toxicity (orig / gen),\n* perplexity (orig / gen).\n\nfor each `(threshold, run_folder)` we write:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen_stats.txt\n```\n\nthe notebook also builds a **summary csv per dataset** by scanning all `DecompX_LLM_DecompX*` directories:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n```\n\nthis csv aggregates:\n\n* `threshold` (decompx masking / reranking threshold),\n* `folder` (run folder name),\n* `bertscore`, `meaningbert`, `bleu4`,\n* `perplexity_gen`, `perplexity_orig`,\n* `toxicity_gen`, `toxicity_orig`.\n\n---\n\n## how to use `detoxify()`\n\na typical function signature for this notebook looks like:\n\n```python\ndef detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_decompx_mask_llm_decompx\",\n    thresholds = (0.20,),\n    echo: bool = False,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    mask_batch_size: int = 10,\n    llm_sample: bool = True,\n    llm_temperature: float = 0.7,\n    llm_top_p: float = 0.95,\n    max_new_tokens: int = 64,\n    num_candidates: int = 3,\n)\n```\n\n### key arguments\n\n#### core i/o\n\n* `data_type`:\n\n  * key in `data_configs`, for example:\n\n    * `\"paradetox\"`, `\"dynabench_val\"`, `\"dynabench_test\"`,\n    * `\"jigsaw_toxic\"`, `\"microagressions_val\"`, `\"sbf_val\"`,\n    * `\"appdia_original\"`, `\"appdia_discourse\"`, etc.\n\n* `output_folder`:\n\n  * top-level directory under:\n\n    ```text\n    data/model_outputs/{output_folder}/{data_type}/...\n    ```\n\n* `num_examples`:\n\n  * `None`: use the full dataset.\n  * integer: run on the first `num_examples` examples.\n\n* `overwrite_gen`:\n\n  * `False`: if `gen.txt` already exists for a given `(threshold, run_folder)`, reuse existing generations.\n  * `True`: regenerate and overwrite `gen.txt`.\n\n* `echo`:\n\n  * if `True`, print:\n\n    * basic dataset info,\n    * example inputs,\n    * example masked sentences,\n    * example final outputs,\n    * per-run metrics (if `run_eval=True`).\n\n#### decompx masking and thresholds\n\n* `thresholds`:\n\n  * tuple of decompx thresholds (e.g. `(0.15, 0.20, 0.25)`).\n  * for each $t$, we:\n\n    * run decompx masking with threshold $t$,\n    * run llm infilling,\n    * apply decompx-based reranking (using the same decompx mechanism).\n\n* `mask_batch_size`:\n\n  * batch size used when running decompx masking over inputs.\n\n#### llm infilling (mistral)\n\n* `llm_sample`:\n\n  * `True`: sampling.\n  * `False`: deterministic decoding.\n\n* `llm_temperature`:\n\n  * sampling temperature for mistral (used when `llm_sample=True`).\n\n* `llm_top_p`:\n\n  * top-p nucleus sampling cutoff.\n\n* `max_new_tokens`:\n\n  * maximum number of new tokens generated per candidate.\n\n* `num_candidates`:\n\n  * number of llm candidates per input to be reranked by decompx.\n\n#### evaluation\n\n* `run_eval`:\n\n  * if `True`, compute evaluation metrics and write `gen_stats.txt` files.\n\n* `overwrite_eval`:\n\n  * if `True`, recompute metrics even if `gen_stats.txt` already exists.\n\n* `skip_ref_eval`:\n\n  * if `True`, skip reference-based evaluation (for example, perplexity on reference outputs).\n\n---\n\n## example calls\n\n### quick sanity check (single threshold, small subset)\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"colab_run_decompx_mask_llm_decompx_demo_50_ex\",\n    thresholds=(0.20,),\n    echo=True,\n    num_examples=50,\n    overwrite_gen=True,\n    run_eval=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n    mask_batch_size=8,\n    llm_sample=True,\n    llm_temperature=0.7,\n    llm_top_p=0.95,\n    max_new_tokens=64,\n    num_candidates=10,\n)\n```\n\n### larger run (multiple thresholds, full dataset)\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"paradetox_decompx_mask_llm_decompx_full\",\n    thresholds=(0.15, 0.20, 0.25),\n    echo=True,\n    num_examples=None,\n    overwrite_gen=False,\n    run_eval=True,\n    overwrite_eval=False,\n    skip_ref_eval=False,\n    mask_batch_size=8,\n    llm_sample=True,\n    llm_temperature=0.7,\n    llm_top_p=0.95,\n    max_new_tokens=64,\n    num_candidates=10,\n)\n```\n\nafter running `detoxify`, you can inspect:\n\n* per-threshold, per-run outputs:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/orig.txt\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen.txt\ndata/model_outputs/{output_folder}/{data_type}/DecompX_LLM_DecompX{t}/{run_folder}/gen_stats.txt\n```\n\n* aggregated metrics:\n\n```text\ndata/model_outputs/{output_folder}/{data_type}/{data_type}.csv\n```\n\nthis notebook lets you compare:\n\n* **decompx masking + llm infilling + decompx reranking** (this pipeline),\n* against:\n\n  * **decompx masking + llm infilling + global reranking**, and\n  * the original **decompx masking + marco + decompx/global reranking** pipelines,\n\non the same datasets, using a decompx-based toxicity selection rule."
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive; drive.mount('/content/drive')\n\nimport os, glob, re, sys, json, shutil, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom subprocess import run, PIPE\nimport torch\nimport nltk\nfrom typing import List\n\ncandidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\nprint(\"try mydrive:\", candidate, \"->\", os.path.isdir(candidate))\n\nXDETOX_DIR = candidate\nprint(\"using xdetox_dir:\", XDETOX_DIR)\nassert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\"",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfBoQTrjtynY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545497901,
     "user_tz": 480,
     "elapsed": 34618,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "e6279c8e-7786-457e-cda6-5a68445ab6b3"
   },
   "id": "kfBoQTrjtynY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\nos.makedirs(HF_CACHE, exist_ok=True)\nos.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n\nif XDETOX_DIR not in sys.path:\n    sys.path.append(XDETOX_DIR)\n\nprint(\"xdetox_dir:\", XDETOX_DIR)\nprint(\"transformers_cache:\", HF_CACHE)\nprint(\"cuda available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ITPlTNBtzQx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545498531,
     "user_tz": 480,
     "elapsed": 633,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "0748982e-bf0b-4c36-da65-89dcba08d3e3"
   },
   "id": "7ITPlTNBtzQx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\nprint(\"ok\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEy2TGYetzIb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545498584,
     "user_tz": 480,
     "elapsed": 16,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "2efd54dc-e9a3-4164-f9de-db7b5111e343"
   },
   "id": "MEy2TGYetzIb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip -q install --upgrade pip setuptools wheel\n!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi \\\n                sentencepiece\n!pip -q install bert-score",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeTzwxVDtzNn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545521124,
     "user_tz": 480,
     "elapsed": 22538,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "1f0cc218-e538-4d46-ad94-09c65ba27d66"
   },
   "id": "GeTzwxVDtzNn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom rewrite.mask_orig import Masker as Masker_single\nfrom rewrite import rewrite_example as rx\nimport argparse as _argparse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfnuR2YVCmW9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534028,
     "user_tz": 480,
     "elapsed": 12900,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "9544c1b2-ddcd-44ff-af74-a9bb6358b78e"
   },
   "id": "tfnuR2YVCmW9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "nltk.download(\"punkt\", quiet=True)\ntry:\n    nltk.download(\"punkt_tab\", quiet=True)\nexcept Exception:\n    pass\nprint(\"nltk ok\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0Up7SKstzK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534428,
     "user_tz": 480,
     "elapsed": 420,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "1237d064-2ad9-425a-d05f-3fc2aed1288a"
   },
   "id": "y0Up7SKstzK9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data_configs = {\n    \"microagressions_val\": {\n        \"data_path\": \"./datasets/microagressions/val.csv\",\n    },\n    \"microagressions_test\": {\n        \"data_path\": \"./datasets/microagressions/test.csv\",\n    },\n    \"sbf_val\": {\n        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n    },\n    \"sbf_test\": {\n        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n    },\n    \"dynabench_val\": {\n        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n    },\n    \"dynabench_test\": {\n        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n    },\n    \"jigsaw_toxic\": {\n        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n    },\n    \"paradetox\": {\n        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n    },\n    \"appdia_original\": {\n        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n    },\n    \"appdia_discourse\": {\n        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n    }\n}\nprint(\"datasets:\", \", \".join(data_configs.keys()))\n\nREPO = XDETOX_DIR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nBku39IuAgb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534477,
     "user_tz": 480,
     "elapsed": 48,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "9fa7fe11-7936-42e4-c4ee-95febc3bf717"
   },
   "id": "7nBku39IuAgb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _abs_repo_path(rel: str) -> str:\n    return os.path.join(REPO, rel.lstrip(\"./\"))\n\ndef _ensure_dir(p: str):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef _subset_for_data_type(data_type, data_path, n, out_dir):\n    if n is None or n <= 0:\n        return data_path\n\n    src = _abs_repo_path(data_path)\n    _ensure_dir(out_dir)\n\n    if \"microagressions\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"sbf\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"dynabench\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n        if data_path.endswith(\".txt\"):\n            with open(src, \"r\") as f:\n                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n            out = os.path.join(out_dir, os.path.basename(src))\n            with open(out, \"w\") as g:\n                for s in lines[:n]:\n                    g.write(s + \"\\n\")\n            return out\n        elif data_path.endswith(\".csv\"):\n            df = pd.read_csv(src).head(n)\n            out = os.path.join(out_dir, os.path.basename(src))\n            df.to_csv(out, index=False)\n            return out\n\n    if \"appdia\" in data_path:\n        df = pd.read_csv(src, sep=\"\\t\").head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        df.to_csv(out, sep=\"\\t\", index=False)\n        return out\n\n    out = os.path.join(out_dir, os.path.basename(src))\n    shutil.copy(src, out)\n    return out",
   "metadata": {
    "id": "ToytrY0SuAjr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534486,
     "user_tz": 480,
     "elapsed": 5,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "ToytrY0SuAjr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _decompx_mask_texts(texts: List[str],\n                        threshold: float = 0.20,\n                        batch_size: int = 16) -> List[str]:\n    if not texts:\n        return []\n\n    masker = Masker_single()\n    masked_all = []\n    for i in tqdm(range(0, len(texts), batch_size),\n                  desc=\"decompx masking\", leave=False):\n        batch = texts[i:i + batch_size]\n        batch_out = masker.process_text(sentence=batch, threshold=threshold)\n        masked_all.extend(batch_out)\n    masker.release_model()\n\n    cleaned = [\n        m.replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n        for m in masked_all\n    ]\n    return cleaned\n\ndef _decompx_toxicity_scores(texts: List[str],\n                             threshold: float = 0.20,\n                             batch_size: int = 16) -> np.ndarray:\n    if not texts:\n        return np.zeros((0,), dtype=float)\n\n    masked = _decompx_mask_texts(texts, threshold=threshold, batch_size=batch_size)\n    scores = []\n    for m in masked:\n        num_masks = len(re.findall(r\"<mask>\", m))\n        tokens = m.split()\n        length = max(len(tokens), 1)\n        scores.append(num_masks / length)\n    return np.asarray(scores, dtype=float)\n\ndef rerank_candidates_decompx(\n    sources: List[str],\n    candidates: List[List[str]],\n    threshold: float = 0.20,\n    batch_size_mask: int = 16,\n):\n    N = len(sources)\n    assert len(candidates) == N, \"candidates length mismatch\"\n\n    if N == 0:\n        return np.array([], dtype=int), {}\n\n    C_list = [len(c) for c in candidates]\n    assert len(set(C_list)) == 1, \"All inputs must have same num_candidates\"\n    C = C_list[0]\n    if C == 0:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    flat_cands = []\n    flat_src_idx = []\n    for i, cand_list in enumerate(candidates):\n        for cand in cand_list:\n            flat_cands.append(cand)\n            flat_src_idx.append(i)\n    flat_src_idx = np.array(flat_src_idx, dtype=int)\n\n    scores = _decompx_toxicity_scores(\n        flat_cands,\n        threshold=threshold,\n        batch_size=batch_size_mask,\n    )\n\n    scores2 = scores.reshape(N, C)\n    best_idx = np.argmin(scores2, axis=1)\n\n    details = {\n        \"score\": scores2,\n    }\n    return best_idx, details",
   "metadata": {
    "id": "PDRoBhdM8461",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534516,
     "user_tz": 480,
     "elapsed": 25,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "PDRoBhdM8461",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _parse_run_folder_name(folder_name):\n    return True\n\ndef _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False,\n                        tox_threshold=0.5, tox_batch_size=32):\n    import sys as _sys, os as _os\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n            continue\n        orig_path = os.path.join(gen_dir, \"orig.txt\")\n        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n            continue\n        if os.path.exists(out_stats) and not overwrite_eval:\n            continue\n\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n        cmd = [\n            _sys.executable, \"-m\", \"evaluation.evaluate_all\",\n            \"--orig_path\", orig_path,\n            \"--gen_path\",  gen_path,\n            \"--tox_threshold\", str(tox_threshold),\n            \"--tox_batch_size\", str(tox_batch_size),\n        ]\n        if skip_ref:\n            cmd.append(\"--skip_ref\")\n        print(\"eval:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n        if res.returncode != 0:\n            print(res.stdout)\n            print(res.stderr)\n            res.check_returncode()\n\ndef _safe_float(x):\n    try:\n        return float(x)\n    except Exception:\n        return float('nan')\n\ndef _read_stats_file(path):\n    out = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            k, v = line.strip().split(\": \", 1)\n            k = k.replace(\"(skipped)\", \"\").strip().lower()\n            out[k] = _safe_float(v)\n    return out\n\ndef _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n    rows = []\n\n    root = os.path.join(base_out_dir, data_type)\n    if not os.path.isdir(root):\n        print(\"no evaluation directory found:\", root)\n        return\n\n    for mask_dir in os.listdir(root):\n        if not mask_dir.startswith(\"DecompX_LLM\"):\n            continue\n        thresh_str = mask_dir.replace(\"DecompX_LLM\", \"\")\n        try:\n            threshold = float(thresh_str)\n        except Exception:\n            threshold = np.nan\n\n        base_path = os.path.join(root, mask_dir)\n        for folder in os.listdir(base_path):\n            gen_dir = os.path.join(base_path, folder)\n            stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n            if not os.path.exists(stats_path):\n                continue\n            s = _read_stats_file(stats_path)\n            rows.append({\n                \"threshold\":        threshold,\n                \"folder\":           folder,\n                \"bertscore\":        s.get(\"bertscore\", np.nan),\n                \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n                \"bleu4\":            s.get(\"bleu4\", np.nan),\n                \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n                \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n                \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n                \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n            })\n\n    if rows:\n        cols = [\n            \"threshold\", \"folder\",\n            \"bertscore\", \"meaningbert\", \"bleu4\",\n            \"perplexity_gen\", \"perplexity_orig\",\n            \"toxicity_gen\", \"toxicity_orig\",\n        ]\n        df = pd.DataFrame(rows)\n        df = df[cols]\n        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n        df.to_csv(out_csv, index=False)\n        print(\"wrote summary csv:\", out_csv)\n    else:\n        print(\"no evaluation files found to summarize.\")",
   "metadata": {
    "id": "u-7I09Uvqb8c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534527,
     "user_tz": 480,
     "elapsed": 6,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "u-7I09Uvqb8c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "USE_LLM_GPU = True\nDEVICE_LLM = torch.device(\"cuda\" if USE_LLM_GPU and torch.cuda.is_available() else \"cpu\")\nprint(\"llm device:\", DEVICE_LLM)\n\nLLM_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n_LLM_TOKENIZER = None\n_LLM_MODEL = None\n\nINFILL_SYSTEM_PROMPT = \"\"\"You are a helpful assistant trained to make toxic or offensive sentences\nmore polite and respectful by INFILLING <mask> tokens in a masked sentence.\n\nYou are given:\n- Toxic Sentence: the original sentence, which may contain offensive language.\n- Masked Sentence: the same sentence, but all toxic spans are replaced by <mask>.\n\nYour rules:\n1. Only replace <mask> tokens in the Masked Sentence with polite, non-toxic alternatives.\n2. Do NOT change any other words or punctuation from the Masked Sentence,\n   except for small grammar fixes needed after infilling.\n3. Preserve the original meaning and intent of the Toxic Sentence as much as possible.\n4. Keep the same language as the Toxic Sentence.\n5. If the Masked Sentence has no <mask> tokens, return it unchanged.\n\nOutput rules (very strict):\n- Return ONLY the final detoxified sentence inside ONE pair of square brackets, like:\n  [You are such a rude person, nobody wants to hear your opinion.]\n- Do NOT print anything before or after the brackets.\n- Do NOT add explanations, comments, or extra lines.\n- Do NOT include additional '[' or ']' characters inside the sentence.\n\"\"\"\n\nINFILL_FEW_SHOT = \"\"\"Toxic Sentence: You're such a stupid idiot, nobody wants to hear your crap.\nMasked Sentence: You're such a <mask>, nobody wants to hear your <mask>.\nStep 1 - Decide polite replacements for <mask>: \"rude person\", \"opinion\"\nStep 2 - Insert them into the Masked Sentence, keeping all other tokens:\nYou're such a rude person, nobody wants to hear your opinion.\nFinal Output: [You're such a rude person, nobody wants to hear your opinion.]\"\"\"\n\ndef _lazy_load_llm_infiller():\n    global _LLM_MODEL, _LLM_TOKENIZER\n    if _LLM_MODEL is not None and _LLM_TOKENIZER is not None:\n        return\n    print(f\"loading llm: {LLM_MODEL_NAME} on {DEVICE_LLM} ...\")\n    _LLM_TOKENIZER = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n    _LLM_MODEL = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL_NAME,\n        torch_dtype=torch.float16 if DEVICE_LLM.type == \"cuda\" else torch.float32,\n        device_map=None,\n    ).to(DEVICE_LLM)\n    _LLM_MODEL.eval()\n    print(\"llm loaded\")\n\ndef _extract_bracket_content(text: str) -> str:\n    text = text.strip()\n    m = re.search(r\"\\[([^\\]]*)\\]\", text, flags=re.DOTALL)\n    if m:\n        return m.group(1).strip()\n    if \"[\" in text:\n        return text.split(\"[\", 1)[1].strip()\n    return text\n\ndef _cleanup_llm_output(s: str) -> str:\n    s = s.strip()\n    if s.startswith(\"[\") and s.endswith(\"]\") and len(s) > 2:\n        s = s[1:-1].strip()\n    else:\n        if s.startswith(\"[\"):\n            s = s[1:].strip()\n        if s.endswith(\"]\"):\n            s = s[:-1].strip()\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\n@torch.no_grad()\ndef llm_infill_candidates(\n    toxic_sentences: List[str],\n    masked_sentences: List[str],\n    num_candidates: int = 3,\n    temperature: float = 0.7,\n    top_p: float = 0.95,\n    max_new_tokens: int = 64,\n    sample: bool = True,\n) -> List[List[str]]:\n    _lazy_load_llm_infiller()\n    assert len(toxic_sentences) == len(masked_sentences), \"length mismatch\"\n\n    all_candidates: List[List[str]] = []\n\n    for idx in tqdm(range(len(toxic_sentences)), desc=\"llm infilling\", leave=False):\n        toxic = toxic_sentences[idx]\n        masked = masked_sentences[idx]\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": INFILL_SYSTEM_PROMPT + \"\\n\\nBelow is an example:\\n\" + INFILL_FEW_SHOT,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    f\"Toxic Sentence: {toxic}\\n\"\n                    f\"Masked Sentence: {masked}\\n\"\n                    \"Final Output:\"\n                ),\n            },\n        ]\n        try:\n            prompt = _LLM_TOKENIZER.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n            )\n        except Exception:\n            prompt = (\n                INFILL_SYSTEM_PROMPT\n                + \"\\n\\nExample:\\n\"\n                + INFILL_FEW_SHOT\n                + \"\\n\\nToxic Sentence: \"\n                + toxic\n                + \"\\nMasked Sentence: \"\n                + masked\n                + \"\\nFinal Output:\"\n            )\n\n        inputs = _LLM_TOKENIZER(prompt, return_tensors=\"pt\").to(DEVICE_LLM)\n        gen = _LLM_MODEL.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=sample,\n            temperature=temperature if sample else 0.0,\n            top_p=top_p,\n            num_return_sequences=num_candidates,\n            pad_token_id=_LLM_TOKENIZER.eos_token_id,\n        )\n        input_len = inputs[\"input_ids\"].shape[1]\n\n        cands_for_this = []\n        for k in range(num_candidates):\n            gen_text = _LLM_TOKENIZER.decode(\n                gen[k][input_len:], skip_special_tokens=True\n            )\n            detox = _extract_bracket_content(gen_text)\n            detox = _cleanup_llm_output(detox)\n            if not detox:\n                detox = masked\n            cands_for_this.append(detox)\n\n        all_candidates.append(cands_for_this)\n\n    return all_candidates",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nG355GLlqcp-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534552,
     "user_tz": 480,
     "elapsed": 21,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "b4b702e3-6317-40ff-f361-acddee65dfff"
   },
   "id": "nG355GLlqcp-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def _bool2str(x: bool) -> str:\n    return \"T\" if x else \"F\"\n\ndef _build_llm_run_folder_name(\n    temperature: float,\n    sample: bool,\n    top_p: float,\n    max_new_tokens: int,\n    num_candidates: int,\n):\n    return (\n        \"llmtemp\" + str(temperature) +\n        \"_sample\" + _bool2str(sample) +\n        \"_topp\" + str(top_p) +\n        \"_maxnew\" + str(max_new_tokens) +\n        \"_ncand\" + str(num_candidates)\n    )\n\ndef _run_decompx_masking_llm_infill_and_decompx_reranking_for_threshold(\n    data_type,\n    subset_path,\n    thresh,\n    base_out_rel,\n    mask_batch_size,\n    llm_sample,\n    llm_temperature,\n    llm_top_p,\n    max_new_tokens,\n    num_candidates,\n    decompx_threshold,\n    overwrite_gen=False,\n    inputs=None,\n    rerank_batch_size: int = 16,\n    echo: bool = False,\n):\n    if inputs is None:\n        args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n        inputs = rx.get_data(args_data)\n    print(f\"#inputs at thresh={thresh}: {len(inputs)}\")\n\n    mask_dir = f\"DecompX_LLM{abs(thresh):g}\" if thresh != 0 else \"DecompX_LLM0.0\"\n    cur_rel = os.path.join(base_out_rel, data_type, mask_dir)\n    cur_abs = os.path.join(REPO, cur_rel)\n    _ensure_dir(cur_abs)\n\n    masked_file = os.path.join(cur_abs, \"masked_inputs.txt\")\n\n    if not os.path.exists(masked_file):\n        print(f\"running decompx masking (threshold={thresh:.2f}) to create masked_inputs.txt ...\")\n        decoded_mask_inputs = _decompx_mask_texts(\n            inputs, threshold=thresh, batch_size=mask_batch_size\n        )\n        with open(masked_file, \"w\") as f:\n            for d in decoded_mask_inputs:\n                f.write(re.sub(r\"\\s+\", \" \", d).strip() + \"\\n\")\n    else:\n        with open(masked_file, \"r\") as f:\n            decoded_mask_inputs = [s.strip() for s in f.readlines()]\n        print(\"reusing existing masked_inputs.txt\")\n\n    assert len(decoded_mask_inputs) == len(inputs), \"Masked vs inputs mismatch\"\n\n    if echo:\n        print(\"\\nexample masked inputs (first up to 3):\")\n        for i, m in enumerate(decoded_mask_inputs[:3]):\n            print(f\"  masked[{i}]: {m}\")\n\n    run_folder = _build_llm_run_folder_name(\n        llm_temperature, llm_sample, llm_top_p, max_new_tokens, num_candidates\n    )\n    final_abs = os.path.join(cur_abs, run_folder)\n    gen_txt = os.path.join(final_abs, \"gen.txt\")\n    orig_txt = os.path.join(final_abs, \"orig.txt\")\n\n    if os.path.exists(gen_txt) and not overwrite_gen:\n        print(\"gen exists at:\", gen_txt, \"â€” skipping\")\n        _ensure_dir(final_abs)\n        with open(gen_txt, \"r\") as f:\n            best_generations = [s.strip() for s in f.readlines()]\n        return inputs, decoded_mask_inputs, best_generations, final_abs\n\n    _ensure_dir(final_abs)\n\n    print(f\"llm infilling: generating {num_candidates} candidates per input (sampling={llm_sample})\")\n    all_candidates = llm_infill_candidates(\n        toxic_sentences=inputs,\n        masked_sentences=decoded_mask_inputs,\n        num_candidates=num_candidates,\n        temperature=llm_temperature,\n        top_p=llm_top_p,\n        max_new_tokens=max_new_tokens,\n        sample=llm_sample,\n    )\n\n    print(f\"decompx reranking of llm candidates (threshold={decompx_threshold:.2f}) ...\")\n    best_idx, details = rerank_candidates_decompx(\n        sources=inputs,\n        candidates=all_candidates,\n        threshold=decompx_threshold,\n        batch_size_mask=rerank_batch_size,\n    )\n    best_generations = [\n        all_candidates[i][best_idx[i]] for i in range(len(inputs))\n    ]\n\n    if echo:\n        print(\"\\nexample detoxified outputs (first up to 3):\")\n        for i, g in enumerate(best_generations[:3]):\n            print(f\"  detox[{i}]: {g}\")\n\n    with open(orig_txt, \"w\") as f:\n        for l in inputs:\n            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n    with open(gen_txt, \"w\") as f:\n        for l in best_generations:\n            f.write(re.sub(r\"\\s+\", \" \", l).strip() + \"\\n\")\n\n    print(\"saved:\", orig_txt)\n    print(\"saved:\", gen_txt)\n\n    return inputs, decoded_mask_inputs, best_generations, final_abs",
   "metadata": {
    "id": "U5oOUWRYuA6E",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534609,
     "user_tz": 480,
     "elapsed": 15,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "U5oOUWRYuA6E",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run_decompx_mask_llm_decompx\",\n    thresholds = (0.20,),\n    echo: bool = False,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    mask_batch_size: int = 10,\n    rerank_batch_size: int = 16,\n    decompx_rerank_threshold: float = None,\n    llm_sample: bool = True,\n    llm_temperature: float = 0.7,\n    llm_top_p: float = 0.95,\n    max_new_tokens: int = 64,\n    num_candidates: int = 3,\n):\n    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n    cfg = data_configs[data_type].copy()\n\n    if num_candidates < 1:\n        raise ValueError(\"num_candidates must be >= 1\")\n\n    base_out_rel = os.path.join(\"data\", \"model_outputs\", output_folder)\n    base_out_abs = os.path.join(REPO, base_out_rel)\n    _ensure_dir(base_out_abs)\n\n    original_data_path = cfg[\"data_path\"]\n    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n    _ensure_dir(subset_dir)\n    subset_path = _subset_for_data_type(\n        data_type, original_data_path, num_examples, subset_dir\n    )\n\n    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n    inputs = rx.get_data(args_data)\n    num_inputs = len(inputs)\n\n    if echo:\n        print(\"=\" * 80)\n        print(f\"dataset: {data_type}\")\n        print(f\"subset path: {subset_path}\")\n        print(f\"output base: {base_out_abs}\")\n        print(f\"number of examples: {num_inputs}\")\n        print(f\"thresholds: {', '.join(f'{t:.2f}' for t in thresholds)}\")\n        print(f\"llm: temperature={llm_temperature}, top_p={llm_top_p}, \"\n              f\"sample={llm_sample}, max_new_tokens={max_new_tokens}\")\n        print(f\"num_candidates per input: {num_candidates}\")\n        print(\"\\nexample inputs (first up to 3):\")\n        for i, s in enumerate(inputs[:3]):\n            print(f\"  input[{i}]: {s}\")\n        print(\"=\" * 80)\n\n    last_run_dir = None\n    for t in thresholds:\n        print(\"=\" * 60)\n        print(f\"decompx masking threshold = {t:.2f}\")\n        effective_rerank_t = decompx_rerank_threshold if decompx_rerank_threshold is not None else t\n\n        inputs, masked_inputs, best_generations, run_dir = \\\n            _run_decompx_masking_llm_infill_and_decompx_reranking_for_threshold(\n                data_type=data_type,\n                subset_path=subset_path,\n                thresh=t,\n                base_out_rel=base_out_rel,\n                mask_batch_size=mask_batch_size,\n                llm_sample=llm_sample,\n                llm_temperature=llm_temperature,\n                llm_top_p=llm_top_p,\n                max_new_tokens=max_new_tokens,\n                num_candidates=num_candidates,\n                decompx_threshold=effective_rerank_t,\n                overwrite_gen=overwrite_gen,\n                inputs=inputs,\n                rerank_batch_size=rerank_batch_size,\n                echo=echo,\n            )\n        last_run_dir = run_dir\n\n        if run_eval:\n            mask_dir = f\"DecompX_LLM{abs(t):g}\" if t != 0 else \"DecompX_LLM0.0\"\n            base_path = os.path.join(base_out_abs, data_type, mask_dir)\n            _eval_with_toxicity(\n                base_path,\n                overwrite_eval=overwrite_eval,\n                skip_ref=skip_ref_eval,\n                tox_threshold=0.5,\n                tox_batch_size=32,\n            )\n\n            if echo:\n                run_folder = os.path.basename(run_dir)\n                stats_path = os.path.join(base_path, run_folder, \"gen_stats.txt\")\n                if os.path.exists(stats_path):\n                    stats = _read_stats_file(stats_path)\n                    print(\"\\neval metrics for this run \"\n                          f\"(t={t:.2f}):\")\n                    metric_keys = [\n                        (\"bertscore\",        \"bertscore\"),\n                        (\"meaningbert\",      \"meaningbert\"),\n                        (\"bleu4\",            \"bleu-4\"),\n                        (\"perplexity gen\",   \"perplexity (gen)\"),\n                        (\"perplexity orig\",  \"perplexity (orig)\"),\n                        (\"toxicity gen\",     \"toxicity (gen)\"),\n                        (\"toxicity orig\",    \"toxicity (orig)\"),\n                    ]\n                    for key, label in metric_keys:\n                        val = stats.get(key, None)\n                        if isinstance(val, float) and math.isnan(val):\n                            continue\n                        if val is None:\n                            continue\n                        print(f\"  {label}: {val:.4f}\")\n                else:\n                    print(f\"gen_stats.txt not found at {stats_path}\")\n\n    if run_eval:\n        _aggregate_eval_csv(\n            output_folder,\n            data_type,\n            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n        )",
   "metadata": {
    "id": "oqBLx5OSuA72",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534638,
     "user_tz": 480,
     "elapsed": 26,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "oqBLx5OSuA72",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# example (small subset; adjust as needed):\n# detoxify(\n#     data_type=\"paradetox\",\n#     output_folder=\"colab_run_decompx_mask_llm_decompx_demo_50_examples\",\n#     thresholds=(0.20,),\n#     echo=True,\n#     num_examples=50,\n#     overwrite_gen=True,\n#     run_eval=True,\n#     overwrite_eval=True,\n#     skip_ref_eval=False,\n#     mask_batch_size=8,\n#     rerank_batch_size=16,\n#     llm_sample=True,\n#     llm_temperature=0.7,\n#     llm_top_p=0.95,\n#     max_new_tokens=64,\n#     num_candidates=10,\n# )",
   "metadata": {
    "id": "u5LlySYquA9g",
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764545534648,
     "user_tz": 480,
     "elapsed": 5,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "u5LlySYquA9g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "detoxify(\n",
    "    data_type=\"paradetox\",\n",
    "    output_folder=\"XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\",\n",
    "    thresholds=(0.20,),\n",
    "    echo=True,\n",
    "    num_examples=1000,\n",
    "    overwrite_gen=True,\n",
    "    run_eval=True,\n",
    "    overwrite_eval=True,\n",
    "    skip_ref_eval=False,\n",
    "    mask_batch_size=8,\n",
    "    rerank_batch_size=16,\n",
    "    llm_sample=True,\n",
    "    llm_temperature=0.7,\n",
    "    llm_top_p=0.95,\n",
    "    max_new_tokens=64,\n",
    "    num_candidates=10,\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "71da2c25a0bf47078c758ed59ef4bc28",
      "ccebb6db12a0418a90189c29c1d960ef",
      "bde93d01ab5b4eb2b216a7953b41931e",
      "59af0aa5311644deb97845669ba73a52",
      "1a6df16002b54f0f9bded2f156637ce0",
      "b43394c03fd84a4380ae84b1cb1523da",
      "ceb67f9b82b54d319e120c0d309d2885",
      "34e0316d7c9b4b3d86dcf7035e06d1c1",
      "efcb58a662224f108c8c1638744e0fe7",
      "30d9e7d8a6ff4c349468d234c5b147c2",
      "74634886ae894a5483a65ca0f7cea548",
      "35c5a71dc5b44c1aa3ce08c1836a32be",
      "faf9af58e8ee496abc7869284d273e30",
      "be2f24f320ce4b81b6eea28bb479287d",
      "4c14692fc6b64f3db7401b350fda7698",
      "955ae2dc83ed4e36b3a6eef238ee6d5e",
      "b47186463c4f4331abd72607498ceec4",
      "6156ce5bc4114a5cb027106a1e6e843f",
      "f456ec232c1649f39607bd025882110e",
      "6bb27cf92319458293255e789a938d75",
      "310b5fb84f36421a8e207324e3c0faf4",
      "f30d9f931ad84451b4a08d9631d42959",
      "efe6108d8690407cab868dc5a9b90141",
      "addc396bf10947af8613fe51d219a50e",
      "a23db48efb224909b3d5a2df332c55d1",
      "059c133b7ee04684b53fe7cc118eb09f",
      "7dd6f07942bf490dbfce7f878bab36ee",
      "91ee2ae926274595a9918a52e832468e",
      "9a0c7eac7abe4461b2ba236e8165cb67",
      "dedf18d9aafa4d92ac0c28b91b7a2fdc",
      "90f3abd167cb4abeb386f7aad213a300",
      "cd97eb140cf242bb9680f1702d3544e1",
      "0db2cf8137fe409985a72b9432385a30",
      "ea1c62620985487f86b5eca38c5d8712",
      "e486adb41aaf4526831716b6b655fd7d",
      "466759ae0d4a411cbf5fa7eb55b5a1f0",
      "60fb26d0acf841f1bb76520a8be4847a",
      "905dfb8f2b734625b3aac9114e283d79",
      "09fa8278eafc4c28a2c0724caf12f764",
      "725dd073f0574d51a2cd58e54426267a",
      "5dab7203ab364959a0f1237c24c7ffb4",
      "d94273b08da242739a30f6012e5dc58d",
      "102b8e7750754cb1bd42633201e2af62",
      "4c3b1ab8a8e9467295fec8bdedbd686e",
      "f3f9f1d51bcf4b009becaaaa03d62a13",
      "f4710f9eb9e54742a5a071227848048a",
      "8ef9e9f677964c44ba7c0213696dd774",
      "49127e0052dd4b62800c755801a65b78",
      "76ff9cb7208c47459f2ad8ea5d6ca642",
      "7908ea2adb4a4866a240736b0ab19fd1",
      "5a745e7cd0574c488b00c8c459c74f2a",
      "397038c370fc46fc876a559dee2cbae8",
      "12fc2ace10614a23ac3769468516997c",
      "6b680ab8f92846b7829a973d5374b811",
      "f360eff002c848549ba42229210e6b09"
     ]
    },
    "id": "O2S9fX-adatU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764548221756,
     "user_tz": 480,
     "elapsed": 2687104,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "53624e95-d426-4739-84bd-4f43a1d062bd"
   },
   "id": "O2S9fX-adatU",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "[echo] Dataset: paradetox\n",
      "[echo] Subset path: /content/drive/MyDrive/w266 - Project/XDetox/datasets/_subsets/paradetox/test_toxic_parallel.txt\n",
      "[echo] Output base: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline\n",
      "[echo] Number of examples to detoxify: 671\n",
      "[echo] Thresholds (DecompX masking): 0.20\n",
      "[echo] LLM: temperature=0.7, top_p=0.95, sample=True, max_new_tokens=64\n",
      "[echo] num_candidates per input: 10\n",
      "\n",
      "[echo] Example inputs (first up to 3):\n",
      "  input[0]: . or the loud ass one - thousand ton beast roaring towards you howling its horn .\n",
      "  input[1]: mandated  and \" right fucking now \" would be good .\n",
      "  input[2]: * neither * of my coworkers gave a shit when it came time to ditch mitch . ugh .\n",
      "================================================================================\n",
      "============================================================\n",
      "DecompX masking threshold = 0.20\n",
      "#inputs at thresh=0.2: 671\n",
      "Running DecompX masking (threshold=0.20) to create masked_inputs.txt ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DecompX masking:   0%|          | 0/84 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71da2c25a0bf47078c758ed59ef4bc28"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[echo] Example masked inputs (first up to 3):\n",
      "  masked[0]: . or the loud <mask> one - thousand ton beast roaring towards you howling its horn .\n",
      "  masked[1]: mandated  and \" right <mask> now \" would be good .\n",
      "  masked[2]: * neither * of my coworkers gave a <mask> when it came time to ditch mitch . ugh .\n",
      "LLM infilling: generating 10 candidates per input (sampling=True)\n",
      "Loading LLM infiller: mistralai/Mistral-7B-Instruct-v0.2 on cuda ...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35c5a71dc5b44c1aa3ce08c1836a32be"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efe6108d8690407cab868dc5a9b90141"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LLM infiller loaded.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LLM infilling:   0%|          | 0/671 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea1c62620985487f86b5eca38c5d8712"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DecompX reranking of LLM candidates (threshold=0.20) ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DecompX masking:   0%|          | 0/420 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3f9f1d51bcf4b009becaaaa03d62a13"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[echo] Example detoxified outputs (first up to 3):\n",
      "  detox[0]: or the loud person one - thousand ton beast roaring towards you howling its horn.\n",
      "  detox[1]: mandated and \" right now \" would be good .\n",
      "  detox[2]: neither of my coworkers showed interest when it came time to ditch mitch . ugh .\n",
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/orig.txt\n",
      "Saved: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/gen.txt\n",
      "Eval: /usr/bin/python3 -m evaluation.evaluate_all --orig_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/orig.txt --gen_path /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/DecompX_LLM0.2/llmtemp0.7_sampleT_topp0.95_maxnew64_ncand10/gen.txt --tox_threshold 0.5 --tox_batch_size 32\n",
      "\n",
      "[echo] Evaluation metrics for this run (t=0.20):\n",
      "  BERTScore: 0.9384\n",
      "  MeaningBERT: 66.1640\n",
      "  BLEU-4: 82.8620\n",
      "  Perplexity (gen): 200.2900\n",
      "  Perplexity (orig): 273.7500\n",
      "  Toxicity (gen): 0.1707\n",
      "  Toxicity (orig): 0.9253\n",
      "Wrote summary CSV: /content/drive/MyDrive/w266 - Project/XDetox/data/model_outputs/XDetox_w_DecompX-Masking_LLM-Infilling_DecompX-Reranking_Pipeline/paradetox/paradetox.csv\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VgZRbEBED8w4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764548221765,
     "user_tz": 480,
     "elapsed": 14,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    }
   },
   "id": "VgZRbEBED8w4",
   "execution_count": 15,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "71da2c25a0bf47078c758ed59ef4bc28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ccebb6db12a0418a90189c29c1d960ef",
       "IPY_MODEL_bde93d01ab5b4eb2b216a7953b41931e",
       "IPY_MODEL_59af0aa5311644deb97845669ba73a52"
      ],
      "layout": "IPY_MODEL_1a6df16002b54f0f9bded2f156637ce0"
     }
    },
    "ccebb6db12a0418a90189c29c1d960ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b43394c03fd84a4380ae84b1cb1523da",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ceb67f9b82b54d319e120c0d309d2885",
      "value": "DecompXâ€‡masking:â€‡â€‡99%"
     }
    },
    "bde93d01ab5b4eb2b216a7953b41931e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34e0316d7c9b4b3d86dcf7035e06d1c1",
      "max": 84,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efcb58a662224f108c8c1638744e0fe7",
      "value": 84
     }
    },
    "59af0aa5311644deb97845669ba73a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30d9e7d8a6ff4c349468d234c5b147c2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_74634886ae894a5483a65ca0f7cea548",
      "value": "â€‡83/84â€‡[00:06&lt;00:00,â€‡14.83it/s]"
     }
    },
    "1a6df16002b54f0f9bded2f156637ce0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "b43394c03fd84a4380ae84b1cb1523da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ceb67f9b82b54d319e120c0d309d2885": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34e0316d7c9b4b3d86dcf7035e06d1c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efcb58a662224f108c8c1638744e0fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30d9e7d8a6ff4c349468d234c5b147c2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74634886ae894a5483a65ca0f7cea548": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35c5a71dc5b44c1aa3ce08c1836a32be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_faf9af58e8ee496abc7869284d273e30",
       "IPY_MODEL_be2f24f320ce4b81b6eea28bb479287d",
       "IPY_MODEL_4c14692fc6b64f3db7401b350fda7698"
      ],
      "layout": "IPY_MODEL_955ae2dc83ed4e36b3a6eef238ee6d5e"
     }
    },
    "faf9af58e8ee496abc7869284d273e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b47186463c4f4331abd72607498ceec4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6156ce5bc4114a5cb027106a1e6e843f",
      "value": "Downloadingâ€‡shards:â€‡100%"
     }
    },
    "be2f24f320ce4b81b6eea28bb479287d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f456ec232c1649f39607bd025882110e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6bb27cf92319458293255e789a938d75",
      "value": 3
     }
    },
    "4c14692fc6b64f3db7401b350fda7698": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_310b5fb84f36421a8e207324e3c0faf4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f30d9f931ad84451b4a08d9631d42959",
      "value": "â€‡3/3â€‡[00:02&lt;00:00,â€‡â€‡1.40it/s]"
     }
    },
    "955ae2dc83ed4e36b3a6eef238ee6d5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b47186463c4f4331abd72607498ceec4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6156ce5bc4114a5cb027106a1e6e843f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f456ec232c1649f39607bd025882110e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bb27cf92319458293255e789a938d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "310b5fb84f36421a8e207324e3c0faf4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f30d9f931ad84451b4a08d9631d42959": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efe6108d8690407cab868dc5a9b90141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_addc396bf10947af8613fe51d219a50e",
       "IPY_MODEL_a23db48efb224909b3d5a2df332c55d1",
       "IPY_MODEL_059c133b7ee04684b53fe7cc118eb09f"
      ],
      "layout": "IPY_MODEL_7dd6f07942bf490dbfce7f878bab36ee"
     }
    },
    "addc396bf10947af8613fe51d219a50e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91ee2ae926274595a9918a52e832468e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9a0c7eac7abe4461b2ba236e8165cb67",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "a23db48efb224909b3d5a2df332c55d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dedf18d9aafa4d92ac0c28b91b7a2fdc",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f3abd167cb4abeb386f7aad213a300",
      "value": 3
     }
    },
    "059c133b7ee04684b53fe7cc118eb09f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd97eb140cf242bb9680f1702d3544e1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0db2cf8137fe409985a72b9432385a30",
      "value": "â€‡3/3â€‡[03:55&lt;00:00,â€‡74.97s/it]"
     }
    },
    "7dd6f07942bf490dbfce7f878bab36ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91ee2ae926274595a9918a52e832468e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a0c7eac7abe4461b2ba236e8165cb67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dedf18d9aafa4d92ac0c28b91b7a2fdc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90f3abd167cb4abeb386f7aad213a300": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd97eb140cf242bb9680f1702d3544e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0db2cf8137fe409985a72b9432385a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea1c62620985487f86b5eca38c5d8712": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e486adb41aaf4526831716b6b655fd7d",
       "IPY_MODEL_466759ae0d4a411cbf5fa7eb55b5a1f0",
       "IPY_MODEL_60fb26d0acf841f1bb76520a8be4847a"
      ],
      "layout": "IPY_MODEL_905dfb8f2b734625b3aac9114e283d79"
     }
    },
    "e486adb41aaf4526831716b6b655fd7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09fa8278eafc4c28a2c0724caf12f764",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_725dd073f0574d51a2cd58e54426267a",
      "value": "LLMâ€‡infilling:â€‡100%"
     }
    },
    "466759ae0d4a411cbf5fa7eb55b5a1f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dab7203ab364959a0f1237c24c7ffb4",
      "max": 671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d94273b08da242739a30f6012e5dc58d",
      "value": 671
     }
    },
    "60fb26d0acf841f1bb76520a8be4847a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_102b8e7750754cb1bd42633201e2af62",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4c3b1ab8a8e9467295fec8bdedbd686e",
      "value": "â€‡671/671â€‡[31:47&lt;00:00,â€‡â€‡2.90s/it]"
     }
    },
    "905dfb8f2b734625b3aac9114e283d79": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "09fa8278eafc4c28a2c0724caf12f764": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "725dd073f0574d51a2cd58e54426267a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5dab7203ab364959a0f1237c24c7ffb4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d94273b08da242739a30f6012e5dc58d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "102b8e7750754cb1bd42633201e2af62": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c3b1ab8a8e9467295fec8bdedbd686e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3f9f1d51bcf4b009becaaaa03d62a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4710f9eb9e54742a5a071227848048a",
       "IPY_MODEL_8ef9e9f677964c44ba7c0213696dd774",
       "IPY_MODEL_49127e0052dd4b62800c755801a65b78"
      ],
      "layout": "IPY_MODEL_76ff9cb7208c47459f2ad8ea5d6ca642"
     }
    },
    "f4710f9eb9e54742a5a071227848048a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7908ea2adb4a4866a240736b0ab19fd1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5a745e7cd0574c488b00c8c459c74f2a",
      "value": "DecompXâ€‡masking:â€‡100%"
     }
    },
    "8ef9e9f677964c44ba7c0213696dd774": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_397038c370fc46fc876a559dee2cbae8",
      "max": 420,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12fc2ace10614a23ac3769468516997c",
      "value": 420
     }
    },
    "49127e0052dd4b62800c755801a65b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b680ab8f92846b7829a973d5374b811",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f360eff002c848549ba42229210e6b09",
      "value": "â€‡420/420â€‡[00:47&lt;00:00,â€‡15.25it/s]"
     }
    },
    "76ff9cb7208c47459f2ad8ea5d6ca642": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "7908ea2adb4a4866a240736b0ab19fd1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a745e7cd0574c488b00c8c459c74f2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "397038c370fc46fc876a559dee2cbae8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12fc2ace10614a23ac3769468516997c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b680ab8f92846b7829a973d5374b811": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f360eff002c848549ba42229210e6b09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}