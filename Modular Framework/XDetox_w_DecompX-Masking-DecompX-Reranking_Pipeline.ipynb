{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d4549b",
   "metadata": {
    "id": "82d4549b"
   },
   "source": "# xdetox pipeline (decompx masking + marco + decompx reranking)\n\nruns the original XDetox pipeline, close to `lab.py`, with a few changes:\n\n1. **dataset selector** - choose any dataset from `data_configs`.\n2. **small-batch mode** - run only on the first `num_examples` examples.\n3. **decompx masking** - token-level toxicity attribution on roberta to decide which tokens to mask with `<mask>`.\n4. **marco generation** - BART base + non-toxic expert + toxic anti-expert.\n5. **optional decompx-based reranking** - generate several candidates and pick the **least toxic** one.\n6. **evaluation** - BLEU, bertscore, meaningbert, perplexity, and toxicity, plus a summary csv per dataset.\n\n---\n\n## what this does\n\nfor each chosen dataset:\n\n1. **subsetting (optional)**  \n   - if you set `num_examples`, the script writes a **subset file** under  \n     `datasets/_subsets/{data_type}/...`  \n   - this subset matches the format expected by `rewrite.rewrite_example.get_data()`.\n\n2. **masking with decompx**\n\n   - we use a roberta toxicity classifier with decompx to get **per-token toxicity importance**.\n   - tokens that push the prediction towards the toxic class are **replaced with `<mask>`**.\n   - masked sentences are saved as:\n\n     - `data/model_outputs/{output_folder}/{data_type}/DecompX{thresh}/masked_inputs.txt`\n\n   - thresholds control how aggressively we mask:\n\n     - for a threshold value $\\tau$, higher $\\tau$ → **less masking**, lower $\\tau$ → **more masking**.\n\n3. **generation with MaRCo (BART ensemble)**\n\n   for each masked input, we use an ensemble of bart models:\n\n   - **base** model (generic BART).\n   - **expert** model (trained on non-toxic text).\n   - **anti-expert** model (trained on toxic text).\n\n   during generation, logits are combined as a **product-of-experts**:\n\n   $$\n   \\text{logits}_{\\text{ens}} = \\alpha_b \\cdot \\text{logits}_{\\text{base}}\n   + \\alpha_e \\cdot \\text{logits}_{\\text{expert}}\n   - \\alpha_a \\cdot \\text{logits}_{\\text{anti}}\n   $$\n\n   where:\n\n   - $\\alpha_a$ controls how strongly we **push away** from toxic patterns.\n   - $\\alpha_e$ controls how strongly we **pull towards** non-toxic patterns.\n   - $\\alpha_b$ controls the influence of the base model.\n\n   sampling is controlled by:\n\n   - `sample` (sampling vs greedy),\n   - `top_k_gen`,\n   - `top_p`,\n   - `filter_p`,\n   - `temperature`,\n   - `rep_penalty`,\n   - `max_length`.\n\n4. **decompx-based reranking (inside `rewrite.rewrite_example`)**\n\n   when `ranking=True`:\n\n   - for each **input sentence**, the generation script samples `num_candidates` candidates.\n   - for each candidate, it runs the decompx toxicity model **again** and computes a scalar \"toxicity importance\" score for that output.\n   - it then chooses the candidate with **lowest summed toxicity importance** (the \"least toxic\" according to decompx).\n\n   this happens inside the `rewrite.rewrite_example` module via:\n\n   - `--ranking`\n   - `--ranking_num_output {num_candidates}`\n\n5. **evaluation**\n\n   if `run_eval=True`, the notebook calls `evaluation.evaluate_all` for each generation folder and computes:\n\n   - bertscore (f1)\n   - meaningbert\n   - bleu-4\n   - perplexity (orig / gen)\n   - toxicity (orig / gen)\n\n   for each folder under:\n\n   - `data/model_outputs/{output_folder}/{data_type}/DecompX{thresh}/aa*_ae*_.../`\n\n   it writes:\n\n   - `orig.txt` - original toxic inputs.\n   - `gen.txt` - final detoxified outputs.\n   - `gen_stats.txt` - metrics for that run.\n\n   then `_aggregate_eval_csv` aggregates across thresholds into:\n\n   - `data/model_outputs/{output_folder}/{data_type}/{data_type}.csv`\n\n---\n\n## `detoxify()` api\n\ndefinition:\n\n```python\ndef detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run\",\n    thresholds = (0.15, 0.20, 0.25),\n    batch_size: int = 10,\n    ranking: bool = True,\n    sample: bool = True,\n    top_k_gen: int = 50,\n    top_p: float = 0.95,\n    filter_p: float = 1.0,\n    max_length: int = 128,\n    alpha_a: float = None,\n    alpha_e: float = None,\n    alpha_b: float = 1.0,\n    temperature: float = None,\n    rep_penalty: float = None,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    num_candidates: int = 10,\n)\n```\n\n### main arguments\n\n**dataset and i/o**\n\n* `data_type`\n  dataset key from `data_configs`, e.g.:\n\n  * `\"paradetox\"`, `\"dynabench_val\"`, `\"jigsaw_toxic\"`, `\"appdia_original\"`, etc.\n\n* `output_folder`\n  name of the top-level run under `data/dexp_outputs/`.\n  all outputs for this run go to:\n\n  * `data/dexp_outputs/{output_folder}/{data_type}/...`\n\n**masking / thresholds**\n\n* `thresholds`\n  tuple of decompx thresholds to try, e.g. `(0.15, 0.20, 0.25)`.\n  each threshold ( \\tau ) creates a folder `DecompX{τ}` and its own `masked_inputs.txt`, `orig.txt`, `gen.txt`, etc.\n\n* `num_examples`\n\n  * if an integer, only the **first N examples** are used (quick debugging).\n  * if `None`, the full dataset is used.\n\n**generation hyperparameters (marco)**\n\n* `sample`\n\n  * `True`: use sampling (random but controlled by temperature / top-k / top-p).\n  * `False`: greedy decoding.\n\n* `top_k_gen`\n  top-k filter on the **ensembled** logits (only the top-k tokens by probability are kept).\n\n* `top_p`\n  nucleus (top-p) sampling on the **ensembled** logits. keeps the smallest set of tokens whose cumulative probability ≥ `top_p`.\n\n* `filter_p`\n  nucleus filter on the **base model** logits before ensembling (advanced; usually leave at `1.0`).\n\n* `max_length`\n  max length of the generated sequence (in tokens).\n\n* `alpha_a`, `alpha_e`, `alpha_b`\n  ensemble weights for anti-expert, expert, and base:\n\n  * if `None`, defaults from `data_configs[data_type]` are used.\n\n* `temperature`\n  softens or sharpens the probability distribution:\n\n  * higher temperature → more random.\n  * lower temperature → more deterministic.\n  * if `None`, the dataset default is used.\n\n* `rep_penalty`\n  repetition penalty (1.0 = no penalty). larger values discourage repeating tokens.\n\n* `batch_size`\n  number of sequences to generate in a batch (trade-off between speed and gpu memory).\n\n**decompx reranking**\n\n* `ranking`\n\n  * `True`: enable decompx-based reranking of candidates inside `rewrite.rewrite_example`.\n  * `False`: generate only **one** candidate per input (no reranking).\n\n* `num_candidates`\n\n  * when `ranking=True`, this sets `--ranking_num_output`.\n  * for each input, the generator samples `num_candidates` candidates and picks the one with **lowest decompx toxicity importance**.\n\n**evaluation**\n\n* `run_eval`\n\n  * if `True`, run `evaluation.evaluate_all` and write `gen_stats.txt` + summary csv.\n\n* `overwrite_gen`\n\n  * if `True`, regenerate outputs even if `gen.txt` already exists.\n\n* `overwrite_eval`\n\n  * if `True`, recompute evaluation values even if `gen_stats.txt` already exists.\n\n* `skip_ref_eval`\n\n  * if `True`, skip perplexity computation on references (faster).\n\n**echo**\n* `echo`\n  * if `True`, print example inputs, masked inputs, generated outputs, and per-threshold evaluation metrics to the notebook.\n\n---\n\n## example usage\n\n**quick test on a subset of paradetox (with reranking):**\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"colab_run_demo\",\n    thresholds=(0.20,),\n    batch_size=8,\n    ranking=True,\n    sample=True,\n    top_k_gen=50,\n    top_p=0.95,\n    max_length=96,\n    num_examples=50,\n    run_eval=True,\n    overwrite_gen=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n    num_candidates=20,\n)\n```\n\n**run without reranking (single candidate per input):**\n\n```python\ndetoxify(\n    data_type=\"paradetox\",\n    output_folder=\"colab_run_no_rerank\",\n    thresholds=(0.20,),\n    batch_size=8,\n    ranking=False,\n    sample=True,\n    top_k_gen=50,\n    top_p=0.95,\n    max_length=96,\n    num_examples=50,\n    run_eval=True,\n    overwrite_gen=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n)\n```\n\nafter the run, you can inspect:\n\n* `orig.txt` / `gen.txt` under `data/model_outputs/{output_folder}/{data_type}/DecompX{thresh}/...`\n* `gen_stats.txt` for per-run metrics.\n* `{data_type}.csv` for a summary over thresholds.\n\n```\n::contentReference[oaicite:0]{index=0}\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h_yizC4pSWVD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32363,
     "status": "ok",
     "timestamp": 1764459613997,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "h_yizC4pSWVD",
    "outputId": "08bd05bc-0b2c-4335-c457-8bad297183bf"
   },
   "outputs": [],
   "source": "from google.colab import drive; drive.mount('/content/drive')\nimport os, glob, re, sys, torch, json, shutil, math, nltk\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom subprocess import run, PIPE\n\ncandidate = \"/content/drive/MyDrive/w266 - Project/XDetox\"\nprint(\"checking mydrive:\", candidate, \"->\", os.path.isdir(candidate))\n\nXDETOX_DIR = candidate\nprint(\"using XDETOX_DIR:\", XDETOX_DIR)\nassert os.path.isdir(XDETOX_DIR), f\"XDETOX_DIR does not exist: {XDETOX_DIR}\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b8054",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1764459614094,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "5c1b8054",
    "outputId": "e0e9d8e4-8531-44f9-9157-4b40d68c717f"
   },
   "outputs": [],
   "source": "HF_CACHE = os.path.join(XDETOX_DIR, \"cache\")\nos.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n\nif XDETOX_DIR not in sys.path:\n    sys.path.append(XDETOX_DIR)\n\nprint(\"xdetox_dir:\", XDETOX_DIR)\nprint(\"transformers_cache:\", HF_CACHE)\nprint(\"cuda available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"gpu:\", torch.cuda.get_device_name(0))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd6c0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1764459615209,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "00dd6c0b",
    "outputId": "fa731756-bfa8-44c1-8945-6eef19e86d86"
   },
   "outputs": [],
   "source": "for d in [\"rewrite\", \"evaluation\", \"datasets\"]:\n    assert os.path.isdir(os.path.join(XDETOX_DIR, d)), f\"Missing folder: {d}\"\nprint(\"repo folders ok\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a96d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22801,
     "status": "ok",
     "timestamp": 1764459638014,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "c14a96d2",
    "outputId": "ff33ae94-c848-4a58-c63d-d782ecee0cf6"
   },
   "outputs": [],
   "source": "!pip -q install --upgrade pip setuptools wheel\n!pip -q install \"transformers==4.41.2\" \"tokenizers==0.19.1\" \\\n                \"datasets==2.19.0\" \"evaluate==0.4.1\" \\\n                \"sacrebleu==2.4.1\" sacremoses ftfy nltk matplotlib pandas jedi\n\n!pip -q install bert-score"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a132b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1764459638551,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "f9a132b9",
    "outputId": "e4fd46a8-81b4-49f7-9ad7-2bb7a13ca485"
   },
   "outputs": [],
   "source": "nltk.download(\"punkt\", quiet=True)\ntry:\n    nltk.download(\"punkt_tab\", quiet=True)\nexcept Exception:\n    pass\nprint(\"nltk ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0I1tK3vN7i9p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11838,
     "status": "ok",
     "timestamp": 1764459650391,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "0I1tK3vN7i9p",
    "outputId": "7eb66cc3-6d67-410f-f9fd-2651a2ce8fbf"
   },
   "outputs": [],
   "source": "from rewrite.generation import Infiller\nfrom rewrite import rewrite_example as rx\nimport argparse as _argparse"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c9217",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1764459650411,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "8a9c9217",
    "outputId": "5ea28fa3-b67e-4ceb-86dd-cd62f21237d0"
   },
   "outputs": [],
   "source": "data_configs = {\n    \"microagressions_val\": {\n        \"data_path\": \"./datasets/microagressions/val.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"microagressions_test\": {\n        \"data_path\": \"./datasets/microagressions/test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.25,\n        \"temperature\": 2.5,\n    },\n    \"sbf_val\": {\n        \"data_path\": \"./datasets/sbf/sbfdev.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"sbf_test\": {\n        \"data_path\": \"./datasets/sbf/sbftst.csv\",\n        \"rep_penalty\": 1.5,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 5.0,\n        \"temperature\": 2.9,\n    },\n    \"dynabench_val\": {\n        \"data_path\": \"./datasets/dynabench/db_dev.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"dynabench_test\": {\n        \"data_path\": \"./datasets/dynabench/db_test.csv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"jigsaw_toxic\": {\n        \"data_path\": \"./datasets/jigsaw_full_30/test_10k_toxic.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"paradetox\": {\n        \"data_path\": \"./datasets/paradetox/test_toxic_parallel.txt\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_original\": {\n        \"data_path\": \"./datasets/appdia/original-annotated-data/original-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    },\n    \"appdia_discourse\": {\n        \"data_path\": \"./datasets/appdia/discourse-augmented-data/discourse-test.tsv\",\n        \"rep_penalty\": 1.0,\n        \"alpha_a\": 1.5,\n        \"alpha_e\": 4.75,\n        \"temperature\": 2.5,\n    }\n}\nprint(\"datasets:\", \", \".join(data_configs.keys()))"
  },
  {
   "cell_type": "markdown",
   "id": "52f799fe",
   "metadata": {
    "id": "52f799fe"
   },
   "source": "## helpers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8508f",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1764459650443,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "fcb8508f"
   },
   "outputs": [],
   "source": "REPO = XDETOX_DIR\n\ndef _abs_repo_path(rel):\n    return os.path.join(REPO, rel.lstrip(\"./\"))\n\ndef _ensure_dir(p):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef _subset_for_data_type(data_type, data_path, n, out_dir):\n    if n is None or n <= 0:\n        return data_path\n\n    src = _abs_repo_path(data_path)\n    _ensure_dir(out_dir)\n\n    if \"microagressions\" in data_path:\n        df = pd.read_csv(src)\n        cols = df.columns.tolist()\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"sbf\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if \"dynabench\" in data_path:\n        df = pd.read_csv(src)\n        sub = df.head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        sub.to_csv(out, index=False)\n        return out\n\n    if any(k in data_path for k in [\"paradetox\", \"jigsaw\"]):\n        if data_path.endswith(\".txt\"):\n            with open(src, \"r\") as f:\n                lines = [s.rstrip(\"\\n\") for s in f.readlines()]\n            out = os.path.join(out_dir, os.path.basename(src))\n            with open(out, \"w\") as g:\n                for s in lines[:n]:\n                    g.write(s + \"\\n\")\n            return out\n        elif data_path.endswith(\".csv\"):\n            df = pd.read_csv(src).head(n)\n            out = os.path.join(out_dir, os.path.basename(src))\n            df.to_csv(out, index=False)\n            return out\n\n    if \"appdia\" in data_path:\n        df = pd.read_csv(src, sep=\"\\t\").head(n)\n        out = os.path.join(out_dir, os.path.basename(src))\n        df.to_csv(out, sep=\"\\t\", index=False)\n        return out\n\n    out = os.path.join(out_dir, os.path.basename(src))\n    shutil.copy(src, out)\n    return out\n\ndef _parse_run_folder_name(folder_name):\n    pattern = r\"aa(\\d+\\.\\d+)_ae(\\d+\\.\\d+)_ab(\\d+\\.\\d+)_base(.*?)_anti(.*?)_expert(.*?)_temp(\\d+\\.\\d+)_sample(.*?)_topk(\\d+)_reppenalty(\\d+\\.\\d+)_filterp(\\d+\\.\\d+)_maxlength(\\d+)_topp(\\d+\\.\\d+)\"\n    m = re.match(pattern, folder_name)\n    return bool(m)\n\ndef _eval_with_toxicity(base_path, overwrite_eval=False, skip_ref=False, tox_threshold=0.5, tox_batch_size=32):\n    import sys, os\n    for folder in os.listdir(base_path):\n        gen_dir = os.path.join(base_path, folder)\n        if not os.path.isdir(gen_dir) or not _parse_run_folder_name(folder):\n            continue\n        orig_path = os.path.join(gen_dir, \"orig.txt\")\n        gen_path  = os.path.join(gen_dir, \"gen.txt\")\n        out_stats = os.path.join(gen_dir, \"gen_stats.txt\")\n        if not (os.path.exists(orig_path) and os.path.exists(gen_path)):\n            continue\n        if os.path.exists(out_stats) and not overwrite_eval:\n            continue\n\n        env = os.environ.copy()\n        env[\"PYTHONPATH\"] = REPO + (\":\" + env.get(\"PYTHONPATH\",\"\") if env.get(\"PYTHONPATH\") else \"\")\n        cmd = [\n            sys.executable, \"-m\", \"evaluation.evaluate_all\",\n            \"--orig_path\", orig_path,\n            \"--gen_path\",  gen_path,\n            \"--tox_threshold\", str(tox_threshold),\n            \"--tox_batch_size\", str(tox_batch_size),\n        ]\n        if skip_ref:\n            cmd.append(\"--skip_ref\")\n        print(\"eval:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, env=env, stdout=PIPE, stderr=PIPE, text=True)\n        if res.returncode != 0:\n            print(res.stdout)\n            print(res.stderr)\n            res.check_returncode()\n\n\ndef _safe_float(x):\n    try:\n        return float(x)\n    except Exception:\n        return float('nan')\n\ndef _read_stats_file(path):\n    out = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            k, v = line.strip().split(\": \", 1)\n            k = k.replace(\"(skipped)\", \"\").strip().lower()\n            out[k] = _safe_float(v)\n    return out\n\ndef _aggregate_eval_csv(output_folder, data_type, base_out_dir):\n    rows = []\n    for thresh in np.arange(0.15, 0.3, 0.05, dtype=np.float64):\n        mask_dir = f\"DecompX{abs(thresh):g}\" if thresh != 0 else \"DecompX0.0\"\n        base_path = os.path.join(base_out_dir, data_type, mask_dir)\n        if not os.path.isdir(base_path):\n            continue\n        for folder in os.listdir(base_path):\n            gen_dir = os.path.join(base_path, folder)\n            stats_path = os.path.join(gen_dir, \"gen_stats.txt\")\n            if not os.path.exists(stats_path):\n                continue\n            s = _read_stats_file(stats_path)\n            rows.append({\n                \"threshold\":        float(f\"{thresh:.2f}\"),\n                \"folder\":           folder,\n                \"bertscore\":        s.get(\"bertscore\", np.nan),\n                \"meaningbert\":      s.get(\"meaningbert\", np.nan),\n                \"bleu4\":            s.get(\"bleu4\", np.nan),\n                \"perplexity_gen\":   s.get(\"perplexity gen\", np.nan),\n                \"perplexity_orig\":  s.get(\"perplexity orig\", np.nan),\n                \"toxicity_gen\":     s.get(\"toxicity gen\", np.nan),\n                \"toxicity_orig\":    s.get(\"toxicity orig\", np.nan),\n            })\n\n    if rows:\n        cols = [\n            \"threshold\", \"folder\",\n            \"bertscore\", \"meaningbert\", \"bleu4\",\n            \"perplexity_gen\", \"perplexity_orig\",\n            \"toxicity_gen\", \"toxicity_orig\",\n        ]\n        df = pd.DataFrame(rows)\n        df = df[cols]\n        out_csv = os.path.join(base_out_dir, data_type, f\"{data_type}.csv\")\n        df.to_csv(out_csv, index=False)\n        print(\"wrote summary csv:\", out_csv)\n    else:\n        print(\"no evaluation files found to summarize.\")\n\ndef _bool2str(x: bool) -> str:\n    return \"T\" if x else \"F\"\n\n\ndef _build_gen_folder_name(\n    alpha_a, alpha_e, alpha_b,\n    base_type, antiexpert_type, expert_type,\n    temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n):\n    return (\n        \"aa\" + str(alpha_a) +\n        \"_ae\" + str(alpha_e) +\n        \"_ab\" + str(alpha_b) +\n        \"_base\" + base_type[:5] +\n        \"_anti\" + antiexpert_type[:5] +\n        \"_expert\" + expert_type[:5] +\n        \"_temp\" + str(temperature) +\n        \"_sample\" + _bool2str(sample) +\n        \"_topk\" + str(top_k_gen) +\n        \"_reppenalty\" + str(rep_penalty) +\n        \"_filterp\" + str(filter_p) +\n        \"_maxlength\" + str(max_length) +\n        \"_topp\" + str(top_p)\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b940a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1764459650462,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     },
     "user_tz": 480
    },
    "id": "92b940a3"
   },
   "outputs": [],
   "source": "def detoxify(\n    data_type: str = \"paradetox\",\n    output_folder: str = \"colab_run\",\n    thresholds = (0.15, 0.20, 0.25),\n    echo: bool = False,\n    batch_size: int = 10,\n    ranking: bool = True,\n    sample: bool = True,\n    top_k_gen: int = 50,\n    top_p: float = 0.95,\n    filter_p: float = 1.0,\n    max_length: int = 128,\n    alpha_a: float = None,\n    alpha_e: float = None,\n    alpha_b: float = 1.0,\n    temperature: float = None,\n    rep_penalty: float = None,\n    num_examples: int = 100,\n    overwrite_gen: bool = False,\n    run_eval: bool = False,\n    overwrite_eval: bool = False,\n    skip_ref_eval: bool = False,\n    num_candidates: int = 10,\n):\n    assert data_type in data_configs, f\"Unknown data_type: {data_type}\"\n    cfg = data_configs[data_type].copy()\n\n    if ranking and num_candidates < 1:\n        raise ValueError(\"num_candidates must be >= 1 when ranking=True\")\n\n    if alpha_a is None:\n        alpha_a = cfg[\"alpha_a\"]\n    if alpha_e is None:\n        alpha_e = cfg[\"alpha_e\"]\n    if temperature is None:\n        temperature = cfg[\"temperature\"]\n    if rep_penalty is None:\n        rep_penalty = cfg[\"rep_penalty\"]\n\n    base_out_dir = os.path.join(\"data\", \"model_outputs\", output_folder)\n    abs_base_out_dir = os.path.join(REPO, base_out_dir)\n    _ensure_dir(abs_base_out_dir)\n\n    original_data_path = cfg[\"data_path\"]\n    subset_dir = os.path.join(REPO, \"datasets\", \"_subsets\", data_type)\n    _ensure_dir(subset_dir)\n    subset_path = _subset_for_data_type(\n        data_type, original_data_path, num_examples, subset_dir\n    )\n\n    args_data = _argparse.Namespace(data_type=data_type, data_path=subset_path)\n    inputs = rx.get_data(args_data)\n    num_inputs = len(inputs)\n\n    if echo:\n        print(\"=\" * 80)\n        print(f\"dataset: {data_type}\")\n        print(f\"subset path: {subset_path}\")\n        print(f\"output base: {abs_base_out_dir}\")\n        print(f\"number of examples to detoxify: {num_inputs}\")\n        print(f\"thresholds: {', '.join(f'{t:.2f}' for t in thresholds)}\")\n        print(f\"ranking: {ranking}, num_candidates: {num_candidates}\")\n        print(\"\\nexample inputs (first up to 3):\")\n        for i, s in enumerate(inputs[:3]):\n            print(f\"  input[{i}]: {s}\")\n        print(\"=\" * 80)\n\n    base_type = \"base\"\n    antiexpert_type = \"antiexpert\"\n    expert_type = \"expert\"\n\n    gen_folder = _build_gen_folder_name(\n        alpha_a, alpha_e, alpha_b,\n        base_type, antiexpert_type, expert_type,\n        temperature, sample, top_k_gen, rep_penalty, filter_p, max_length, top_p\n    )\n\n    for t in thresholds:\n        mask_dir = f\"DecompX{abs(t):g}\" if t != 0 else \"DecompX0.0\"\n        thresh_root_dir = os.path.join(abs_base_out_dir, data_type, mask_dir)\n        _ensure_dir(thresh_root_dir)\n\n        cmd = [\n            sys.executable, \"-m\", \"rewrite.rewrite_example\",\n            \"--output_dir\", base_out_dir,\n            \"--data_type\", data_type,\n            \"--data_path\", subset_path.replace(REPO + \"/\", \"./\"),\n            \"--rep_penalty\", str(rep_penalty),\n            \"--alpha_a\", str(alpha_a),\n            \"--alpha_e\", str(alpha_e),\n            \"--temperature\", str(temperature),\n            \"--alpha_b\", str(alpha_b),\n            \"--max_length\", str(max_length),\n            \"--batch_size\", str(batch_size),\n            \"--top_k_gen\", str(top_k_gen),\n            \"--top_p\", str(top_p),\n            \"--filter_p\", str(filter_p),\n            \"--thresh\", f\"{t:.2f}\",\n        ]\n\n        if ranking:\n            cmd.append(\"--ranking\")\n            cmd.extend([\"--ranking_num_output\", str(num_candidates)])\n\n        if sample:\n            cmd.append(\"--sample\")\n        if overwrite_gen:\n            cmd.append(\"--overwrite_gen\")\n\n        print(\"run:\", \" \".join(cmd))\n        res = run(cmd, cwd=REPO, stdout=PIPE, stderr=PIPE, text=True)\n\n        print(\"\\n----- rewrite_example stdout -----\")\n        print(res.stdout)\n        print(\"----- rewrite_example stderr -----\")\n        print(res.stderr)\n        print(\"----- end -----\\n\")\n\n        res.check_returncode()\n\n        if echo:\n            print(\"\\n\" + \"-\" * 80)\n            print(f\"threshold t={t:.2f} - sample masked and generated outputs\")\n\n            masked_path = os.path.join(thresh_root_dir, \"masked_inputs.txt\")\n            if os.path.exists(masked_path):\n                with open(masked_path, \"r\") as f:\n                    masked_lines = [l.strip() for l in f.readlines()]\n                print(\"example masked inputs (first up to 3):\")\n                for i, m in enumerate(masked_lines[:3]):\n                    print(f\"  masked[{i}]: {m}\")\n            else:\n                print(f\"masked_inputs.txt not found at {masked_path}\")\n\n            run_dir = os.path.join(thresh_root_dir, gen_folder)\n            gen_txt = os.path.join(run_dir, \"gen.txt\")\n            if os.path.exists(gen_txt):\n                with open(gen_txt, \"r\") as f:\n                    gen_lines = [l.strip() for l in f.readlines()]\n                print(\"\\nexample detoxified outputs (first up to 3):\")\n                for i, g in enumerate(gen_lines[:3]):\n                    print(f\"  detox[{i}]: {g}\")\n            else:\n                print(f\"gen.txt not found at {gen_txt}\")\n\n        if run_eval:\n            base_path = os.path.join(abs_base_out_dir, data_type, mask_dir)\n            _eval_with_toxicity(\n                base_path,\n                overwrite_eval=overwrite_eval,\n                skip_ref=skip_ref_eval,\n                tox_threshold=0.5,\n                tox_batch_size=32,\n            )\n\n            if echo:\n                stats_path = os.path.join(base_path, gen_folder, \"gen_stats.txt\")\n                if os.path.exists(stats_path):\n                    stats = _read_stats_file(stats_path)\n                    print(\"\\nevaluation metrics for this run \"\n                          f\"(t={t:.2f}):\")\n                    metric_keys = [\n                        (\"bertscore\", \"bertscore\"),\n                        (\"meaningbert\", \"meaningbert\"),\n                        (\"bleu4\", \"bleu-4\"),\n                        (\"perplexity gen\", \"perplexity (gen)\"),\n                        (\"perplexity orig\", \"perplexity (orig)\"),\n                        (\"toxicity gen\", \"toxicity (gen)\"),\n                        (\"toxicity orig\", \"toxicity (orig)\"),\n                    ]\n                    for key, label in metric_keys:\n                        val = stats.get(key, None)\n                        if isinstance(val, float) and math.isnan(val):\n                            continue\n                        if val is None:\n                            continue\n                        print(f\"  {label}: {val:.4f}\")\n                else:\n                    print(f\"gen_stats.txt not found at {stats_path}\")\n\n    if run_eval:\n        _aggregate_eval_csv(\n            output_folder,\n            data_type,\n            os.path.join(REPO, \"data\", \"model_outputs\", output_folder),\n        )"
  },
  {
   "cell_type": "markdown",
   "id": "72e393cd",
   "metadata": {
    "id": "72e393cd"
   },
   "source": "## example run"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7626ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b7626ce",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461023125,
     "user_tz": 480,
     "elapsed": 1372660,
     "user": {
      "displayName": "Kent Bourgoing",
      "userId": "01773768369839516808"
     }
    },
    "outputId": "0a210f16-6373-449c-9b8a-2d73dad55d3e"
   },
   "outputs": [],
   "source": "detoxify(\n    data_type=\"paradetox\",\n    output_folder=\"XDetox_w_DecompX-Masking-DecompX-Reranking\",\n    thresholds=(0.20,),\n    echo=True,\n    batch_size=8,\n    ranking=True,\n    sample=True,\n    top_k_gen=50,\n    top_p=0.95,\n    max_length=96,\n    num_examples=1000,\n    run_eval=True,\n    overwrite_gen=True,\n    overwrite_eval=True,\n    skip_ref_eval=False,\n    num_candidates=10,\n)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}